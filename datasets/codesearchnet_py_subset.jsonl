{"task_id": "codesearchnet/python/train/0", "input": "def __msgc_step3_discontinuity_localization(self):\n        \"\"\"\n        Estimate discontinuity in basis of low resolution image segmentation.\n        :return: discontinuity in low resolution\n        \"\"\"\n        import scipy\n\n        start = self._start_time\n        seg = 1 - self.segmentation.astype(np.int8)\n        self.stats[\"low level object voxels\"] = np.sum(seg)\n        self.stats[\"low level image voxels\"] = np.prod(seg.shape)\n        # in seg is now stored low resolution segmentation\n        # back to normal parameters\n        # step 2: discontinuity localization\n        # self.segparams = sparams_hi\n        seg_border = scipy.ndimage.filters.laplace(seg, mode=\"constant\")\n        logger.debug(\"seg_border: %s\", scipy.stats.describe(seg_border, axis=None))\n        # logger.debug(str(np.max(seg_border)))\n        # logger.debug(str(np.min(seg_border)))\n        seg_border[seg_border != 0] = 1\n        logger.debug(\"seg_border: %s\", scipy.stats.describe(seg_border, axis=None))\n        # scipy.ndimage.morphology.distance_transform_edt\n        boundary_dilatation_distance = self.segparams[\"boundary_dilatation_distance\"]\n        seg = scipy.ndimage.morphology.binary_dilation(\n            seg_border,\n            # seg,\n            np.ones(\n                [\n                    (boundary_dilatation_distance * 2) + 1,\n                    (boundary_dilatation_distance * 2) + 1,\n                    (boundary_dilatation_distance * 2) + 1,\n                ]\n            ),\n        )\n        if self.keep_temp_properties:\n            self.temp_msgc_lowres_discontinuity = seg\n        else:\n            self.temp_msgc_lowres_discontinuity = None\n\n        if self.debug_images:\n            import sed3\n\n            pd = sed3.sed3(seg_border)  # ), contour=seg)\n            pd.show()\n            pd = sed3.sed3(seg)  # ), contour=seg)\n            pd.show()\n        # segzoom = scipy.ndimage.interpolation.zoom(seg.astype('float'), zoom,\n        #                                                order=0).astype('int8')\n        self.stats[\"t3\"] = time.time() - start\n        return seg", "target": "Estimate discontinuity in basis of low resolution image segmentation.\n        :return: discontinuity in low resolution"}
{"task_id": "codesearchnet/python/train/1", "input": "def __multiscale_gc_lo2hi_run(self):  # , pyed):\n        \"\"\"\n        Run Graph-Cut segmentation with refinement of low resolution multiscale graph.\n        In first step is performed normal GC on low resolution data\n        Second step construct finer grid on edges of segmentation from first\n        step.\n        There is no option for use without `use_boundary_penalties`\n        \"\"\"\n        # from PyQt4.QtCore import pyqtRemoveInputHook\n        # pyqtRemoveInputHook()\n        self._msgc_lo2hi_resize_init()\n        self.__msgc_step0_init()\n\n        hard_constraints = self.__msgc_step12_low_resolution_segmentation()\n        # ===== high resolution data processing\n        seg = self.__msgc_step3_discontinuity_localization()\n\n        self.stats[\"t3.1\"] = (time.time() - self._start_time)\n        graph = Graph(\n            seg,\n            voxelsize=self.voxelsize,\n            nsplit=self.segparams[\"block_size\"],\n            edge_weight_table=self._msgc_npenalty_table,\n            compute_low_nodes_index=True,\n        )\n\n        # graph.run() = graph.generate_base_grid() + graph.split_voxels()\n        # graph.run()\n        graph.generate_base_grid()\n        self.stats[\"t3.2\"] = (time.time() - self._start_time)\n        graph.split_voxels()\n\n        self.stats[\"t3.3\"] = (time.time() - self._start_time)\n\n        self.stats.update(graph.stats)\n        self.stats[\"t4\"] = (time.time() - self._start_time)\n        mul_mask, mul_val = self.__msgc_tlinks_area_weight_from_low_segmentation(seg)\n        area_weight = 1\n        unariesalt = self.__create_tlinks(\n            self.img,\n            self.voxelsize,\n            self.seeds,\n            area_weight=area_weight,\n            hard_constraints=hard_constraints,\n            mul_mask=None,\n            mul_val=None,\n        )\n        # N-links prepared\n        self.stats[\"t5\"] = (time.time() - self._start_time)\n        un, ind = np.unique(graph.msinds, return_index=True)\n        self.stats[\"t6\"] = (time.time() - self._start_time)\n\n        self.stats[\"t7\"] = (time.time() - self._start_time)\n        unariesalt2_lo2hi = np.hstack(\n            [unariesalt[ind, 0, 0].reshape(-1, 1), unariesalt[ind, 0, 1].reshape(-1, 1)]\n        )\n        nlinks_lo2hi = np.hstack([graph.edges, graph.edges_weights.reshape(-1, 1)])\n        if self.debug_images:\n            import sed3\n\n            ed = sed3.sed3(unariesalt[:, :, 0].reshape(self.img.shape))\n            ed.show()\n            import sed3\n\n            ed = sed3.sed3(unariesalt[:, :, 1].reshape(self.img.shape))\n            ed.show()\n            # ed = sed3.sed3(seg)\n            # ed.show()\n            # import sed3\n            # ed = sed3.sed3(graph.data)\n            # ed.show()\n            # import sed3\n            # ed = sed3.sed3(graph.msinds)\n            # ed.show()\n\n        # nlinks, unariesalt2, msinds = self.__msgc_step45678_construct_graph(area_weight, hard_constraints, seg)\n        # self.__msgc_step9_finish_perform_gc_and_reshape(nlinks, unariesalt2, msinds)\n        self.__msgc_step9_finish_perform_gc_and_reshape(\n            nlinks_lo2hi, unariesalt2_lo2hi, graph.msinds\n        )\n        self._msgc_lo2hi_resize_clean_finish()", "target": "Run Graph-Cut segmentation with refinement of low resolution multiscale graph.\n        In first step is performed normal GC on low resolution data\n        Second step construct finer grid on edges of segmentation from first\n        step.\n        There is no option for use without `use_boundary_penalties`"}
{"task_id": "codesearchnet/python/train/2", "input": "def __multiscale_gc_hi2lo_run(self):  # , pyed):\n        \"\"\"\n        Run Graph-Cut segmentation with simplifiyng of high resolution multiscale graph.\n        In first step is performed normal GC on low resolution data\n        Second step construct finer grid on edges of segmentation from first\n        step.\n        There is no option for use without `use_boundary_penalties`\n        \"\"\"\n        # from PyQt4.QtCore import pyqtRemoveInputHook\n        # pyqtRemoveInputHook()\n\n        self.__msgc_step0_init()\n        hard_constraints = self.__msgc_step12_low_resolution_segmentation()\n        # ===== high resolution data processing\n        seg = self.__msgc_step3_discontinuity_localization()\n        nlinks, unariesalt2, msinds = self.__msgc_step45678_hi2lo_construct_graph(\n            hard_constraints, seg\n        )\n        self.__msgc_step9_finish_perform_gc_and_reshape(nlinks, unariesalt2, msinds)", "target": "Run Graph-Cut segmentation with simplifiyng of high resolution multiscale graph.\n        In first step is performed normal GC on low resolution data\n        Second step construct finer grid on edges of segmentation from first\n        step.\n        There is no option for use without `use_boundary_penalties`"}
{"task_id": "codesearchnet/python/train/3", "input": "def __ordered_values_by_indexes(self, data, inds):\n        \"\"\"\n        Return values (intensities) by indexes.\n\n        Used for multiscale graph cut.\n        data = [[0 1 1],\n                [0 2 2],\n                [0 2 2]]\n\n        inds = [[0 1 2],\n                [3 4 4],\n                [5 4 4]]\n\n        return: [0, 1, 1, 0, 2, 0]\n\n        If the data are not consistent, it will take the maximal value\n\n        \"\"\"\n        # get unique labels and their first indexes\n        # lab, linds = np.unique(inds, return_index=True)\n        # compute values by indexes\n        # values = data.reshape(-1)[linds]\n\n        # alternative slow implementation\n        # if there are different data on same index, it will take\n        # maximal value\n        # lab = np.unique(inds)\n        # values = [0]*len(lab)\n        # for label in lab:\n        #     values[label] = np.max(data[inds == label])\n        #\n        # values = np.asarray(values)\n\n        # yet another implementation\n        values = [None] * (np.max(inds) + 1)\n\n        linear_inds = inds.ravel()\n        linear_data = data.ravel()\n        for i in range(0, len(linear_inds)):\n            # going over all data pixels\n\n            if values[linear_inds[i]] is None:\n                # this index is found for first\n                values[linear_inds[i]] = linear_data[i]\n            elif values[linear_inds[i]] < linear_data[i]:\n                # here can be changed maximal or minimal value\n                values[linear_inds[i]] = linear_data[i]\n\n        values = np.asarray(values)\n\n        return values", "target": "Return values (intensities) by indexes.\n\n        Used for multiscale graph cut.\n        data = [[0 1 1],\n                [0 2 2],\n                [0 2 2]]\n\n        inds = [[0 1 2],\n                [3 4 4],\n                [5 4 4]]\n\n        return: [0, 1, 1, 0, 2, 0]\n\n        If the data are not consistent, it will take the maximal value"}
{"task_id": "codesearchnet/python/train/4", "input": "def __hi2lo_multiscale_indexes(self, mask, orig_shape):  # , zoom):\n        \"\"\"\n        Function computes multiscale indexes of ndarray.\n\n        mask: Says where is original resolution (0) and where is small\n        resolution (1). Mask is in small resolution.\n\n        orig_shape: Original shape of input data.\n        zoom: Usually number greater then 1\n\n        result = [[0 1 2],\n                  [3 4 4],\n                  [5 4 4]]\n        \"\"\"\n\n        mask_orig = zoom_to_shape(mask, orig_shape, dtype=np.int8)\n\n        inds_small = np.arange(mask.size).reshape(mask.shape)\n        inds_small_in_orig = zoom_to_shape(inds_small, orig_shape, dtype=np.int8)\n        inds_orig = np.arange(np.prod(orig_shape)).reshape(orig_shape)\n\n        # inds_orig = inds_orig * mask_orig\n        inds_orig += np.max(inds_small_in_orig) + 1\n        # print 'indexes'\n        # import py3DSeedEditor as ped\n        # import pdb; pdb.set_trace() # BREAKPOINT\n\n        #  '==' is not the same as 'is' for numpy.array\n        inds_small_in_orig[mask_orig == True] = inds_orig[mask_orig == True]  # noqa\n        inds = inds_small_in_orig\n        # print np.max(inds)\n        # print np.min(inds)\n        inds = relabel_squeeze(inds)\n        logger.debug(\n            \"Index after relabeling: %s\", scipy.stats.describe(inds, axis=None)\n        )\n        # logger.debug(\"Minimal index after relabeling: \" + str(np.min(inds)))\n        # inds_orig[mask_orig==True] = 0\n        # inds_small_in_orig[mask_orig==False] = 0\n        # inds = (inds_orig + np.max(inds_small_in_orig) + 1) + inds_small_in_orig\n\n        return inds, mask_orig", "target": "Function computes multiscale indexes of ndarray.\n\n        mask: Says where is original resolution (0) and where is small\n        resolution (1). Mask is in small resolution.\n\n        orig_shape: Original shape of input data.\n        zoom: Usually number greater then 1\n\n        result = [[0 1 2],\n                  [3 4 4],\n                  [5 4 4]]"}
{"task_id": "codesearchnet/python/train/5", "input": "def interactivity(self, min_val=None, max_val=None, qt_app=None):\n        \"\"\"\n        Interactive seed setting with 3d seed editor\n        \"\"\"\n        from .seed_editor_qt import QTSeedEditor\n        from PyQt4.QtGui import QApplication\n\n        if min_val is None:\n            min_val = np.min(self.img)\n\n        if max_val is None:\n            max_val = np.max(self.img)\n\n        window_c = (max_val + min_val) / 2  # .astype(np.int16)\n        window_w = max_val - min_val  # .astype(np.int16)\n\n        if qt_app is None:\n            qt_app = QApplication(sys.argv)\n\n        pyed = QTSeedEditor(\n            self.img,\n            modeFun=self.interactivity_loop,\n            voxelSize=self.voxelsize,\n            seeds=self.seeds,\n            volume_unit=self.volume_unit,\n        )\n\n        pyed.changeC(window_c)\n        pyed.changeW(window_w)\n\n        qt_app.exec_()", "target": "Interactive seed setting with 3d seed editor"}
{"task_id": "codesearchnet/python/train/6", "input": "def set_seeds(self, seeds):\n        \"\"\"\n        Function for manual seed setting. Sets variable seeds and prepares\n        voxels for density model.\n        :param seeds: ndarray (0 - nothing, 1 - object, 2 - background,\n        3 - object just hard constraints, no model training, 4 - background \n        just hard constraints, no model training)\n        \"\"\"\n        if self.img.shape != seeds.shape:\n            raise Exception(\"Seeds must be same size as input image\")\n\n        self.seeds = seeds.astype(\"int8\")\n        self.voxels1 = self.img[self.seeds == 1]\n        self.voxels2 = self.img[self.seeds == 2]", "target": "Function for manual seed setting. Sets variable seeds and prepares\n        voxels for density model.\n        :param seeds: ndarray (0 - nothing, 1 - object, 2 - background,\n        3 - object just hard constraints, no model training, 4 - background \n        just hard constraints, no model training)"}
{"task_id": "codesearchnet/python/train/7", "input": "def run(self, run_fit_model=True):\n        \"\"\"\n        Run the Graph Cut segmentation according to preset parameters.\n\n        :param run_fit_model: Allow to skip model fit when the model is prepared before\n        :return:\n        \"\"\"\n\n        if run_fit_model:\n            self.fit_model(self.img, self.voxelsize, self.seeds)\n\n        self._start_time = time.time()\n        if self.segparams[\"method\"].lower() in (\"graphcut\", \"gc\"):\n            self.__single_scale_gc_run()\n        elif self.segparams[\"method\"].lower() in (\n            \"multiscale_graphcut\",\n            \"multiscale_gc\",\n            \"msgc\",\n            \"msgc_lo2hi\",\n            \"lo2hi\",\n            \"multiscale_graphcut_lo2hi\",\n        ):\n            logger.debug(\"performing multiscale Graph-Cut lo2hi\")\n            self.__multiscale_gc_lo2hi_run()\n        elif self.segparams[\"method\"].lower() in (\n            \"msgc_hi2lo\",\n            \"hi2lo\",\n            \"multiscale_graphcut_hi2lo\",\n        ):\n            logger.debug(\"performing multiscale Graph-Cut hi2lo\")\n            self.__multiscale_gc_hi2lo_run()\n        else:\n            logger.error(\"Unknown segmentation method: \" + self.segparams[\"method\"])", "target": "Run the Graph Cut segmentation according to preset parameters.\n\n        :param run_fit_model: Allow to skip model fit when the model is prepared before\n        :return:"}
{"task_id": "codesearchnet/python/train/8", "input": "def __set_hard_hard_constraints(self, tdata1, tdata2, seeds):\n        \"\"\"\n        it works with seed labels:\n        0: nothing\n        1: object 1 - full seeds\n        2: object 2 - full seeds\n        3: object 1 - not a training seeds\n        4: object 2 - not a training seeds\n        \"\"\"\n        seeds_mask = (seeds == 1) | (seeds == 3)\n        tdata2[seeds_mask] = np.max(tdata2) + 1\n        tdata1[seeds_mask] = 0\n\n        seeds_mask = (seeds == 2) | (seeds == 4)\n        tdata1[seeds_mask] = np.max(tdata1) + 1\n        tdata2[seeds_mask] = 0\n\n        return tdata1, tdata2", "target": "it works with seed labels:\n        0: nothing\n        1: object 1 - full seeds\n        2: object 2 - full seeds\n        3: object 1 - not a training seeds\n        4: object 2 - not a training seeds"}
{"task_id": "codesearchnet/python/train/9", "input": "def __similarity_for_tlinks_obj_bgr(\n        self,\n        data,\n        voxelsize,\n        # voxels1, voxels2,\n        # seeds, otherfeatures=None\n    ):\n        \"\"\"\n        Compute edge values for graph cut tlinks based on image intensity\n        and texture.\n        \"\"\"\n        # self.fit_model(data, voxelsize, seeds)\n        # There is a need to have small vaues for good fit\n        # R(obj) = -ln( Pr (Ip | O) )\n        # R(bck) = -ln( Pr (Ip | B) )\n        # Boykov2001b\n        # ln is computed in likelihood\n        tdata1 = (-(self.mdl.likelihood_from_image(data, voxelsize, 1))) * 10\n        tdata2 = (-(self.mdl.likelihood_from_image(data, voxelsize, 2))) * 10\n\n        # to spare some memory\n        dtype = np.int16\n        if np.any(tdata1 > 32760):\n            dtype = np.float32\n        if np.any(tdata2 > 32760):\n            dtype = np.float32\n\n        if self.segparams[\"use_apriori_if_available\"] and self.apriori is not None:\n            logger.debug(\"using apriori information\")\n            gamma = self.segparams[\"apriori_gamma\"]\n            a1 = (-np.log(self.apriori * 0.998 + 0.001)) * 10\n            a2 = (-np.log(0.999 - (self.apriori * 0.998))) * 10\n            # logger.debug('max ' + str(np.max(tdata1)) + ' min ' + str(np.min(tdata1)))\n            # logger.debug('max ' + str(np.max(tdata2)) + ' min ' + str(np.min(tdata2)))\n            # logger.debug('max ' + str(np.max(a1)) + ' min ' + str(np.min(a1)))\n            # logger.debug('max ' + str(np.max(a2)) + ' min ' + str(np.min(a2)))\n            tdata1u = (((1 - gamma) * tdata1) + (gamma * a1)).astype(dtype)\n            tdata2u = (((1 - gamma) * tdata2) + (gamma * a2)).astype(dtype)\n            tdata1 = tdata1u\n            tdata2 = tdata2u\n            # logger.debug('   max ' + str(np.max(tdata1)) + ' min ' + str(np.min(tdata1)))\n            # logger.debug('   max ' + str(np.max(tdata2)) + ' min ' + str(np.min(tdata2)))\n            # logger.debug('gamma ' + str(gamma))\n\n            # import sed3\n            # ed = sed3.show_slices(tdata1)\n            # ed = sed3.show_slices(tdata2)\n            del tdata1u\n            del tdata2u\n            del a1\n            del a2\n\n        # if np.any(tdata1 < 0) or np.any(tdata2 <0):\n        #     logger.error(\"Problem with tlinks. Likelihood is < 0\")\n\n        # if self.debug_images:\n        #     self.__show_debug_tdata_images(tdata1, tdata2, suptitle=\"likelihood\")\n        return tdata1, tdata2", "target": "Compute edge values for graph cut tlinks based on image intensity\n        and texture."}
{"task_id": "codesearchnet/python/train/10", "input": "def __create_nlinks(self, data, inds=None, boundary_penalties_fcn=None):\n        \"\"\"\n        Compute nlinks grid from data shape information. For boundary penalties\n        are data (intensities) values are used.\n\n        ins: Default is None. Used for multiscale GC. This are indexes of\n        multiscale pixels. Next example shows one superpixel witn index 2.\n        inds = [\n            [1 2 2],\n            [3 2 2],\n            [4 5 6]]\n\n        boundary_penalties_fcn: is function with one argument - axis. It can\n            it can be used for setting penalty weights between neighbooring\n            pixels.\n\n        \"\"\"\n        # use the gerneral graph algorithm\n        # first, we construct the grid graph\n        start = time.time()\n        if inds is None:\n            inds = np.arange(data.size).reshape(data.shape)\n        # if not self.segparams['use_boundary_penalties'] and \\\n        #         boundary_penalties_fcn is None :\n        if boundary_penalties_fcn is None:\n            # This is faster for some specific format\n            edgx = np.c_[inds[:, :, :-1].ravel(), inds[:, :, 1:].ravel()]\n            edgy = np.c_[inds[:, :-1, :].ravel(), inds[:, 1:, :].ravel()]\n            edgz = np.c_[inds[:-1, :, :].ravel(), inds[1:, :, :].ravel()]\n\n        else:\n            logger.info(\"use_boundary_penalties\")\n\n            bpw = self.segparams[\"boundary_penalties_weight\"]\n\n            bpa = boundary_penalties_fcn(2)\n            # id1=inds[:, :, :-1].ravel()\n            edgx = np.c_[\n                inds[:, :, :-1].ravel(),\n                inds[:, :, 1:].ravel(),\n                # cc * np.ones(id1.shape)\n                bpw * bpa[:, :, 1:].ravel(),\n            ]\n\n            bpa = boundary_penalties_fcn(1)\n            # id1 =inds[:, 1:, :].ravel()\n            edgy = np.c_[\n                inds[:, :-1, :].ravel(),\n                inds[:, 1:, :].ravel(),\n                # cc * np.ones(id1.shape)]\n                bpw * bpa[:, 1:, :].ravel(),\n            ]\n\n            bpa = boundary_penalties_fcn(0)\n            # id1 = inds[1:, :, :].ravel()\n            edgz = np.c_[\n                inds[:-1, :, :].ravel(),\n                inds[1:, :, :].ravel(),\n                # cc * np.ones(id1.shape)]\n                bpw * bpa[1:, :, :].ravel(),\n            ]\n\n        # import pdb; pdb.set_trace()\n        edges = np.vstack([edgx, edgy, edgz]).astype(np.int32)\n        # edges - seznam indexu hran, kteres spolu sousedi\\\n        elapsed = time.time() - start\n        self.stats[\"_create_nlinks time\"] = elapsed\n        logger.info(\"__create nlinks time \" + str(elapsed))\n        return edges", "target": "Compute nlinks grid from data shape information. For boundary penalties\n        are data (intensities) values are used.\n\n        ins: Default is None. Used for multiscale GC. This are indexes of\n        multiscale pixels. Next example shows one superpixel witn index 2.\n        inds = [\n            [1 2 2],\n            [3 2 2],\n            [4 5 6]]\n\n        boundary_penalties_fcn: is function with one argument - axis. It can\n            it can be used for setting penalty weights between neighbooring\n            pixels."}
{"task_id": "codesearchnet/python/train/11", "input": "def debug_get_reconstructed_similarity(\n        self,\n        data3d=None,\n        voxelsize=None,\n        seeds=None,\n        area_weight=1,\n        hard_constraints=True,\n        return_unariesalt=False,\n    ):\n        \"\"\"\n        Use actual model to calculate similarity. If no input is given the last image is used.\n        :param data3d:\n        :param voxelsize:\n        :param seeds:\n        :param area_weight:\n        :param hard_constraints:\n        :param return_unariesalt:\n        :return:\n        \"\"\"\n        if data3d is None:\n            data3d = self.img\n        if voxelsize is None:\n            voxelsize = self.voxelsize\n        if seeds is None:\n            seeds = self.seeds\n\n        unariesalt = self.__create_tlinks(\n            data3d,\n            voxelsize,\n            # voxels1, voxels2,\n            seeds,\n            area_weight,\n            hard_constraints,\n        )\n        if return_unariesalt:\n            return unariesalt\n        else:\n            return self._reshape_unariesalt_to_similarity(unariesalt, data3d.shape)", "target": "Use actual model to calculate similarity. If no input is given the last image is used.\n        :param data3d:\n        :param voxelsize:\n        :param seeds:\n        :param area_weight:\n        :param hard_constraints:\n        :param return_unariesalt:\n        :return:"}
{"task_id": "codesearchnet/python/train/12", "input": "def debug_show_reconstructed_similarity(\n        self,\n        data3d=None,\n        voxelsize=None,\n        seeds=None,\n        area_weight=1,\n        hard_constraints=True,\n        show=True,\n        bins=20,\n        slice_number=None,\n    ):\n        \"\"\"\n        Show tlinks.\n        :param data3d: ndarray with input data\n        :param voxelsize:\n        :param seeds:\n        :param area_weight:\n        :param hard_constraints:\n        :param show:\n        :param bins: histogram bins number\n        :param slice_number:\n        :return:\n        \"\"\"\n\n        unariesalt = self.debug_get_reconstructed_similarity(\n            data3d,\n            voxelsize=voxelsize,\n            seeds=seeds,\n            area_weight=area_weight,\n            hard_constraints=hard_constraints,\n            return_unariesalt=True,\n        )\n\n        self._debug_show_unariesalt(\n            unariesalt, show=show, bins=bins, slice_number=slice_number\n        )", "target": "Show tlinks.\n        :param data3d: ndarray with input data\n        :param voxelsize:\n        :param seeds:\n        :param area_weight:\n        :param hard_constraints:\n        :param show:\n        :param bins: histogram bins number\n        :param slice_number:\n        :return:"}
{"task_id": "codesearchnet/python/train/13", "input": "def debug_inspect_node(self, node_msindex):\n        \"\"\"\n        Get info about the node. See pycut.inspect_node() for details.\n        Processing is done in temporary shape.\n\n        :param node_seed:\n        :return: node_unariesalt, node_neighboor_edges_and_weights, node_neighboor_seeds\n        \"\"\"\n        return inspect_node(self.nlinks, self.unariesalt2, self.msinds, node_msindex)", "target": "Get info about the node. See pycut.inspect_node() for details.\n        Processing is done in temporary shape.\n\n        :param node_seed:\n        :return: node_unariesalt, node_neighboor_edges_and_weights, node_neighboor_seeds"}
{"task_id": "codesearchnet/python/train/14", "input": "def debug_interactive_inspect_node(self):\n        \"\"\"\n        Call after segmentation to see selected node neighborhood.\n        User have to select one node by click.\n        :return:\n        \"\"\"\n        if (\n            np.sum(\n                np.abs(\n                    np.asarray(self.msinds.shape) - np.asarray(self.segmentation.shape)\n                )\n            )\n            == 0\n        ):\n            segmentation = self.segmentation\n        else:\n            segmentation = self.temp_msgc_resized_segmentation\n\n        logger.info(\"Click to select one voxel of interest\")\n        import sed3\n\n        ed = sed3.sed3(self.msinds, contour=segmentation == 0)\n        ed.show()\n        edseeds = ed.seeds\n        node_msindex = get_node_msindex(self.msinds, edseeds)\n\n        node_unariesalt, node_neighboor_edges_and_weights, node_neighboor_seeds = self.debug_inspect_node(\n            node_msindex\n        )\n        import sed3\n\n        ed = sed3.sed3(\n            self.msinds, contour=segmentation == 0, seeds=node_neighboor_seeds\n        )\n        ed.show()\n\n        return (\n            node_unariesalt,\n            node_neighboor_edges_and_weights,\n            node_neighboor_seeds,\n            node_msindex,\n        )", "target": "Call after segmentation to see selected node neighborhood.\n        User have to select one node by click.\n        :return:"}
{"task_id": "codesearchnet/python/train/15", "input": "def _ssgc_prepare_data_and_run_computation(\n        self,\n        # voxels1, voxels2,\n        hard_constraints=True,\n        area_weight=1,\n    ):\n        \"\"\"\n        Setting of data.\n        You need set seeds if you want use hard_constraints.\n        \"\"\"\n        # from PyQt4.QtCore import pyqtRemoveInputHook\n        # pyqtRemoveInputHook()\n        # import pdb; pdb.set_trace() # BREAKPOINT\n\n        unariesalt = self.__create_tlinks(\n            self.img,\n            self.voxelsize,\n            # voxels1, voxels2,\n            self.seeds,\n            area_weight,\n            hard_constraints,\n        )\n        #  některém testu  organ semgmentation dosahují unaries -15. což je podiné\n        # stačí vyhodit print před if a je to vidět\n        logger.debug(\"unaries %.3g , %.3g\" % (np.max(unariesalt), np.min(unariesalt)))\n        # create potts pairwise\n        # pairwiseAlpha = -10\n        pairwise = -(np.eye(2) - 1)\n        pairwise = (self.segparams[\"pairwise_alpha\"] * pairwise).astype(np.int32)\n        # pairwise = np.array([[0,30],[30,0]]).astype(np.int32)\n        # print pairwise\n\n        self.iparams = {}\n\n        if self.segparams[\"use_boundary_penalties\"]:\n            sigma = self.segparams[\"boundary_penalties_sigma\"]\n            # set boundary penalties function\n            # Default are penalties based on intensity differences\n            boundary_penalties_fcn = lambda ax: self._boundary_penalties_array(\n                axis=ax, sigma=sigma\n            )\n        else:\n            boundary_penalties_fcn = None\n        nlinks = self.__create_nlinks(\n            self.img, boundary_penalties_fcn=boundary_penalties_fcn\n        )\n\n        self.stats[\"tlinks shape\"].append(unariesalt.reshape(-1, 2).shape)\n        self.stats[\"nlinks shape\"].append(nlinks.shape)\n        # we flatten the unaries\n        # result_graph = cut_from_graph(nlinks, unaries.reshape(-1, 2),\n        # pairwise)\n        start = time.time()\n        if self.debug_images:\n            self._debug_show_unariesalt(unariesalt)\n        result_graph = pygco.cut_from_graph(nlinks, unariesalt.reshape(-1, 2), pairwise)\n        elapsed = time.time() - start\n        self.stats[\"gc time\"] = elapsed\n        result_labeling = result_graph.reshape(self.img.shape)\n\n        return result_labeling", "target": "Setting of data.\n        You need set seeds if you want use hard_constraints."}
{"task_id": "codesearchnet/python/train/16", "input": "def resize_to_shape(data, shape, zoom=None, mode=\"nearest\", order=0):\n    \"\"\"\n    Function resize input data to specific shape.\n    :param data: input 3d array-like data\n    :param shape: shape of output data\n    :param zoom: zoom is used for back compatibility\n    :mode: default is 'nearest'\n    \"\"\"\n    # @TODO remove old code in except part\n    # TODO use function from library in future\n\n    try:\n        # rint 'pred vyjimkou'\n        # aise Exception ('test without skimage')\n        # rint 'za vyjimkou'\n        import skimage\n        import skimage.transform\n\n        # Now we need reshape  seeds and segmentation to original size\n\n        # with warnings.catch_warnings():\n        #     warnings.filterwarnings(\"ignore\", \".*'constant', will be changed to.*\")\n        segm_orig_scale = skimage.transform.resize(\n            data, shape, order=0, preserve_range=True, mode=\"reflect\"\n        )\n\n        segmentation = segm_orig_scale\n        logger.debug(\"resize to orig with skimage\")\n    except:\n        if zoom is None:\n            zoom = shape / np.asarray(data.shape).astype(np.double)\n        segmentation = resize_to_shape_with_zoom(\n            data, zoom=zoom, mode=mode, order=order\n        )\n\n    return segmentation", "target": "Function resize input data to specific shape.\n    :param data: input 3d array-like data\n    :param shape: shape of output data\n    :param zoom: zoom is used for back compatibility\n    :mode: default is 'nearest'"}
{"task_id": "codesearchnet/python/train/17", "input": "def seed_zoom(seeds, zoom):\n    \"\"\"\n    Smart zoom for sparse matrix. If there is resize to bigger resolution\n    thin line of label could be lost. This function prefers labels larger\n    then zero. If there is only one small voxel in larger volume with zeros\n    it is selected.\n    \"\"\"\n    # import scipy\n    # loseeds=seeds\n    labels = np.unique(seeds)\n    # remove first label - 0\n    labels = np.delete(labels, 0)\n    # @TODO smart interpolation for seeds in one block\n    #        loseeds = scipy.ndimage.interpolation.zoom(\n    #            seeds, zoom, order=0)\n    loshape = np.ceil(np.array(seeds.shape) * 1.0 / zoom).astype(np.int)\n    loseeds = np.zeros(loshape, dtype=np.int8)\n    loseeds = loseeds.astype(np.int8)\n    for label in labels:\n        a, b, c = np.where(seeds == label)\n        loa = np.round(a // zoom)\n        lob = np.round(b // zoom)\n        loc = np.round(c // zoom)\n        # loseeds = np.zeros(loshape)\n\n        loseeds[loa, lob, loc] += label\n        # this is to detect conflict seeds\n        loseeds[loseeds > label] = 100\n\n    # remove conflict seeds\n    loseeds[loseeds > 99] = 0\n\n    # import py3DSeedEditor\n    # ped = py3DSeedEditor.py3DSeedEditor(loseeds)\n    # ped.show()\n\n    return loseeds", "target": "Smart zoom for sparse matrix. If there is resize to bigger resolution\n    thin line of label could be lost. This function prefers labels larger\n    then zero. If there is only one small voxel in larger volume with zeros\n    it is selected."}
{"task_id": "codesearchnet/python/train/18", "input": "def zoom_to_shape(data, shape, dtype=None):\n    \"\"\"\n    Zoom data to specific shape.\n    \"\"\"\n    import scipy\n    import scipy.ndimage\n\n    zoomd = np.array(shape) / np.array(data.shape, dtype=np.double)\n    import warnings\n\n    datares = scipy.ndimage.interpolation.zoom(data, zoomd, order=0, mode=\"reflect\")\n\n    if datares.shape != shape:\n        logger.warning(\"Zoom with different output shape\")\n    dataout = np.zeros(shape, dtype=dtype)\n    shpmin = np.minimum(dataout.shape, shape)\n\n    dataout[: shpmin[0], : shpmin[1], : shpmin[2]] = datares[\n        : shpmin[0], : shpmin[1], : shpmin[2]\n    ]\n    return datares", "target": "Zoom data to specific shape."}
{"task_id": "codesearchnet/python/train/19", "input": "def crop(data, crinfo):\n    \"\"\"\n    Crop the data.\n\n    crop(data, crinfo)\n\n    :param crinfo: min and max for each axis - [[minX, maxX], [minY, maxY], [minZ, maxZ]]\n\n    \"\"\"\n    crinfo = fix_crinfo(crinfo)\n    return data[\n        __int_or_none(crinfo[0][0]) : __int_or_none(crinfo[0][1]),\n        __int_or_none(crinfo[1][0]) : __int_or_none(crinfo[1][1]),\n        __int_or_none(crinfo[2][0]) : __int_or_none(crinfo[2][1]),\n    ]", "target": "Crop the data.\n\n    crop(data, crinfo)\n\n    :param crinfo: min and max for each axis - [[minX, maxX], [minY, maxY], [minZ, maxZ]]"}
{"task_id": "codesearchnet/python/train/20", "input": "def combinecrinfo(crinfo1, crinfo2):\n    \"\"\"\n    Combine two crinfos. First used is crinfo1, second used is crinfo2.\n    \"\"\"\n    crinfo1 = fix_crinfo(crinfo1)\n    crinfo2 = fix_crinfo(crinfo2)\n\n    crinfo = [\n        [crinfo1[0][0] + crinfo2[0][0], crinfo1[0][0] + crinfo2[0][1]],\n        [crinfo1[1][0] + crinfo2[1][0], crinfo1[1][0] + crinfo2[1][1]],\n        [crinfo1[2][0] + crinfo2[2][0], crinfo1[2][0] + crinfo2[2][1]],\n    ]\n\n    return crinfo", "target": "Combine two crinfos. First used is crinfo1, second used is crinfo2."}
{"task_id": "codesearchnet/python/train/21", "input": "def crinfo_from_specific_data(data, margin=0):\n    \"\"\"\n    Create crinfo of minimum orthogonal nonzero block in input data.\n\n    :param data: input data\n    :param margin: add margin to minimum block\n    :return:\n    \"\"\"\n    # hledáme automatický ořez, nonzero dá indexy\n    logger.debug(\"crinfo\")\n    logger.debug(str(margin))\n    nzi = np.nonzero(data)\n    logger.debug(str(nzi))\n\n    if np.isscalar(margin):\n        margin = [margin] * 3\n\n    x1 = np.min(nzi[0]) - margin[0]\n    x2 = np.max(nzi[0]) + margin[0] + 1\n    y1 = np.min(nzi[1]) - margin[0]\n    y2 = np.max(nzi[1]) + margin[0] + 1\n    z1 = np.min(nzi[2]) - margin[0]\n    z2 = np.max(nzi[2]) + margin[0] + 1\n\n    # ošetření mezí polí\n    if x1 < 0:\n        x1 = 0\n    if y1 < 0:\n        y1 = 0\n    if z1 < 0:\n        z1 = 0\n\n    if x2 > data.shape[0]:\n        x2 = data.shape[0] - 1\n    if y2 > data.shape[1]:\n        y2 = data.shape[1] - 1\n    if z2 > data.shape[2]:\n        z2 = data.shape[2] - 1\n\n    # ořez\n    crinfo = [[x1, x2], [y1, y2], [z1, z2]]\n    return crinfo", "target": "Create crinfo of minimum orthogonal nonzero block in input data.\n\n    :param data: input data\n    :param margin: add margin to minimum block\n    :return:"}
{"task_id": "codesearchnet/python/train/22", "input": "def uncrop(data, crinfo, orig_shape, resize=False, outside_mode=\"constant\", cval=0):\n    \"\"\"\n    Put some boundary to input image.\n\n\n    :param data: input data\n    :param crinfo: array with minimum and maximum index along each axis\n        [[minX, maxX],[minY, maxY],[minZ, maxZ]]. If crinfo is None, the whole input image is placed into [0, 0, 0].\n        If crinfo is just series of three numbers, it is used as an initial point for input image placement.\n    :param orig_shape: shape of uncropped image\n    :param resize: True or False (default). Usefull if the data.shape does not fit to crinfo shape.\n    :param outside_mode: 'constant', 'nearest'\n    :return:\n    \"\"\"\n\n    if crinfo is None:\n        crinfo = list(zip([0] * data.ndim, orig_shape))\n    elif np.asarray(crinfo).size == data.ndim:\n        crinfo = list(zip(crinfo, np.asarray(crinfo) + data.shape))\n\n    crinfo = fix_crinfo(crinfo)\n    data_out = np.ones(orig_shape, dtype=data.dtype) * cval\n\n    # print 'uncrop ', crinfo\n    # print orig_shape\n    # print data.shape\n    if resize:\n        data = resize_to_shape(data, crinfo[:, 1] - crinfo[:, 0])\n\n    startx = np.round(crinfo[0][0]).astype(int)\n    starty = np.round(crinfo[1][0]).astype(int)\n    startz = np.round(crinfo[2][0]).astype(int)\n\n    data_out[\n        # np.round(crinfo[0][0]).astype(int):np.round(crinfo[0][1]).astype(int)+1,\n        # np.round(crinfo[1][0]).astype(int):np.round(crinfo[1][1]).astype(int)+1,\n        # np.round(crinfo[2][0]).astype(int):np.round(crinfo[2][1]).astype(int)+1\n        startx : startx + data.shape[0],\n        starty : starty + data.shape[1],\n        startz : startz + data.shape[2],\n    ] = data\n\n    if outside_mode == \"nearest\":\n        # for ax in range(data.ndims):\n        # ax = 0\n\n        # copy border slice to pixels out of boundary - the higher part\n        for ax in range(data.ndim):\n            # the part under the crop\n            start = np.round(crinfo[ax][0]).astype(int)\n            slices = [slice(None), slice(None), slice(None)]\n            slices[ax] = start\n            repeated_slice = np.expand_dims(data_out[slices], ax)\n            append_sz = start\n            if append_sz > 0:\n                tile0 = np.repeat(repeated_slice, append_sz, axis=ax)\n                slices = [slice(None), slice(None), slice(None)]\n                slices[ax] = slice(None, start)\n                # data_out[start + data.shape[ax] : , :, :] = tile0\n                data_out[slices] = tile0\n                # plt.imshow(np.squeeze(repeated_slice))\n                # plt.show()\n\n            # the part over the crop\n            start = np.round(crinfo[ax][0]).astype(int)\n            slices = [slice(None), slice(None), slice(None)]\n            slices[ax] = start + data.shape[ax] - 1\n            repeated_slice = np.expand_dims(data_out[slices], ax)\n            append_sz = data_out.shape[ax] - (start + data.shape[ax])\n            if append_sz > 0:\n                tile0 = np.repeat(repeated_slice, append_sz, axis=ax)\n                slices = [slice(None), slice(None), slice(None)]\n                slices[ax] = slice(start + data.shape[ax], None)\n                # data_out[start + data.shape[ax] : , :, :] = tile0\n                data_out[slices] = tile0\n                # plt.imshow(np.squeeze(repeated_slice))\n                # plt.show()\n\n    return data_out", "target": "Put some boundary to input image.\n\n\n    :param data: input data\n    :param crinfo: array with minimum and maximum index along each axis\n        [[minX, maxX],[minY, maxY],[minZ, maxZ]]. If crinfo is None, the whole input image is placed into [0, 0, 0].\n        If crinfo is just series of three numbers, it is used as an initial point for input image placement.\n    :param orig_shape: shape of uncropped image\n    :param resize: True or False (default). Usefull if the data.shape does not fit to crinfo shape.\n    :param outside_mode: 'constant', 'nearest'\n    :return:"}
{"task_id": "codesearchnet/python/train/23", "input": "def fix_crinfo(crinfo, to=\"axis\"):\n    \"\"\"\n    Function recognize order of crinfo and convert it to proper format.\n    \"\"\"\n\n    crinfo = np.asarray(crinfo)\n    if crinfo.shape[0] == 2:\n        crinfo = crinfo.T\n\n    return crinfo", "target": "Function recognize order of crinfo and convert it to proper format."}
{"task_id": "codesearchnet/python/train/24", "input": "def grid_edges(shape, inds=None, return_directions=True):\n    \"\"\"\n    Get list of grid edges\n    :param shape:\n    :param inds:\n    :param return_directions:\n    :return:\n    \"\"\"\n    if inds is None:\n        inds = np.arange(np.prod(shape)).reshape(shape)\n    # if not self.segparams['use_boundary_penalties'] and \\\n    #         boundary_penalties_fcn is None :\n    if len(shape) == 2:\n        edgx = np.c_[inds[:, :-1].ravel(), inds[:, 1:].ravel()]\n        edgy = np.c_[inds[:-1, :].ravel(), inds[1:, :].ravel()]\n\n        edges = [edgx, edgy]\n\n        directions = [\n            np.ones([edgx.shape[0]], dtype=np.int8) * 0,\n            np.ones([edgy.shape[0]], dtype=np.int8) * 1,\n        ]\n\n    elif len(shape) == 3:\n        # This is faster for some specific format\n        edgx = np.c_[inds[:, :, :-1].ravel(), inds[:, :, 1:].ravel()]\n        edgy = np.c_[inds[:, :-1, :].ravel(), inds[:, 1:, :].ravel()]\n        edgz = np.c_[inds[:-1, :, :].ravel(), inds[1:, :, :].ravel()]\n        edges = [edgx, edgy, edgz]\n    else:\n        logger.error(\"Expected 2D or 3D data\")\n\n    # for all edges along first direction put 0, for second direction put 1, for third direction put 3\n    if return_directions:\n        directions = []\n        for idirection in range(len(shape)):\n            directions.append(\n                np.ones([edges[idirection].shape[0]], dtype=np.int8) * idirection\n            )\n    edges = np.concatenate(edges)\n    if return_directions:\n        edge_dir = np.concatenate(directions)\n        return edges, edge_dir\n    else:\n        return edges", "target": "Get list of grid edges\n    :param shape:\n    :param inds:\n    :param return_directions:\n    :return:"}
{"task_id": "codesearchnet/python/train/25", "input": "def gen_grid_2d(shape, voxelsize):\n    \"\"\"\n    Generate list of edges for a base grid.\n    \"\"\"\n    nr, nc = shape\n    nrm1, ncm1 = nr - 1, nc - 1\n    # sh = nm.asarray(shape)\n    # calculate number of edges, in 2D: (nrows * (ncols - 1)) + ((nrows - 1) * ncols)\n    nedges = 0\n    for direction in range(len(shape)):\n        sh = copy.copy(list(shape))\n        sh[direction] += -1\n        nedges += nm.prod(sh)\n\n    nedges_old = ncm1 * nr + nrm1 * nc\n    edges = nm.zeros((nedges, 2), dtype=nm.int16)\n    edge_dir = nm.zeros((ncm1 * nr + nrm1 * nc,), dtype=nm.bool)\n    nodes = nm.zeros((nm.prod(shape), 3), dtype=nm.float32)\n\n    # edges\n    idx = 0\n    row = nm.zeros((ncm1, 2), dtype=nm.int16)\n    row[:, 0] = nm.arange(ncm1)\n    row[:, 1] = nm.arange(ncm1) + 1\n    for ii in range(nr):\n        edges[slice(idx, idx + ncm1), :] = row + nc * ii\n        idx += ncm1\n\n    edge_dir[slice(0, idx)] = 0  # horizontal dir\n\n    idx0 = idx\n    col = nm.zeros((nrm1, 2), dtype=nm.int16)\n    col[:, 0] = nm.arange(nrm1) * nc\n    col[:, 1] = nm.arange(nrm1) * nc + nc\n    for ii in range(nc):\n        edges[slice(idx, idx + nrm1), :] = col + ii\n        idx += nrm1\n\n    edge_dir[slice(idx0, idx)] = 1  # vertical dir\n\n    # nodes\n    idx = 0\n    row = nm.zeros((nc, 3), dtype=nm.float32)\n    row[:, 0] = voxelsize[0] * (nm.arange(nc) + 0.5)\n    row[:, 1] = voxelsize[1] * 0.5\n    for ii in range(nr):\n        nodes[slice(idx, idx + nc), :] = row\n        row[:, 1] += voxelsize[1]\n        idx += nc\n\n    return nodes, edges, edge_dir", "target": "Generate list of edges for a base grid."}
{"task_id": "codesearchnet/python/train/26", "input": "def write_grid_to_vtk(fname, nodes, edges, node_flag=None, edge_flag=None):\n    \"\"\"\n    Write nodes and edges to VTK file\n    :param fname: VTK filename\n    :param nodes:\n    :param edges:\n    :param node_flag: set if this node is really used in output\n    :param edge_flag: set if this flag is used in output\n    :return:\n    \"\"\"\n\n    if node_flag is None:\n        node_flag = np.ones([nodes.shape[0]], dtype=np.bool)\n    if edge_flag is None:\n        edge_flag = np.ones([edges.shape[0]], dtype=np.bool)\n    nodes = make_nodes_3d(nodes)\n    f = open(fname, \"w\")\n\n    f.write(\"# vtk DataFile Version 2.6\\n\")\n    f.write(\"output file\\nASCII\\nDATASET UNSTRUCTURED_GRID\\n\")\n\n    idxs = nm.where(node_flag > 0)[0]\n    nnd = len(idxs)\n    aux = -nm.ones(node_flag.shape, dtype=nm.int32)\n    aux[idxs] = nm.arange(nnd, dtype=nm.int32)\n    f.write(\"\\nPOINTS %d float\\n\" % nnd)\n    for ndi in idxs:\n        f.write(\"%.6f %.6f %.6f\\n\" % tuple(nodes[ndi, :]))\n\n    idxs = nm.where(edge_flag > 0)[0]\n    ned = len(idxs)\n    f.write(\"\\nCELLS %d %d\\n\" % (ned, ned * 3))\n    for edi in idxs:\n        f.write(\"2 %d %d\\n\" % tuple(aux[edges[edi, :]]))\n\n    f.write(\"\\nCELL_TYPES %d\\n\" % ned)\n    for edi in idxs:\n        f.write(\"3\\n\")", "target": "Write nodes and edges to VTK file\n    :param fname: VTK filename\n    :param nodes:\n    :param edges:\n    :param node_flag: set if this node is really used in output\n    :param edge_flag: set if this flag is used in output\n    :return:"}
{"task_id": "codesearchnet/python/train/27", "input": "def add_nodes(self, coors, node_low_or_high=None):\n        \"\"\"\n        Add new nodes at the end of the list.\n        \"\"\"\n        last = self.lastnode\n        if type(coors) is nm.ndarray:\n            if len(coors.shape) == 1:\n                coors = coors.reshape((1, coors.size))\n\n            nadd = coors.shape[0]\n            idx = slice(last, last + nadd)\n        else:\n            nadd = 1\n            idx = self.lastnode\n        right_dimension = coors.shape[1]\n        self.nodes[idx, :right_dimension] = coors\n        self.node_flag[idx] = True\n        self.lastnode += nadd\n        self.nnodes += nadd", "target": "Add new nodes at the end of the list."}
{"task_id": "codesearchnet/python/train/28", "input": "def add_edges(self, conn, edge_direction, edge_group=None, edge_low_or_high=None):\n        \"\"\"\n        Add new edges at the end of the list.\n        :param edge_direction: direction flag\n        :param edge_group: describes group of edges from same low super node and same direction\n        :param edge_low_or_high: zero for low to low resolution, one for high to high or high to low resolution.\n        It is used to set weight from weight table.\n        \"\"\"\n        last = self.lastedge\n        if type(conn) is nm.ndarray:\n            nadd = conn.shape[0]\n            idx = slice(last, last + nadd)\n            if edge_group is None:\n                edge_group = nm.arange(nadd) + last\n        else:\n            nadd = 1\n            idx = nm.array([last])\n            conn = nm.array(conn).reshape((1, 2))\n            if edge_group is None:\n                edge_group = idx\n\n        self.edges[idx, :] = conn\n        self.edge_flag[idx] = True\n        # t_start0 = time.time()\n        # self.edge_flag_idx.extend(list(range(idx.start, idx.stop)))\n        # self.stats[\"t split 082\"] += time.time() - t_start0\n        self.edge_dir[idx] = edge_direction\n        self.edge_group[idx] = edge_group\n        # TODO change this just to array of low_or_high_resolution\n        if edge_low_or_high is not None and self._edge_weight_table is not None:\n            self.edges_weights[idx] = self._edge_weight_table[\n                edge_low_or_high, edge_direction\n            ]\n        self.lastedge += nadd\n        self.nedges += nadd", "target": "Add new edges at the end of the list.\n        :param edge_direction: direction flag\n        :param edge_group: describes group of edges from same low super node and same direction\n        :param edge_low_or_high: zero for low to low resolution, one for high to high or high to low resolution.\n        It is used to set weight from weight table."}
{"task_id": "codesearchnet/python/train/29", "input": "def _edge_group_substitution(\n        self, ndid, nsplit, idxs, sr_tab, ndoffset, ed_remove, into_or_from\n    ):\n        \"\"\"\n        Reconnect edges.\n        :param ndid: id of low resolution edges\n        :param nsplit: number of split\n        :param idxs: indexes of low resolution\n        :param sr_tab:\n        :param ndoffset:\n        :param ed_remove:\n        :param into_or_from: if zero, connection of input edges is done. If one, connection of output edges\n        is performed.\n        :return:\n        \"\"\"\n        # this is useful for type(idxs) == np.ndarray\n        eidxs = idxs[nm.where(self.edges[idxs, 1 - into_or_from] == ndid)[0]]\n        # selected_edges = self.edges[idxs, 1 - into_or_from]\n        # selected_edges == ndid\n        # whre = nm.where(self.edges[idxs, 1 - into_or_from] == ndid)\n        # whre0 = (nm.where(self.edges[idxs, 1 - into_or_from] == ndid) == ndid)[0]\n        # eidxs = [idxs[i] for i in idxs]\n        for igrp in self.edges_by_group(eidxs):\n            if igrp.shape[0] > 1:\n                # high resolution block to high resolution block\n                # all directions are the same\n                directions = self.edge_dir[igrp[0]]\n                edge_indexes = sr_tab[directions, :].T.flatten() + ndoffset\n                # debug code\n                # if len(igrp) != len(edge_indexes):\n                #     print(\"Problem \")\n                self.edges[igrp, 1] = edge_indexes\n                if self._edge_weight_table is not None:\n                    self.edges_weights[igrp] = self._edge_weight_table[1, directions]\n            else:\n                # low res block to hi res block, if into_or_from is set to 0\n                # hig res block to low res block, if into_or_from is set to 1\n                ed_remove.append(igrp[0])\n                # number of new edges is equal to number of pixels on one side of the box (in 2D and D too)\n                nnewed = np.power(nsplit, self.data.ndim - 1)\n                muleidxs = nm.tile(igrp, nnewed)\n                # copy the low-res edge multipletime\n                newed = self.edges[muleidxs, :]\n                neweddir = self.edge_dir[muleidxs]\n                local_node_ids = sr_tab[\n                    self.edge_dir[igrp] + self.data.ndim * into_or_from, :\n                ].T.flatten()\n                # first or second (the actual) node id is substitued by new node indexes\n                newed[:, 1 - into_or_from] = local_node_ids + ndoffset\n                if self._edge_weight_table is not None:\n                    self.add_edges(\n                        newed, neweddir, self.edge_group[igrp], edge_low_or_high=1\n                    )\n                else:\n                    self.add_edges(\n                        newed, neweddir, self.edge_group[igrp], edge_low_or_high=None\n                    )\n        return ed_remove", "target": "Reconnect edges.\n        :param ndid: id of low resolution edges\n        :param nsplit: number of split\n        :param idxs: indexes of low resolution\n        :param sr_tab:\n        :param ndoffset:\n        :param ed_remove:\n        :param into_or_from: if zero, connection of input edges is done. If one, connection of output edges\n        is performed.\n        :return:"}
{"task_id": "codesearchnet/python/train/30", "input": "def generate_base_grid(self, vtk_filename=None):\n        \"\"\"\n        Run first step of algorithm. Next step is split_voxels\n        :param vtk_filename:\n        :return:\n        \"\"\"\n        nd, ed, ed_dir = self.gen_grid_fcn(self.data.shape, self.voxelsize)\n        self.add_nodes(nd)\n        self.add_edges(ed, ed_dir, edge_low_or_high=0)\n\n        if vtk_filename is not None:\n            self.write_vtk(vtk_filename)", "target": "Run first step of algorithm. Next step is split_voxels\n        :param vtk_filename:\n        :return:"}
{"task_id": "codesearchnet/python/train/31", "input": "def split_voxels(self, vtk_filename=None):\n        \"\"\"\n        Second step of algorithm\n        :return:()\n        \"\"\"\n        self.cache = {}\n        self.stats[\"t graph 10\"] = time.time() - self.start_time\n        self.msi = MultiscaleArray(self.data.shape, block_size=self.nsplit)\n\n        # old implementation\n        # idxs = nm.where(self.data)\n        # nr, nc = self.data.shape\n        # for k, (ir, ic) in enumerate(zip(*idxs)):\n        #     ndid = ic + ir * nc\n        #     self.split_voxel(ndid, self.nsplit)\n\n        # new_implementation\n        # for ndid in np.flatnonzero(self.data):\n        #     self.split_voxel(ndid, self.nsplit)\n\n        # even newer implementation\n        self.stats[\"t graph 11\"] = time.time() - self.start_time\n        for ndid, val in enumerate(self.data.ravel()):\n            t_split_start = time.time()\n            if val == 0:\n                if self.compute_msindex:\n                    self.msi.set_block_lowres(ndid, ndid)\n                self.stats[\"t graph low\"] += time.time() - t_split_start\n            else:\n                self.split_voxel(ndid)\n                self.stats[\"t graph high\"] += time.time() - t_split_start\n\n        self.stats[\"t graph 13\"] = time.time() - self.start_time\n        self.finish()\n        if vtk_filename is not None:\n            self.write_vtk(vtk_filename)\n        self.stats[\"t graph 14\"] = time.time() - self.start_time", "target": "Second step of algorithm\n        :return:()"}
{"task_id": "codesearchnet/python/train/32", "input": "def mul_block(self, index, val):\n        \"\"\"Multiply values in block\"\"\"\n        self._prepare_cache_slice(index)\n        self.msinds[self.cache_slice] *= val", "target": "Multiply values in block"}
{"task_id": "codesearchnet/python/train/33", "input": "def select_from_fv_by_seeds(fv, seeds, unique_cls):\n    \"\"\"\n    Tool to make simple feature functions take features from feature array by seeds.\n    :param fv: ndarray with lineariezed feature. It's shape is MxN, where M is number of image pixels and N is number\n    of features\n    :param seeds: ndarray with seeds. Does not to be linear.\n    :param unique_cls: number of used seeds clases. Like [1, 2]\n    :return: fv_selection, seeds_selection - selection from feature vector and selection from seeds\n    \"\"\"\n    logger.debug(\"seeds\" + str(seeds))\n    # fvlin = fv.reshape(-1, int(fv.size/seeds.size))\n    expected_shape = [seeds.size, int(fv.size/seeds.size)]\n    if fv.shape[0] != expected_shape[0] or fv.shape[1] != expected_shape[1]:\n        raise AssertionError(\"Wrong shape of input feature vector array fv\")\n    # sd = seeds.reshape(-1, 1)\n    selection = np.in1d(seeds, unique_cls)\n    fv_selection = fv[selection]\n    seeds_selection = seeds.flatten()[selection]\n    # sd = sd[]\n    return fv_selection, seeds_selection", "target": "Tool to make simple feature functions take features from feature array by seeds.\n    :param fv: ndarray with lineariezed feature. It's shape is MxN, where M is number of image pixels and N is number\n    of features\n    :param seeds: ndarray with seeds. Does not to be linear.\n    :param unique_cls: number of used seeds clases. Like [1, 2]\n    :return: fv_selection, seeds_selection - selection from feature vector and selection from seeds"}
{"task_id": "codesearchnet/python/train/34", "input": "def return_fv_by_seeds(fv, seeds=None, unique_cls=None):\n    \"\"\"\n    Return features selected by seeds and unique_cls or selection from features and corresponding seed classes.\n\n    :param fv: ndarray with lineariezed feature. It's shape is MxN, where M is number of image pixels and N is number\n    of features\n    :param seeds: ndarray with seeds. Does not to be linear.\n    :param unique_cls: number of used seeds clases. Like [1, 2]\n    :return: fv, sd - selection from feature vector and selection from seeds or just fv for whole image\n    \"\"\"\n    if seeds is not None:\n        if unique_cls is not None:\n            return select_from_fv_by_seeds(fv, seeds, unique_cls)\n        else:\n            raise AssertionError(\"Input unique_cls has to be not None if seeds is not None.\")\n    else:\n        return fv", "target": "Return features selected by seeds and unique_cls or selection from features and corresponding seed classes.\n\n    :param fv: ndarray with lineariezed feature. It's shape is MxN, where M is number of image pixels and N is number\n    of features\n    :param seeds: ndarray with seeds. Does not to be linear.\n    :param unique_cls: number of used seeds clases. Like [1, 2]\n    :return: fv, sd - selection from feature vector and selection from seeds or just fv for whole image"}
{"task_id": "codesearchnet/python/train/35", "input": "def expand(self, expression):\n        \"\"\"Expands logical constructions.\"\"\"\n        self.logger.debug(\"expand : expression %s\", str(expression))\n        if not is_string(expression):\n            return expression\n\n        result = self._pattern.sub(lambda var: str(self._variables[var.group(1)]), expression)\n\n        result = result.strip()\n        self.logger.debug('expand : %s - result : %s', expression, result)\n\n        if is_number(result):\n            if result.isdigit():\n                self.logger.debug('     expand is integer !!!')\n                return int(result)\n            else:\n                self.logger.debug('     expand is float !!!')\n                return float(result)\n        return result", "target": "Expands logical constructions."}
{"task_id": "codesearchnet/python/train/36", "input": "def get_gutter_client(\n        alias='default',\n        cache=CLIENT_CACHE,\n        **kwargs\n):\n    \"\"\"\n    Creates gutter clients and memoizes them in a registry for future quick access.\n\n    Args:\n        alias (str or None): Name of the client. Used for caching.\n            If name is falsy then do not use the cache.\n        cache (dict): cache to store gutter managers in.\n        **kwargs: kwargs to be passed the Manger class.\n\n    Returns (Manager):\n        A gutter client.\n\n    \"\"\"\n    from gutter.client.models import Manager\n\n    if not alias:\n        return Manager(**kwargs)\n    elif alias not in cache:\n        cache[alias] = Manager(**kwargs)\n\n    return cache[alias]", "target": "Creates gutter clients and memoizes them in a registry for future quick access.\n\n    Args:\n        alias (str or None): Name of the client. Used for caching.\n            If name is falsy then do not use the cache.\n        cache (dict): cache to store gutter managers in.\n        **kwargs: kwargs to be passed the Manger class.\n\n    Returns (Manager):\n        A gutter client."}
{"task_id": "codesearchnet/python/train/37", "input": "def _modulo(self, decimal_argument):\n        \"\"\"\n        The mod operator is prone to floating point errors, so use decimal.\n\n        101.1 % 100\n        >>> 1.0999999999999943\n\n        decimal_context.divmod(Decimal('100.1'), 100)\n        >>> (Decimal('1'), Decimal('0.1'))\n        \"\"\"\n        _times, remainder = self._context.divmod(decimal_argument, 100)\n\n        # match the builtin % behavior by adding the N to the result if negative\n        return remainder if remainder >= 0 else remainder + 100", "target": "The mod operator is prone to floating point errors, so use decimal.\n\n        101.1 % 100\n        >>> 1.0999999999999943\n\n        decimal_context.divmod(Decimal('100.1'), 100)\n        >>> (Decimal('1'), Decimal('0.1'))"}
{"task_id": "codesearchnet/python/train/38", "input": "def enabled_for(self, inpt):\n        \"\"\"\n        Checks to see if this switch is enabled for the provided input.\n\n        If ``compounded``, all switch conditions must be ``True`` for the switch\n        to be enabled.  Otherwise, *any* condition needs to be ``True`` for the\n        switch to be enabled.\n\n        The switch state is then checked to see if it is ``GLOBAL`` or\n        ``DISABLED``.  If it is not, then the switch is ``SELECTIVE`` and each\n        condition is checked.\n\n        Keyword Arguments:\n        inpt -- An instance of the ``Input`` class.\n        \"\"\"\n\n        signals.switch_checked.call(self)\n        signal_decorated = partial(self.__signal_and_return, inpt)\n\n        if self.state is self.states.GLOBAL:\n            return signal_decorated(True)\n        elif self.state is self.states.DISABLED:\n            return signal_decorated(False)\n\n        conditions_dict = ConditionsDict.from_conditions_list(self.conditions)\n        conditions = conditions_dict.get_by_input(inpt)\n\n        if conditions:\n            result = self.__enabled_func(\n                cond.call(inpt)\n                for cond\n                in conditions\n                if cond.argument(inpt).applies\n            )\n        else:\n            result = None\n\n        return signal_decorated(result)", "target": "Checks to see if this switch is enabled for the provided input.\n\n        If ``compounded``, all switch conditions must be ``True`` for the switch\n        to be enabled.  Otherwise, *any* condition needs to be ``True`` for the\n        switch to be enabled.\n\n        The switch state is then checked to see if it is ``GLOBAL`` or\n        ``DISABLED``.  If it is not, then the switch is ``SELECTIVE`` and each\n        condition is checked.\n\n        Keyword Arguments:\n        inpt -- An instance of the ``Input`` class."}
{"task_id": "codesearchnet/python/train/39", "input": "def call(self, inpt):\n        \"\"\"\n        Returns if the condition applies to the ``inpt``.\n\n        If the class ``inpt`` is an instance of is not the same class as the\n        condition's own ``argument``, then ``False`` is returned.  This also\n        applies to the ``NONE`` input.\n\n        Otherwise, ``argument`` is called, with ``inpt`` as the instance and\n        the value is compared to the ``operator`` and the Value is returned.  If\n        the condition is ``negative``, then then ``not`` the value is returned.\n\n        Keyword Arguments:\n        inpt -- An instance of the ``Input`` class.\n        \"\"\"\n        if inpt is Manager.NONE_INPUT:\n            return False\n\n        # Call (construct) the argument with the input object\n        argument_instance = self.argument(inpt)\n\n        if not argument_instance.applies:\n            return False\n\n        application = self.__apply(argument_instance, inpt)\n\n        if self.negative:\n            application = not application\n\n        return application", "target": "Returns if the condition applies to the ``inpt``.\n\n        If the class ``inpt`` is an instance of is not the same class as the\n        condition's own ``argument``, then ``False`` is returned.  This also\n        applies to the ``NONE`` input.\n\n        Otherwise, ``argument`` is called, with ``inpt`` as the instance and\n        the value is compared to the ``operator`` and the Value is returned.  If\n        the condition is ``negative``, then then ``not`` the value is returned.\n\n        Keyword Arguments:\n        inpt -- An instance of the ``Input`` class."}
{"task_id": "codesearchnet/python/train/40", "input": "def switches(self):\n        \"\"\"\n        List of all switches currently registered.\n        \"\"\"\n        results = [\n            switch for name, switch in self.storage.iteritems()\n            if name.startswith(self.__joined_namespace)\n        ]\n\n        return results", "target": "List of all switches currently registered."}
{"task_id": "codesearchnet/python/train/41", "input": "def switch(self, name):\n        \"\"\"\n        Returns the switch with the provided ``name``.\n\n        If ``autocreate`` is set to ``True`` and no switch with that name\n        exists, a ``DISABLED`` switch will be with that name.\n\n        Keyword Arguments:\n        name -- A name of a switch.\n        \"\"\"\n        try:\n            switch = self.storage[self.__namespaced(name)]\n        except KeyError:\n            if not self.autocreate:\n                raise ValueError(\"No switch named '%s' registered in '%s'\" % (name, self.namespace))\n\n            switch = self.__create_and_register_disabled_switch(name)\n\n        switch.manager = self\n        return switch", "target": "Returns the switch with the provided ``name``.\n\n        If ``autocreate`` is set to ``True`` and no switch with that name\n        exists, a ``DISABLED`` switch will be with that name.\n\n        Keyword Arguments:\n        name -- A name of a switch."}
{"task_id": "codesearchnet/python/train/42", "input": "def register(self, switch, signal=signals.switch_registered):\n        '''\n        Register a switch and persist it to the storage.\n        '''\n        if not switch.name:\n            raise ValueError('Switch name cannot be blank')\n\n        switch.manager = self\n        self.__persist(switch)\n\n        signal.call(switch)", "target": "Register a switch and persist it to the storage."}
{"task_id": "codesearchnet/python/train/43", "input": "def verify(obj, times=1, atleast=None, atmost=None, between=None,\n           inorder=False):\n    \"\"\"Central interface to verify interactions.\n\n    `verify` uses a fluent interface::\n\n        verify(<obj>, times=2).<method_name>(<args>)\n\n    `args` can be as concrete as necessary. Often a catch-all is enough,\n    especially if you're working with strict mocks, bc they throw at call\n    time on unwanted, unconfigured arguments::\n\n        from mockito import ANY, ARGS, KWARGS\n        when(manager).add_tasks(1, 2, 3)\n        ...\n        # no need to duplicate the specification; every other argument pattern\n        # would have raised anyway.\n        verify(manager).add_tasks(1, 2, 3)  # duplicates `when`call\n        verify(manager).add_tasks(*ARGS)\n        verify(manager).add_tasks(...)       # Py3\n        verify(manager).add_tasks(Ellipsis)  # Py2\n\n    \"\"\"\n\n    if isinstance(obj, str):\n        obj = get_obj(obj)\n\n    verification_fn = _get_wanted_verification(\n        times=times, atleast=atleast, atmost=atmost, between=between)\n    if inorder:\n        verification_fn = verification.InOrder(verification_fn)\n\n    # FIXME?: Catch error if obj is neither a Mock nor a known stubbed obj\n    theMock = _get_mock_or_raise(obj)\n\n    class Verify(object):\n        def __getattr__(self, method_name):\n            return invocation.VerifiableInvocation(\n                theMock, method_name, verification_fn)\n\n    return Verify()", "target": "Central interface to verify interactions.\n\n    `verify` uses a fluent interface::\n\n        verify(<obj>, times=2).<method_name>(<args>)\n\n    `args` can be as concrete as necessary. Often a catch-all is enough,\n    especially if you're working with strict mocks, bc they throw at call\n    time on unwanted, unconfigured arguments::\n\n        from mockito import ANY, ARGS, KWARGS\n        when(manager).add_tasks(1, 2, 3)\n        ...\n        # no need to duplicate the specification; every other argument pattern\n        # would have raised anyway.\n        verify(manager).add_tasks(1, 2, 3)  # duplicates `when`call\n        verify(manager).add_tasks(*ARGS)\n        verify(manager).add_tasks(...)       # Py3\n        verify(manager).add_tasks(Ellipsis)  # Py2"}
{"task_id": "codesearchnet/python/train/44", "input": "def when(obj, strict=None):\n    \"\"\"Central interface to stub functions on a given `obj`\n\n    `obj` should be a module, a class or an instance of a class; it can be\n    a Dummy you created with :func:`mock`. ``when`` exposes a fluent interface\n    where you configure a stub in three steps::\n\n        when(<obj>).<method_name>(<args>).thenReturn(<value>)\n\n    Compared to simple *patching*, stubbing in mockito requires you to specify\n    conrete `args` for which the stub will answer with a concrete `<value>`.\n    All invocations that do not match this specific call signature will be\n    rejected. They usually throw at call time.\n\n    Stubbing in mockito's sense thus means not only to get rid of unwanted\n    side effects, but effectively to turn function calls into constants.\n\n    E.g.::\n\n        # Given ``dog`` is an instance of a ``Dog``\n        when(dog).bark('Grrr').thenReturn('Wuff')\n        when(dog).bark('Miau').thenRaise(TypeError())\n\n        # With this configuration set up:\n        assert dog.bark('Grrr') == 'Wuff'\n        dog.bark('Miau')  # will throw TypeError\n        dog.bark('Wuff')  # will throw unwanted interaction\n\n    Stubbing can effectively be used as monkeypatching; usage shown with\n    the `with` context managing::\n\n        with when(os.path).exists('/foo').thenReturn(True):\n            ...\n\n    Most of the time verifying your interactions is not necessary, because\n    your code under tests implicitly verifies the return value by evaluating\n    it. See :func:`verify` if you need to, see also :func:`expect` to setup\n    expected call counts up front.\n\n    If your function is pure side effect and does not return something, you\n    can omit the specific answer. The default then is `None`::\n\n        when(manager).do_work()\n\n    `when` verifies the method name, the expected argument signature, and the\n    actual, factual arguments your code under test uses against the original\n    object and its function so its easier to spot changing interfaces.\n\n    Sometimes it's tedious to spell out all arguments::\n\n        from mockito import ANY, ARGS, KWARGS\n        when(requests).get('http://example.com/', **KWARGS).thenReturn(...)\n        when(os.path).exists(ANY)\n        when(os.path).exists(ANY(str))\n\n    .. note:: You must :func:`unstub` after stubbing, or use `with`\n        statement.\n\n    Set ``strict=False`` to bypass the function signature checks.\n\n    See related :func:`when2` which has a more pythonic interface.\n\n    \"\"\"\n\n    if isinstance(obj, str):\n        obj = get_obj(obj)\n\n    if strict is None:\n        strict = True\n    theMock = _get_mock(obj, strict=strict)\n\n    class When(object):\n        def __getattr__(self, method_name):\n            return invocation.StubbedInvocation(\n                theMock, method_name, strict=strict)\n\n    return When()", "target": "Central interface to stub functions on a given `obj`\n\n    `obj` should be a module, a class or an instance of a class; it can be\n    a Dummy you created with :func:`mock`. ``when`` exposes a fluent interface\n    where you configure a stub in three steps::\n\n        when(<obj>).<method_name>(<args>).thenReturn(<value>)\n\n    Compared to simple *patching*, stubbing in mockito requires you to specify\n    conrete `args` for which the stub will answer with a concrete `<value>`.\n    All invocations that do not match this specific call signature will be\n    rejected. They usually throw at call time.\n\n    Stubbing in mockito's sense thus means not only to get rid of unwanted\n    side effects, but effectively to turn function calls into constants.\n\n    E.g.::\n\n        # Given ``dog`` is an instance of a ``Dog``\n        when(dog).bark('Grrr').thenReturn('Wuff')\n        when(dog).bark('Miau').thenRaise(TypeError())\n\n        # With this configuration set up:\n        assert dog.bark('Grrr') == 'Wuff'\n        dog.bark('Miau')  # will throw TypeError\n        dog.bark('Wuff')  # will throw unwanted interaction\n\n    Stubbing can effectively be used as monkeypatching; usage shown with\n    the `with` context managing::\n\n        with when(os.path).exists('/foo').thenReturn(True):\n            ...\n\n    Most of the time verifying your interactions is not necessary, because\n    your code under tests implicitly verifies the return value by evaluating\n    it. See :func:`verify` if you need to, see also :func:`expect` to setup\n    expected call counts up front.\n\n    If your function is pure side effect and does not return something, you\n    can omit the specific answer. The default then is `None`::\n\n        when(manager).do_work()\n\n    `when` verifies the method name, the expected argument signature, and the\n    actual, factual arguments your code under test uses against the original\n    object and its function so its easier to spot changing interfaces.\n\n    Sometimes it's tedious to spell out all arguments::\n\n        from mockito import ANY, ARGS, KWARGS\n        when(requests).get('http://example.com/', **KWARGS).thenReturn(...)\n        when(os.path).exists(ANY)\n        when(os.path).exists(ANY(str))\n\n    .. note:: You must :func:`unstub` after stubbing, or use `with`\n        statement.\n\n    Set ``strict=False`` to bypass the function signature checks.\n\n    See related :func:`when2` which has a more pythonic interface."}
{"task_id": "codesearchnet/python/train/45", "input": "def when2(fn, *args, **kwargs):\n    \"\"\"Stub a function call with the given arguments\n\n    Exposes a more pythonic interface than :func:`when`. See :func:`when` for\n    more documentation.\n\n    Returns `AnswerSelector` interface which exposes `thenReturn`,\n    `thenRaise`, and `thenAnswer` as usual. Always `strict`.\n\n    Usage::\n\n        # Given `dog` is an instance of a `Dog`\n        when2(dog.bark, 'Miau').thenReturn('Wuff')\n\n    .. note:: You must :func:`unstub` after stubbing, or use `with`\n        statement.\n\n    \"\"\"\n    obj, name = get_obj_attr_tuple(fn)\n    theMock = _get_mock(obj, strict=True)\n    return invocation.StubbedInvocation(theMock, name)(*args, **kwargs)", "target": "Stub a function call with the given arguments\n\n    Exposes a more pythonic interface than :func:`when`. See :func:`when` for\n    more documentation.\n\n    Returns `AnswerSelector` interface which exposes `thenReturn`,\n    `thenRaise`, and `thenAnswer` as usual. Always `strict`.\n\n    Usage::\n\n        # Given `dog` is an instance of a `Dog`\n        when2(dog.bark, 'Miau').thenReturn('Wuff')\n\n    .. note:: You must :func:`unstub` after stubbing, or use `with`\n        statement."}
{"task_id": "codesearchnet/python/train/46", "input": "def patch(fn, attr_or_replacement, replacement=None):\n    \"\"\"Patch/Replace a function.\n\n    This is really like monkeypatching, but *note* that all interactions\n    will be recorded and can be verified. That is, using `patch` you stay in\n    the domain of mockito.\n\n    Two ways to call this. Either::\n\n        patch(os.path.exists, lambda str: True)  # two arguments\n        # OR\n        patch(os.path, 'exists', lambda str: True)  # three arguments\n\n    If called with three arguments, the mode is *not* strict to allow *adding*\n    methods. If called with two arguments, mode is always `strict`.\n\n    .. note:: You must :func:`unstub` after stubbing, or use `with`\n        statement.\n\n    \"\"\"\n    if replacement is None:\n        replacement = attr_or_replacement\n        return when2(fn, Ellipsis).thenAnswer(replacement)\n    else:\n        obj, name = fn, attr_or_replacement\n        theMock = _get_mock(obj, strict=True)\n        return invocation.StubbedInvocation(\n            theMock, name, strict=False)(Ellipsis).thenAnswer(replacement)", "target": "Patch/Replace a function.\n\n    This is really like monkeypatching, but *note* that all interactions\n    will be recorded and can be verified. That is, using `patch` you stay in\n    the domain of mockito.\n\n    Two ways to call this. Either::\n\n        patch(os.path.exists, lambda str: True)  # two arguments\n        # OR\n        patch(os.path, 'exists', lambda str: True)  # three arguments\n\n    If called with three arguments, the mode is *not* strict to allow *adding*\n    methods. If called with two arguments, mode is always `strict`.\n\n    .. note:: You must :func:`unstub` after stubbing, or use `with`\n        statement."}
{"task_id": "codesearchnet/python/train/47", "input": "def expect(obj, strict=None,\n           times=None, atleast=None, atmost=None, between=None):\n    \"\"\"Stub a function call, and set up an expected call count.\n\n    Usage::\n\n        # Given `dog` is an instance of a `Dog`\n        expect(dog, times=1).bark('Wuff').thenReturn('Miau')\n        dog.bark('Wuff')\n        dog.bark('Wuff')  # will throw at call time: too many invocations\n\n        # maybe if you need to ensure that `dog.bark()` was called at all\n        verifyNoUnwantedInteractions()\n\n    .. note:: You must :func:`unstub` after stubbing, or use `with`\n        statement.\n\n    See :func:`when`, :func:`when2`, :func:`verifyNoUnwantedInteractions`\n\n    \"\"\"\n    if strict is None:\n        strict = True\n    theMock = _get_mock(obj, strict=strict)\n\n    verification_fn = _get_wanted_verification(\n        times=times, atleast=atleast, atmost=atmost, between=between)\n\n    class Expect(object):\n        def __getattr__(self, method_name):\n            return invocation.StubbedInvocation(\n                theMock, method_name, verification=verification_fn,\n                strict=strict)\n\n    return Expect()", "target": "Stub a function call, and set up an expected call count.\n\n    Usage::\n\n        # Given `dog` is an instance of a `Dog`\n        expect(dog, times=1).bark('Wuff').thenReturn('Miau')\n        dog.bark('Wuff')\n        dog.bark('Wuff')  # will throw at call time: too many invocations\n\n        # maybe if you need to ensure that `dog.bark()` was called at all\n        verifyNoUnwantedInteractions()\n\n    .. note:: You must :func:`unstub` after stubbing, or use `with`\n        statement.\n\n    See :func:`when`, :func:`when2`, :func:`verifyNoUnwantedInteractions`"}
{"task_id": "codesearchnet/python/train/48", "input": "def unstub(*objs):\n    \"\"\"Unstubs all stubbed methods and functions\n\n    If you don't pass in any argument, *all* registered mocks and\n    patched modules, classes etc. will be unstubbed.\n\n    Note that additionally, the underlying registry will be cleaned.\n    After an `unstub` you can't :func:`verify` anymore because all\n    interactions will be forgotten.\n    \"\"\"\n\n    if objs:\n        for obj in objs:\n            mock_registry.unstub(obj)\n    else:\n        mock_registry.unstub_all()", "target": "Unstubs all stubbed methods and functions\n\n    If you don't pass in any argument, *all* registered mocks and\n    patched modules, classes etc. will be unstubbed.\n\n    Note that additionally, the underlying registry will be cleaned.\n    After an `unstub` you can't :func:`verify` anymore because all\n    interactions will be forgotten."}
{"task_id": "codesearchnet/python/train/49", "input": "def verifyZeroInteractions(*objs):\n    \"\"\"Verify that no methods have been called on given objs.\n\n    Note that strict mocks usually throw early on unexpected, unstubbed\n    invocations. Partial mocks ('monkeypatched' objects or modules) do not\n    support this functionality at all, bc only for the stubbed invocations\n    the actual usage gets recorded. So this function is of limited use,\n    nowadays.\n\n    \"\"\"\n    for obj in objs:\n        theMock = _get_mock_or_raise(obj)\n\n        if len(theMock.invocations) > 0:\n            raise VerificationError(\n                \"\\nUnwanted interaction: %s\" % theMock.invocations[0])", "target": "Verify that no methods have been called on given objs.\n\n    Note that strict mocks usually throw early on unexpected, unstubbed\n    invocations. Partial mocks ('monkeypatched' objects or modules) do not\n    support this functionality at all, bc only for the stubbed invocations\n    the actual usage gets recorded. So this function is of limited use,\n    nowadays."}
{"task_id": "codesearchnet/python/train/50", "input": "def verifyNoUnwantedInteractions(*objs):\n    \"\"\"Verifies that expectations set via `expect` are met\n\n    E.g.::\n\n        expect(os.path, times=1).exists(...).thenReturn(True)\n        os.path('/foo')\n        verifyNoUnwantedInteractions(os.path)  # ok, called once\n\n    If you leave out the argument *all* registered objects will\n    be checked.\n\n    .. note:: **DANGERZONE**: If you did not :func:`unstub` correctly,\n        it is possible that old registered mocks, from other tests\n        leak.\n\n    See related :func:`expect`\n    \"\"\"\n\n    if objs:\n        theMocks = map(_get_mock_or_raise, objs)\n    else:\n        theMocks = mock_registry.get_registered_mocks()\n\n    for mock in theMocks:\n        for i in mock.stubbed_invocations:\n            i.verify()", "target": "Verifies that expectations set via `expect` are met\n\n    E.g.::\n\n        expect(os.path, times=1).exists(...).thenReturn(True)\n        os.path('/foo')\n        verifyNoUnwantedInteractions(os.path)  # ok, called once\n\n    If you leave out the argument *all* registered objects will\n    be checked.\n\n    .. note:: **DANGERZONE**: If you did not :func:`unstub` correctly,\n        it is possible that old registered mocks, from other tests\n        leak.\n\n    See related :func:`expect`"}
{"task_id": "codesearchnet/python/train/51", "input": "def verifyStubbedInvocationsAreUsed(*objs):\n    \"\"\"Ensure stubs are actually used.\n\n    This functions just ensures that stubbed methods are actually used. Its\n    purpose is to detect interface changes after refactorings. It is meant\n    to be invoked usually without arguments just before :func:`unstub`.\n\n    \"\"\"\n    if objs:\n        theMocks = map(_get_mock_or_raise, objs)\n    else:\n        theMocks = mock_registry.get_registered_mocks()\n\n\n    for mock in theMocks:\n        for i in mock.stubbed_invocations:\n            if not i.allow_zero_invocations and i.used < len(i.answers):\n                raise VerificationError(\"\\nUnused stub: %s\" % i)", "target": "Ensure stubs are actually used.\n\n    This functions just ensures that stubbed methods are actually used. Its\n    purpose is to detect interface changes after refactorings. It is meant\n    to be invoked usually without arguments just before :func:`unstub`."}
{"task_id": "codesearchnet/python/train/52", "input": "def get_function_host(fn):\n    \"\"\"Destructure a given function into its host and its name.\n\n    The 'host' of a function is a module, for methods it is usually its\n    instance or its class. This is safe only for methods, for module wide,\n    globally declared names it must be considered experimental.\n\n    For all reasonable fn: ``getattr(*get_function_host(fn)) == fn``\n\n    Returns tuple (host, fn-name)\n    Otherwise should raise TypeError\n    \"\"\"\n\n    obj = None\n    try:\n        name = fn.__name__\n        obj = fn.__self__\n    except AttributeError:\n        pass\n\n    if obj is None:\n        # Due to how python imports work, everything that is global on a module\n        # level must be regarded as not safe here. For now, we go for the extra\n        # mile, TBC, because just specifying `os.path.exists` would be 'cool'.\n        #\n        # TLDR;:\n        # E.g. `inspect.getmodule(os.path.exists)` returns `genericpath` bc\n        # that's where `exists` is defined and comes from. But from the point\n        # of view of the user `exists` always comes and is used from `os.path`\n        # which points e.g. to `ntpath`. We thus must patch `ntpath`.\n        # But that's the same for most imports::\n        #\n        #     # b.py\n        #     from a import foo\n        #\n        # Now asking `getmodule(b.foo)` it tells you `a`, but we access and use\n        # `b.foo` and we therefore must patch `b`.\n\n        obj, name = find_invoking_frame_and_try_parse()\n        # safety check!\n        assert getattr(obj, name) == fn\n\n\n    return obj, name", "target": "Destructure a given function into its host and its name.\n\n    The 'host' of a function is a module, for methods it is usually its\n    instance or its class. This is safe only for methods, for module wide,\n    globally declared names it must be considered experimental.\n\n    For all reasonable fn: ``getattr(*get_function_host(fn)) == fn``\n\n    Returns tuple (host, fn-name)\n    Otherwise should raise TypeError"}
{"task_id": "codesearchnet/python/train/53", "input": "def get_obj(path):\n    \"\"\"Return obj for given dotted path.\n\n    Typical inputs for `path` are 'os' or 'os.path' in which case you get a\n    module; or 'os.path.exists' in which case you get a function from that\n    module.\n\n    Just returns the given input in case it is not a str.\n\n    Note: Relative imports not supported.\n    Raises ImportError or AttributeError as appropriate.\n\n    \"\"\"\n    # Since we usually pass in mocks here; duck typing is not appropriate\n    # (mocks respond to every attribute).\n    if not isinstance(path, str):\n        return path\n\n    if path.startswith('.'):\n        raise TypeError('relative imports are not supported')\n\n    parts = path.split('.')\n    head, tail = parts[0], parts[1:]\n\n    obj = importlib.import_module(head)\n\n    # Normally a simple reduce, but we go the extra mile\n    # for good exception messages.\n    for i, name in enumerate(tail):\n        try:\n            obj = getattr(obj, name)\n        except AttributeError:\n            # Note the [:i] instead of [:i+1], so we get the path just\n            # *before* the AttributeError, t.i. the part of it that went ok.\n            module = '.'.join([head] + tail[:i])\n            try:\n                importlib.import_module(module)\n            except ImportError:\n                raise AttributeError(\n                    \"object '%s' has no attribute '%s'\" % (module, name))\n            else:\n                raise AttributeError(\n                    \"module '%s' has no attribute '%s'\" % (module, name))\n    return obj", "target": "Return obj for given dotted path.\n\n    Typical inputs for `path` are 'os' or 'os.path' in which case you get a\n    module; or 'os.path.exists' in which case you get a function from that\n    module.\n\n    Just returns the given input in case it is not a str.\n\n    Note: Relative imports not supported.\n    Raises ImportError or AttributeError as appropriate."}
{"task_id": "codesearchnet/python/train/54", "input": "def get_obj_attr_tuple(path):\n    \"\"\"Split path into (obj, attribute) tuple.\n\n    Given `path` is 'os.path.exists' will thus return `(os.path, 'exists')`\n\n    If path is not a str, delegates to `get_function_host(path)`\n\n    \"\"\"\n    if not isinstance(path, str):\n        return get_function_host(path)\n\n    if path.startswith('.'):\n        raise TypeError('relative imports are not supported')\n\n    try:\n        leading, end = path.rsplit('.', 1)\n    except ValueError:\n        raise TypeError('path must have dots')\n\n    return get_obj(leading), end", "target": "Split path into (obj, attribute) tuple.\n\n    Given `path` is 'os.path.exists' will thus return `(os.path, 'exists')`\n\n    If path is not a str, delegates to `get_function_host(path)`"}
{"task_id": "codesearchnet/python/train/55", "input": "def spy(object):\n    \"\"\"Spy an object.\n\n    Spying means that all functions will behave as before, so they will\n    be side effects, but the interactions can be verified afterwards.\n\n    Returns Dummy-like, almost empty object as proxy to `object`.\n\n    The *returned* object must be injected and used by the code under test;\n    after that all interactions can be verified as usual.\n    T.i. the original object **will not be patched**, and has no further\n    knowledge as before.\n\n    E.g.::\n\n        import time\n        time = spy(time)\n        # inject time\n        do_work(..., time)\n        verify(time).time()\n\n    \"\"\"\n    if inspect.isclass(object) or inspect.ismodule(object):\n        class_ = None\n    else:\n        class_ = object.__class__\n\n    class Spy(_Dummy):\n        if class_:\n            __class__ = class_\n\n        def __getattr__(self, method_name):\n            return RememberedProxyInvocation(theMock, method_name)\n\n        def __repr__(self):\n            name = 'Spied'\n            if class_:\n                name += class_.__name__\n            return \"<%s id=%s>\" % (name, id(self))\n\n\n    obj = Spy()\n    theMock = Mock(obj, strict=True, spec=object)\n\n    mock_registry.register(obj, theMock)\n    return obj", "target": "Spy an object.\n\n    Spying means that all functions will behave as before, so they will\n    be side effects, but the interactions can be verified afterwards.\n\n    Returns Dummy-like, almost empty object as proxy to `object`.\n\n    The *returned* object must be injected and used by the code under test;\n    after that all interactions can be verified as usual.\n    T.i. the original object **will not be patched**, and has no further\n    knowledge as before.\n\n    E.g.::\n\n        import time\n        time = spy(time)\n        # inject time\n        do_work(..., time)\n        verify(time).time()"}
{"task_id": "codesearchnet/python/train/56", "input": "def spy2(fn):  # type: (...) -> None\n    \"\"\"Spy usage of given `fn`.\n\n    Patches the module, class or object `fn` lives in, so that all\n    interactions can be recorded; otherwise executes `fn` as before, so\n    that all side effects happen as before.\n\n    E.g.::\n\n        import time\n        spy(time.time)\n        do_work(...)  # nothing injected, uses global patched `time` module\n        verify(time).time()\n\n    Note that builtins often cannot be patched because they're read-only.\n\n\n    \"\"\"\n    if isinstance(fn, str):\n        answer = get_obj(fn)\n    else:\n        answer = fn\n\n    when2(fn, Ellipsis).thenAnswer(answer)", "target": "Spy usage of given `fn`.\n\n    Patches the module, class or object `fn` lives in, so that all\n    interactions can be recorded; otherwise executes `fn` as before, so\n    that all side effects happen as before.\n\n    E.g.::\n\n        import time\n        spy(time.time)\n        do_work(...)  # nothing injected, uses global patched `time` module\n        verify(time).time()\n\n    Note that builtins often cannot be patched because they're read-only."}
{"task_id": "codesearchnet/python/train/57", "input": "def mock(config_or_spec=None, spec=None, strict=OMITTED):\n    \"\"\"Create 'empty' objects ('Mocks').\n\n    Will create an empty unconfigured object, that you can pass\n    around. All interactions (method calls) will be recorded and can be\n    verified using :func:`verify` et.al.\n\n    A plain `mock()` will be not `strict`, and thus all methods regardless\n    of the arguments will return ``None``.\n\n    .. note:: Technically all attributes will return an internal interface.\n        Because of that a simple ``if mock().foo:`` will surprisingly pass.\n\n    If you set strict to ``True``: ``mock(strict=True)`` all unexpected\n    interactions will raise an error instead.\n\n    You configure a mock using :func:`when`, :func:`when2` or :func:`expect`.\n    You can also very conveniently just pass in a dict here::\n\n        response = mock({'text': 'ok', 'raise_for_status': lambda: None})\n\n    You can also create an empty Mock which is specced against a given\n    `spec`: ``mock(requests.Response)``. These mock are by default strict,\n    thus they raise if you want to stub a method, the spec does not implement.\n    Mockito will also match the function signature.\n\n    You can pre-configure a specced mock as well::\n\n        response = mock({'json': lambda: {'status': 'Ok'}},\n                        spec=requests.Response)\n\n    Mocks are by default callable. Configure the callable behavior using\n    `when`::\n\n        dummy = mock()\n        when(dummy).__call_(1).thenReturn(2)\n\n    All other magic methods must be configured this way or they will raise an\n    AttributeError.\n\n\n    See :func:`verify` to verify your interactions after usage.\n\n    \"\"\"\n\n    if type(config_or_spec) is dict:\n        config = config_or_spec\n    else:\n        config = {}\n        spec = config_or_spec\n\n    if strict is OMITTED:\n        strict = False if spec is None else True\n\n\n    class Dummy(_Dummy):\n        if spec:\n            __class__ = spec  # make isinstance work\n\n        def __getattr__(self, method_name):\n            if strict:\n                raise AttributeError(\n                    \"'Dummy' has no attribute %r configured\" % method_name)\n            return functools.partial(\n                remembered_invocation_builder, theMock, method_name)\n\n        def __repr__(self):\n            name = 'Dummy'\n            if spec:\n                name += spec.__name__\n            return \"<%s id=%s>\" % (name, id(self))\n\n\n    # That's a tricky one: The object we will return is an *instance* of our\n    # Dummy class, but the mock we register will point and patch the class.\n    # T.i. so that magic methods (`__call__` etc.) can be configured.\n    obj = Dummy()\n    theMock = Mock(Dummy, strict=strict, spec=spec)\n\n    for n, v in config.items():\n        if inspect.isfunction(v):\n            invocation.StubbedInvocation(theMock, n)(Ellipsis).thenAnswer(v)\n        else:\n            setattr(obj, n, v)\n\n    mock_registry.register(obj, theMock)\n    return obj", "target": "Create 'empty' objects ('Mocks').\n\n    Will create an empty unconfigured object, that you can pass\n    around. All interactions (method calls) will be recorded and can be\n    verified using :func:`verify` et.al.\n\n    A plain `mock()` will be not `strict`, and thus all methods regardless\n    of the arguments will return ``None``.\n\n    .. note:: Technically all attributes will return an internal interface.\n        Because of that a simple ``if mock().foo:`` will surprisingly pass.\n\n    If you set strict to ``True``: ``mock(strict=True)`` all unexpected\n    interactions will raise an error instead.\n\n    You configure a mock using :func:`when`, :func:`when2` or :func:`expect`.\n    You can also very conveniently just pass in a dict here::\n\n        response = mock({'text': 'ok', 'raise_for_status': lambda: None})\n\n    You can also create an empty Mock which is specced against a given\n    `spec`: ``mock(requests.Response)``. These mock are by default strict,\n    thus they raise if you want to stub a method, the spec does not implement.\n    Mockito will also match the function signature.\n\n    You can pre-configure a specced mock as well::\n\n        response = mock({'json': lambda: {'status': 'Ok'}},\n                        spec=requests.Response)\n\n    Mocks are by default callable. Configure the callable behavior using\n    `when`::\n\n        dummy = mock()\n        when(dummy).__call_(1).thenReturn(2)\n\n    All other magic methods must be configured this way or they will raise an\n    AttributeError.\n\n\n    See :func:`verify` to verify your interactions after usage."}
{"task_id": "codesearchnet/python/train/58", "input": "def importPuppetClasses(self, smartProxyId):\n        \"\"\" Function importPuppetClasses\n        Force the reload of puppet classes\n\n        @param smartProxyId: smartProxy Id\n        @return RETURN: the API result\n        \"\"\"\n        return self.api.create('{}/{}/import_puppetclasses'\n                               .format(self.objName, smartProxyId), '{}')", "target": "Function importPuppetClasses\n        Force the reload of puppet classes\n\n        @param smartProxyId: smartProxy Id\n        @return RETURN: the API result"}
{"task_id": "codesearchnet/python/train/59", "input": "def get_templates(model):\n    \"\"\" Return a list of templates usable by a model. \"\"\"\n    for template_name, template in templates.items():\n        if issubclass(template.model, model):\n            yield (template_name, template.layout._meta.verbose_name)", "target": "Return a list of templates usable by a model."}
{"task_id": "codesearchnet/python/train/60", "input": "def attach(*layouts, **kwargs):\n    \"\"\"\n    Registers the given layout(s) classes\n    admin site:\n\n    @pages.register(Page)\n    class Default(PageLayout):\n        pass\n    \"\"\"\n\n    def _model_admin_wrapper(layout_class):\n        register(layout_class, layouts[0])\n        return layout_class\n    return _model_admin_wrapper", "target": "Registers the given layout(s) classes\n    admin site:\n\n    @pages.register(Page)\n    class Default(PageLayout):\n        pass"}
{"task_id": "codesearchnet/python/train/61", "input": "def enhance(self):\n        \"\"\" Function enhance\n        Enhance the object with new item or enhanced items\n        \"\"\"\n        self.update({'os_default_templates':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemOsDefaultTemplate)})\n        self.update({'config_templates':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemConfigTemplate)})\n        self.update({'ptables':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemPTable)})\n        self.update({'media':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemMedia)})\n        self.update({'architectures':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemArchitecture)})", "target": "Function enhance\n        Enhance the object with new item or enhanced items"}
{"task_id": "codesearchnet/python/train/62", "input": "def get_api_envs():\n    \"\"\"Get required API keys from environment variables.\"\"\"\n    client_id = os.environ.get('CLIENT_ID')\n    user_id = os.environ.get('USER_ID')\n    if not client_id or not user_id:\n        raise ValueError('API keys are not found in the environment')\n    return client_id, user_id", "target": "Get required API keys from environment variables."}
{"task_id": "codesearchnet/python/train/63", "input": "def api_call(method, end_point, params=None, client_id=None, access_token=None):\n    \"\"\"Call given API end_point with API keys.\n    :param method: HTTP method (e.g. 'get', 'delete').\n    :param end_point: API endpoint (e.g. 'users/john/sets').\n    :param params: Dictionary to be sent in the query string (e.g. {'myparam': 'myval'})\n    :param client_id: Quizlet client ID as string.\n    :param access_token: Quizlet access token as string.\n    client_id and access_token are mutually exclusive but mandatory.\n    \"\"\"\n    if bool(client_id) == bool(access_token):\n        raise ValueError('Either client_id or access_token')\n\n    url = 'https://api.quizlet.com/2.0/{}'.format(end_point)\n\n    if not params:\n        params = {}\n    if client_id:\n        params['client_id'] = client_id\n\n    headers = {'Authorization': 'Bearer {}'.format(access_token)} if access_token else None\n\n    response = requests.request(method, url, params=params, headers=headers)\n\n    if int(response.status_code / 100) != 2:\n        error_title = ''\n        try:\n            error_title += ', ' + response.json()['error_title']\n        except ValueError:\n            pass\n        except KeyError:\n            pass\n        raise ValueError(\n            '{} returned {}{}'.format(url, response.status_code, error_title)\n        )\n\n    try:\n        return response.json()\n    except json.decoder.JSONDecodeError:\n        pass", "target": "Call given API end_point with API keys.\n    :param method: HTTP method (e.g. 'get', 'delete').\n    :param end_point: API endpoint (e.g. 'users/john/sets').\n    :param params: Dictionary to be sent in the query string (e.g. {'myparam': 'myval'})\n    :param client_id: Quizlet client ID as string.\n    :param access_token: Quizlet access token as string.\n    client_id and access_token are mutually exclusive but mandatory."}
{"task_id": "codesearchnet/python/train/64", "input": "def request_upload_secret(self, secret_id):\n        \"\"\"\n        :return: json with \"keyId\" as secret and \"url\" for posting key\n        \"\"\"\n        return self._router.post_request_upload_secret(org_id=self.organizationId,\n                                                       instance_id=self.instanceId,\n                                                       secret_id=secret_id).json()", "target": ":return: json with \"keyId\" as secret and \"url\" for posting key"}
{"task_id": "codesearchnet/python/train/65", "input": "def checkAndCreate(self, key, payload, domainId):\n        \"\"\" Function checkAndCreate\n        Check if a subnet exists and create it if not\n\n        @param key: The targeted subnet\n        @param payload: The targeted subnet description\n        @param domainId: The domainId to be attached wiuth the subnet\n        @return RETURN: The id of the subnet\n        \"\"\"\n        if key not in self:\n            self[key] = payload\n        oid = self[key]['id']\n        if not oid:\n            return False\n        #~ Ensure subnet contains the domain\n        subnetDomainIds = []\n        for domain in self[key]['domains']:\n            subnetDomainIds.append(domain['id'])\n        if domainId not in subnetDomainIds:\n            subnetDomainIds.append(domainId)\n            self[key][\"domain_ids\"] = subnetDomainIds\n            if len(self[key][\"domains\"]) is not len(subnetDomainIds):\n                return False\n        return oid", "target": "Function checkAndCreate\n        Check if a subnet exists and create it if not\n\n        @param key: The targeted subnet\n        @param payload: The targeted subnet description\n        @param domainId: The domainId to be attached wiuth the subnet\n        @return RETURN: The id of the subnet"}
{"task_id": "codesearchnet/python/train/66", "input": "def removeDomain(self, subnetId, domainId):\n        \"\"\" Function removeDomain\n        Delete a domain from a subnet\n\n        @param subnetId: The subnet Id\n        @param domainId: The domainId to be attached wiuth the subnet\n        @return RETURN: boolean\n        \"\"\"\n        subnetDomainIds = []\n        for domain in self[subnetId]['domains']:\n            subnetDomainIds.append(domain['id'])\n        subnetDomainIds.remove(domainId)\n        self[subnetId][\"domain_ids\"] = subnetDomainIds\n        return len(self[subnetId][\"domains\"]) is len(subnetDomainIds)", "target": "Function removeDomain\n        Delete a domain from a subnet\n\n        @param subnetId: The subnet Id\n        @param domainId: The domainId to be attached wiuth the subnet\n        @return RETURN: boolean"}
{"task_id": "codesearchnet/python/train/67", "input": "def exclusive(via=threading.Lock):\n    \"\"\"\n    Mark a callable as exclusive\n\n    :param via: factory for a Lock to guard the callable\n\n    Guards the callable against being entered again before completion.\n    Explicitly raises a :py:exc:`RuntimeError` on violation.\n\n    :note: If applied to a method, it is exclusive across all instances.\n    \"\"\"\n    def make_exclusive(fnc):\n        fnc_guard = via()\n\n        @functools.wraps(fnc)\n        def exclusive_call(*args, **kwargs):\n            if fnc_guard.acquire(blocking=False):\n                try:\n                    return fnc(*args, **kwargs)\n                finally:\n                    fnc_guard.release()\n            else:\n                raise RuntimeError('exclusive call to %s violated')\n        return exclusive_call\n    return make_exclusive", "target": "Mark a callable as exclusive\n\n    :param via: factory for a Lock to guard the callable\n\n    Guards the callable against being entered again before completion.\n    Explicitly raises a :py:exc:`RuntimeError` on violation.\n\n    :note: If applied to a method, it is exclusive across all instances."}
{"task_id": "codesearchnet/python/train/68", "input": "def service(flavour):\n    r\"\"\"\n    Mark a class as implementing a Service\n\n    Each Service class must have a ``run`` method, which does not take any arguments.\n    This method is :py:meth:`~.ServiceRunner.adopt`\\ ed after the daemon starts, unless\n\n    * the Service has been garbage collected, or\n    * the ServiceUnit has been :py:meth:`~.ServiceUnit.cancel`\\ ed.\n\n    For each service instance, its :py:class:`~.ServiceUnit` is available at ``service_instance.__service_unit__``.\n    \"\"\"\n    def service_unit_decorator(raw_cls):\n        __new__ = raw_cls.__new__\n\n        def __new_service__(cls, *args, **kwargs):\n            if __new__ is object.__new__:\n                self = __new__(cls)\n            else:\n                self = __new__(cls, *args, **kwargs)\n            service_unit = ServiceUnit(self, flavour)\n            self.__service_unit__ = service_unit\n            return self\n\n        raw_cls.__new__ = __new_service__\n        if raw_cls.run.__doc__ is None:\n            raw_cls.run.__doc__ = \"Service entry point\"\n        return raw_cls\n    return service_unit_decorator", "target": "r\"\"\"\n    Mark a class as implementing a Service\n\n    Each Service class must have a ``run`` method, which does not take any arguments.\n    This method is :py:meth:`~.ServiceRunner.adopt`\\ ed after the daemon starts, unless\n\n    * the Service has been garbage collected, or\n    * the ServiceUnit has been :py:meth:`~.ServiceUnit.cancel`\\ ed.\n\n    For each service instance, its :py:class:`~.ServiceUnit` is available at ``service_instance.__service_unit__``."}
{"task_id": "codesearchnet/python/train/69", "input": "def execute(self, payload, *args, flavour: ModuleType, **kwargs):\n        \"\"\"\n        Synchronously run ``payload`` and provide its output\n\n        If ``*args*`` and/or ``**kwargs`` are provided, pass them to ``payload`` upon execution.\n        \"\"\"\n        if args or kwargs:\n            payload = functools.partial(payload, *args, **kwargs)\n        return self._meta_runner.run_payload(payload, flavour=flavour)", "target": "Synchronously run ``payload`` and provide its output\n\n        If ``*args*`` and/or ``**kwargs`` are provided, pass them to ``payload`` upon execution."}
{"task_id": "codesearchnet/python/train/70", "input": "def adopt(self, payload, *args, flavour: ModuleType, **kwargs):\n        \"\"\"\n        Concurrently run ``payload`` in the background\n\n        If ``*args*`` and/or ``**kwargs`` are provided, pass them to ``payload`` upon execution.\n        \"\"\"\n        if args or kwargs:\n            payload = functools.partial(payload, *args, **kwargs)\n        self._meta_runner.register_payload(payload, flavour=flavour)", "target": "Concurrently run ``payload`` in the background\n\n        If ``*args*`` and/or ``**kwargs`` are provided, pass them to ``payload`` upon execution."}
{"task_id": "codesearchnet/python/train/71", "input": "def accept(self):\n        \"\"\"\n        Start accepting synchronous, asynchronous and service payloads\n\n        Since services are globally defined, only one :py:class:`ServiceRunner`\n        may :py:meth:`accept` payloads at any time.\n        \"\"\"\n        if self._meta_runner:\n            raise RuntimeError('payloads scheduled for %s before being started' % self)\n        self._must_shutdown = False\n        self._logger.info('%s starting', self.__class__.__name__)\n        # force collecting objects so that defunct, migrated and overwritten services are destroyed now\n        gc.collect()\n        self._adopt_services()\n        self.adopt(self._accept_services, flavour=trio)\n        self._meta_runner.run()", "target": "Start accepting synchronous, asynchronous and service payloads\n\n        Since services are globally defined, only one :py:class:`ServiceRunner`\n        may :py:meth:`accept` payloads at any time."}
{"task_id": "codesearchnet/python/train/72", "input": "def shutdown(self):\n        \"\"\"Shutdown the accept loop and stop running payloads\"\"\"\n        self._must_shutdown = True\n        self._is_shutdown.wait()\n        self._meta_runner.stop()", "target": "Shutdown the accept loop and stop running payloads"}
{"task_id": "codesearchnet/python/train/73", "input": "def milestones(ctx, list, close):\n    \"\"\"View/edit/close milestones on github\n    \"\"\"\n    repos = get_repos(ctx.parent.agile.get('labels'))\n    if list:\n        _list_milestones(repos)\n    elif close:\n        click.echo('Closing milestones \"%s\"' % close)\n        _close_milestone(repos, close)\n    else:\n        click.echo(ctx.get_help())", "target": "View/edit/close milestones on github"}
{"task_id": "codesearchnet/python/train/74", "input": "def start_console(local_vars={}):\n    '''Starts a console; modified from code.interact'''\n    transforms.CONSOLE_ACTIVE = True\n    transforms.remove_not_allowed_in_console()\n    sys.ps1 = prompt\n    console = ExperimentalInteractiveConsole(locals=local_vars)\n    console.interact(banner=banner)", "target": "Starts a console; modified from code.interact"}
{"task_id": "codesearchnet/python/train/75", "input": "def push(self, line):\n        \"\"\"Transform and push a line to the interpreter.\n\n        The line should not have a trailing newline; it may have\n        internal newlines.  The line is appended to a buffer and the\n        interpreter's runsource() method is called with the\n        concatenated contents of the buffer as source.  If this\n        indicates that the command was executed or invalid, the buffer\n        is reset; otherwise, the command is incomplete, and the buffer\n        is left as it was after the line was appended.  The return\n        value is 1 if more input is required, 0 if the line was dealt\n        with in some way (this is the same as runsource()).\n\n        \"\"\"\n        if transforms.FROM_EXPERIMENTAL.match(line):\n            transforms.add_transformers(line)\n            self.buffer.append(\"\\n\")\n        else:\n            self.buffer.append(line)\n\n        add_pass = False\n        if line.rstrip(' ').endswith(\":\"):\n            add_pass = True\n        source = \"\\n\".join(self.buffer)\n        if add_pass:\n            source += \"pass\"\n        source = transforms.transform(source)\n        if add_pass:\n            source = source.rstrip(' ')\n            if source.endswith(\"pass\"):\n                source = source[:-4]\n\n        # some transformations may strip an empty line meant to end a block\n        if not self.buffer[-1]:\n            source += \"\\n\"\n        try:\n            more = self.runsource(source, self.filename)\n        except SystemExit:\n            os._exit(1)\n\n        if not more:\n            self.resetbuffer()\n        return more", "target": "Transform and push a line to the interpreter.\n\n        The line should not have a trailing newline; it may have\n        internal newlines.  The line is appended to a buffer and the\n        interpreter's runsource() method is called with the\n        concatenated contents of the buffer as source.  If this\n        indicates that the command was executed or invalid, the buffer\n        is reset; otherwise, the command is incomplete, and the buffer\n        is left as it was after the line was appended.  The return\n        value is 1 if more input is required, 0 if the line was dealt\n        with in some way (this is the same as runsource())."}
{"task_id": "codesearchnet/python/train/76", "input": "def dump(obj, f, preserve=False):\n    \"\"\"Write dict object into file\n\n    :param obj: the object to be dumped into toml\n    :param f: the file object\n    :param preserve: optional flag to preserve the inline table in result\n    \"\"\"\n    if not f.write:\n        raise TypeError('You can only dump an object into a file object')\n    encoder = Encoder(f, preserve=preserve)\n    return encoder.write_dict(obj)", "target": "Write dict object into file\n\n    :param obj: the object to be dumped into toml\n    :param f: the file object\n    :param preserve: optional flag to preserve the inline table in result"}
{"task_id": "codesearchnet/python/train/77", "input": "def dumps(obj, preserve=False):\n    \"\"\"Stringifies a dict as toml\n\n    :param obj: the object to be dumped into toml\n    :param preserve: optional flag to preserve the inline table in result\n    \"\"\"\n    f = StringIO()\n    dump(obj, f, preserve)\n    return f.getvalue()", "target": "Stringifies a dict as toml\n\n    :param obj: the object to be dumped into toml\n    :param preserve: optional flag to preserve the inline table in result"}
{"task_id": "codesearchnet/python/train/78", "input": "def license_loader(lic_dir=LIC_DIR):\n    \"\"\"Loads licenses from the given directory.\"\"\"\n    lics = []\n    for ln in os.listdir(lic_dir):\n        lp = os.path.join(lic_dir, ln)\n        with open(lp) as lf:\n            txt = lf.read()\n            lic = License(txt)\n            lics.append(lic)\n    return lics", "target": "Loads licenses from the given directory."}
{"task_id": "codesearchnet/python/train/79", "input": "def get_vector(self, max_choice=3):\n        \"\"\"Return pseudo-choice vectors.\"\"\"\n        vec = {}\n        for dim in ['forbidden', 'required', 'permitted']:\n            if self.meta[dim] is None:\n                continue\n            dim_vec = map(lambda x: (x, max_choice), self.meta[dim])\n            vec[dim] = dict(dim_vec)\n        return vec", "target": "Return pseudo-choice vectors."}
{"task_id": "codesearchnet/python/train/80", "input": "def entity(ctx, debug, uncolorize, **kwargs):\n    \"\"\"\n    CLI for tonomi.com using contrib-python-qubell-client\n\n    To enable completion:\n\n      eval \"$(_NOMI_COMPLETE=source nomi)\"\n    \"\"\"\n    global PROVIDER_CONFIG\n\n    if debug:\n        log.basicConfig(level=log.DEBUG)\n        log.getLogger(\"requests.packages.urllib3.connectionpool\").setLevel(log.DEBUG)\n    for (k, v) in kwargs.iteritems():\n        if v:\n            QUBELL[k] = v\n    PROVIDER_CONFIG = {\n        'configuration.provider': PROVIDER['provider_type'],\n        'configuration.legacy-regions': PROVIDER['provider_region'],\n        'configuration.endpoint-url': '',\n        'configuration.legacy-security-group': '',\n        'configuration.identity': PROVIDER['provider_identity'],\n        'configuration.credential': PROVIDER['provider_credential']\n    }\n\n    class UserContext(object):\n        def __init__(self):\n            self.platform = None\n            self.unauthenticated_platform = None\n            self.colorize = not (uncolorize)\n\n        def get_platform(self):\n            if not self.platform:\n                assert QUBELL[\"tenant\"], \"No platform URL provided. Set QUBELL_TENANT or use --tenant option.\"\n                if not QUBELL[\"token\"]:\n                    assert QUBELL[\"user\"], \"No username. Set QUBELL_USER or use --user option.\"\n                    assert QUBELL[\"password\"], \"No password provided. Set QUBELL_PASSWORD or use --password option.\"\n\n                self.platform = QubellPlatform.connect(\n                    tenant=QUBELL[\"tenant\"],\n                    user=QUBELL[\"user\"],\n                    password=QUBELL[\"password\"],\n                    token=QUBELL[\"token\"])\n            return self.platform\n\n        def get_unauthenticated_platform(self):\n            if not self.unauthenticated_platform:\n                assert QUBELL[\"tenant\"], \"No platform URL provided. Set QUBELL_TENANT or use --tenant option.\"\n\n                self.unauthenticated_platform = QubellPlatform.connect(tenant=QUBELL[\"tenant\"])\n\n            return self.unauthenticated_platform\n\n    ctx = click.get_current_context()\n    ctx.obj = UserContext()", "target": "CLI for tonomi.com using contrib-python-qubell-client\n\n    To enable completion:\n\n      eval \"$(_NOMI_COMPLETE=source nomi)\""}
{"task_id": "codesearchnet/python/train/81", "input": "def import_app(files, category, overwrite, id, name):\n    \"\"\" Upload application from file.\n\n    By default, file name will be used as application name, with \"-vXX.YYY\" suffix stripped.\n    Application is looked up by one of these classifiers, in order of priority:\n    app-id, app-name, filename.\n\n    If app-id is provided, looks up existing application and updates its manifest.\n    If app-id is NOT specified, looks up by name, or creates new application.\n\n    \"\"\"\n    platform = _get_platform()\n    org = platform.get_organization(QUBELL[\"organization\"])\n    if category:\n        category = org.categories[category]\n    regex = re.compile(r\"^(.*?)(-v(\\d+)|)\\.[^.]+$\")\n    if (id or name) and len(files) > 1:\n        raise Exception(\"--id and --name are supported only for single-file mode\")\n\n    for filename in files:\n        click.echo(\"Importing \" + filename, nl=False)\n        if not name:\n            match = regex.match(basename(filename))\n            if not match:\n                click.echo(_color(\"RED\", \"FAIL\") + \" unknown filename format\")\n                break\n            name = regex.match(basename(filename)).group(1)\n        click.echo(\" => \", nl=False)\n        app = None\n        try:\n            app = org.get_application(id=id, name=name)\n            if app and not overwrite:\n                click.echo(\"%s %s already exists %s\" % (\n                    app.id, _color(\"BLUE\", app and app.name or name), _color(\"RED\", \"FAIL\")))\n                break\n        except NotFoundError:\n            if id:\n                click.echo(\"%s %s not found %s\" % (\n                    id or \"\", _color(\"BLUE\", app and app.name or name), _color(\"RED\", \"FAIL\")))\n                break\n        click.echo(_color(\"BLUE\", app and app.name or name) + \" \", nl=False)\n        try:\n            with file(filename, \"r\") as f:\n                if app:\n                    app.update(name=app.name,\n                               category=category and category.id or app.category,\n                               manifest=Manifest(content=f.read()))\n                else:\n                    app = org.application(id=id, name=name, manifest=Manifest(content=f.read()))\n                    if category:\n                        app.update(category=category.id)\n            click.echo(app.id + _color(\"GREEN\", \" OK\"))\n        except IOError as e:\n            click.echo(_color(\"RED\", \" FAIL\") + \" \" + e.message)\n            break", "target": "Upload application from file.\n\n    By default, file name will be used as application name, with \"-vXX.YYY\" suffix stripped.\n    Application is looked up by one of these classifiers, in order of priority:\n    app-id, app-name, filename.\n\n    If app-id is provided, looks up existing application and updates its manifest.\n    If app-id is NOT specified, looks up by name, or creates new application."}
{"task_id": "codesearchnet/python/train/82", "input": "def show_account():\n    \"\"\"\n    Exports current account configuration in\n    shell-friendly form. Takes into account\n    explicit top-level flags like --organization.\n    \"\"\"\n    click.echo(\"# tonomi api\")\n    for (key, env) in REVERSE_MAPPING.items():\n        value = QUBELL.get(key, None)\n        if value:\n            click.echo(\"export %s='%s'\" % (env, value))\n    if any(map(lambda x: PROVIDER.get(x), REVERSE_PROVIDER_MAPPING.keys())):\n        click.echo(\"# cloud account\")\n        for (key, env) in REVERSE_PROVIDER_MAPPING.items():\n            value = PROVIDER.get(key, None)\n            if value:\n                click.echo(\"export %s='%s'\" % (env, value))", "target": "Exports current account configuration in\n    shell-friendly form. Takes into account\n    explicit top-level flags like --organization."}
{"task_id": "codesearchnet/python/train/83", "input": "def generate_session_token(refresh_token, verbose):\n    \"\"\"\n    Generates new session token from the given refresh token.\n    :param refresh_token: refresh token to generate from\n    :param verbose: whether expiration time should be added to output\n    \"\"\"\n\n    platform = _get_platform(authenticated=False)\n    session_token, expires_in = platform.generate_session_token(refresh_token)\n\n    if verbose:\n        click.echo(\"%s\\n\\n%s\" % (session_token, _color('YELLOW', \"Expires in %d seconds\" % expires_in)))\n    else:\n        click.echo(session_token)", "target": "Generates new session token from the given refresh token.\n    :param refresh_token: refresh token to generate from\n    :param verbose: whether expiration time should be added to output"}
{"task_id": "codesearchnet/python/train/84", "input": "def runcommand(cosmology='WMAP5'):\n    \"\"\" Example interface commands \"\"\"\n\n    # Return the WMAP5 cosmology concentration predicted for\n    # z=0 range of masses\n    Mi = [1e8, 1e9, 1e10]\n    zi = 0\n    print(\"Concentrations for haloes of mass %s at z=%s\" % (Mi, zi))\n    output = commah.run(cosmology=cosmology, zi=zi, Mi=Mi)\n\n    print(output['c'].flatten())\n\n    # Return the WMAP5 cosmology concentration predicted for\n    # z=0 range of masses AND cosmological parameters\n    Mi = [1e8, 1e9, 1e10]\n    zi = 0\n    print(\"Concentrations for haloes of mass %s at z=%s\" % (Mi, zi))\n    output, cosmo = commah.run(cosmology=cosmology, zi=zi, Mi=Mi,\n                               retcosmo=True)\n\n    print(output['c'].flatten())\n    print(cosmo)\n\n    # Return the WMAP5 cosmology concentration predicted for MW\n    # mass (2e12 Msol) across redshift\n    Mi = 2e12\n    z = [0, 0.5, 1, 1.5, 2, 2.5]\n    output = commah.run(cosmology=cosmology, zi=0, Mi=Mi, z=z)\n    for zval in z:\n        print(\"M(z=0)=%s has c(z=%s)=%s\"\n              % (Mi, zval, output[output['z'] == zval]['c'].flatten()))\n\n    # Return the WMAP5 cosmology concentration predicted for MW\n    # mass (2e12 Msol) across redshift\n    Mi = 2e12\n    zi = [0, 0.5, 1, 1.5, 2, 2.5]\n    output = commah.run(cosmology=cosmology, zi=zi, Mi=Mi)\n    for zval in zi:\n        print(\"M(z=%s)=%s has concentration %s\"\n              % (zval, Mi, output[(output['zi'] == zval) &\n                                  (output['z'] == zval)]['c'].flatten()))\n\n    # Return the WMAP5 cosmology concentration and\n    # rarity of high-z cluster\n    Mi = 2e14\n    zi = 6\n    output = commah.run(cosmology=cosmology, zi=zi, Mi=Mi)\n    print(\"Concentrations for haloes of mass %s at z=%s\" % (Mi, zi))\n    print(output['c'].flatten())\n    print(\"Mass variance sigma of haloes of mass %s at z=%s\" % (Mi, zi))\n    print(output['sig'].flatten())\n    print(\"Fluctuation for haloes of mass %s at z=%s\" % (Mi, zi))\n    print(output['nu'].flatten())\n\n    # Return the WMAP5 cosmology accretion rate prediction\n    # for haloes at range of redshift and mass\n    Mi = [1e8, 1e9, 1e10]\n    zi = [0]\n    z = [0, 0.5, 1, 1.5, 2, 2.5]\n    output = commah.run(cosmology=cosmology, zi=zi, Mi=Mi, z=z)\n    for Mval in Mi:\n        print(\"dM/dt for halo of mass %s at z=%s across redshift %s is: \"\n              % (Mval, zi, z))\n        print(output[output['Mi'] == Mval]['dMdt'].flatten())\n\n    # Return the WMAP5 cosmology Halo Mass History for haloes with M(z=0) = 1e8\n    M = [1e8]\n    z = [0, 0.5, 1, 1.5, 2, 2.5]\n    print(\"Halo Mass History for z=0 mass of %s across z=%s\" % (M, z))\n    output = commah.run(cosmology=cosmology, zi=0, Mi=M, z=z)\n    print(output['Mz'].flatten())\n\n    # Return the WMAP5 cosmology formation redshifts for haloes at\n    # range of redshift and mass\n    M = [1e8, 1e9, 1e10]\n    z = [0]\n    print(\"Formation Redshifts for haloes of mass %s at z=%s\" % (M, z))\n    output = commah.run(cosmology=cosmology, zi=0, Mi=M, z=z)\n    for Mval in M:\n        print(output[output['Mi'] == Mval]['zf'].flatten())\n\n    return(\"Done\")", "target": "Example interface commands"}
{"task_id": "codesearchnet/python/train/85", "input": "def plotcommand(cosmology='WMAP5', plotname=None):\n    \"\"\" Example ways to interrogate the dataset and plot the commah output \"\"\"\n\n    # Plot the c-M relation as a functon of redshift\n    xarray = 10**(np.arange(1, 15, 0.2))\n    yval = 'c'\n\n    # Specify the redshift range\n    zarray = np.arange(0, 5, 0.5)\n\n    xtitle = r\"Halo Mass (M$_{sol}$)\"\n    ytitle = r\"Concentration\"\n    linelabel = \"z=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    plt.ylim([2, 30])\n\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=zval, Mi=xarray)\n\n        # Access the column yval from the data file\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, yarray, label=linelabel+str(zval), color=colors[zind])\n        # Overplot the D08 predictions in black\n        ax.plot(xarray, commah.commah.cduffy(zval, xarray), color=\"black\")\n\n    ax.set_xscale('log')\n    ax.set_yscale('log')\n\n    leg = ax.legend(loc=1)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_CM_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_CM_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the c-z relation as a function of mass (so always Mz=M0)\n    xarray = 10**(np.arange(0, 1, 0.05)) - 1\n    yval = 'c'\n\n    # Specify the mass range\n    zarray = 10**np.arange(6, 14, 2)\n\n    xtitle = r\"Redshift\"\n    ytitle = r\"NFW Concentration\"\n    linelabel = r\"log$_{10}$ M$_{z}$(M$_{sol}$)=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=xarray, Mi=zval)\n\n        # Access the column yval from the data file\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colours\n        ax.plot(xarray, yarray,\n                label=linelabel+\"{0:.1f}\".format(np.log10(zval)),\n                color=colors[zind],)\n\n    leg = ax.legend(loc=1)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_Cz_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_Cz_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the zf-z relation for different masses (so always Mz=M0)\n    xarray = 10**(np.arange(0, 1, 0.05)) - 1\n    yval = 'zf'\n\n    # Specify the mass range\n    zarray = 10**np.arange(6, 14, 2)\n\n    xtitle = r\"Redshift\"\n    ytitle = r\"Formation Redshift\"\n    linelabel = r\"log$_{10}$ M$_{z}$(M$_{sol}$)=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=xarray, Mi=zval)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, yarray,\n                label=linelabel+\"{0:.1f}\".format(np.log10(zval)),\n                color=colors[zind],)\n\n    leg = ax.legend(loc=2)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_zfz_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_zfz_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the dM/dt-z relation for different masses (so always Mz=M0)\n    xarray = 10**(np.arange(0, 1, 0.05)) - 1\n    yval = 'dMdt'\n\n    # Specify the mass range\n    zarray = 10**np.arange(10, 14, 0.5)\n\n    xtitle = r\"log$_{10}$ (1+z)\"\n    ytitle = r\"log$_{10}$ Accretion Rate M$_{sol}$ yr$^{-1}$\"\n    linelabel = r\"log$_{10}$ M$_z$(M$_{sol}$)=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    cosmo = commah.getcosmo(cosmology)\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=xarray, Mi=zval,\n                            com=False, mah=True)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(np.log10(xarray+1.), np.log10(yarray),\n                label=linelabel+\"{0:.1f}\".format(np.log10(zval)),\n                color=colors[zind],)\n\n        # Plot the semi-analytic approximate formula from Correa et al 2015b\n        semianalytic_approx = 71.6 * (zval / 1e12) * (cosmo['h'] / 0.7) *\\\n            (-0.24 + 0.75 * (xarray + 1)) * np.sqrt(\n            cosmo['omega_M_0'] * (xarray + 1)**3 + cosmo['omega_lambda_0'])\n\n        ax.plot(np.log10(xarray + 1), np.log10(semianalytic_approx),\n                color='black')\n\n    leg = ax.legend(loc=2)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_dMdtz_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_dMdtz_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the dMdt-M relation as a function of redshift\n    xarray = 10**(np.arange(10, 14, 0.5))\n    yval = 'dMdt'\n\n    # Specify the redshift range\n    zarray = np.arange(0, 5, 0.5)\n\n    xtitle = r\"Halo Mass M$_{sol}$\"\n    ytitle = r\"Accretion Rate M$_{sol}$ yr$^{-1}$\"\n    linelabel = \"z=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=zval, Mi=xarray,\n                            com=False, mah=True)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, yarray, label=linelabel+str(zval),\n                color=colors[zind],)\n\n    ax.set_xscale('log')\n    ax.set_yscale('log')\n\n    leg = ax.legend(loc=2)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_MAH_M_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_MAH_M_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the (dM/M)dt-M relation as a function of redshift\n    xarray = 10**(np.arange(10, 14, 0.5))\n    yval = 'dMdt'\n\n    # Specify the redshift range\n    zarray = np.arange(0, 5, 0.5)\n\n    xtitle = r\"Halo Mass M$_{sol}$\"\n    ytitle = r\"Specific Accretion Rate yr$^{-1}$\"\n    linelabel = \"z=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=zval, Mi=xarray,\n                            mah=True, com=False)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, yarray/xarray, label=linelabel+str(zval),\n                color=colors[zind],)\n\n    ax.set_xscale('log')\n    ax.set_yscale('log')\n\n    leg = ax.legend(loc=1)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_specificMAH_M_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_specificMAH_M_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the Mz-z relation as a function of mass\n    # (so mass is decreasing to zero as z-> inf)\n    xarray = 10**(np.arange(0, 1, 0.05)) - 1\n    yval = 'Mz'\n\n    # Specify the mass range\n    zarray = 10**np.arange(10, 14, 0.5)\n\n    xtitle = r\"Redshift\"\n    ytitle = r\"M(z) (M$_{sol}$)\"\n    linelabel = r\"log$_{10}$ M$_{0}$(M$_{sol}$)=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=0, Mi=zval, z=xarray)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, yarray,\n                label=linelabel+\"{0:.1f}\".format(np.log10(zval)),\n                color=colors[zind],)\n\n    ax.set_yscale('log')\n\n    leg = ax.legend(loc=1)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_Mzz_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_Mzz_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the Mz/M0-z relation as a function of mass\n    xarray = 10**(np.arange(0, 1, 0.02)) - 1\n    yval = 'Mz'\n\n    # Specify the mass range\n    zarray = 10**np.arange(10, 14, 0.5)\n\n    xtitle = r\"Redshift\"\n    ytitle = r\"log$_{10}$ M(z)/M$_{0}$\"\n    linelabel = r\"log$_{10}$ M$_{0}$(M$_{sol}$)=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=0, Mi=zval, z=xarray)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, np.log10(yarray/zval),\n                label=linelabel+\"{0:.1f}\".format(np.log10(zval)),\n                color=colors[zind],)\n\n    leg = ax.legend(loc=3)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_MzM0z_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_MzM0z_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    return(\"Done\")", "target": "Example ways to interrogate the dataset and plot the commah output"}
{"task_id": "codesearchnet/python/train/86", "input": "def enhance(self):\n        \"\"\" Function enhance\n        Enhance the object with new item or enhanced items\n        \"\"\"\n        self.update({'puppetclasses':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemPuppetClasses)})\n        self.update({'parameters':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemParameter)})\n        self.update({'smart_class_parameters':\n                    SubDict(self.api, self.objName,\n                            self.payloadObj, self.key,\n                            ItemSmartClassParameter)})", "target": "Function enhance\n        Enhance the object with new item or enhanced items"}
{"task_id": "codesearchnet/python/train/87", "input": "def add_transformers(line):\n    '''Extract the transformers names from a line of code of the form\n       from __experimental__ import transformer1 [,...]\n       and adds them to the globally known dict\n    '''\n    assert FROM_EXPERIMENTAL.match(line)\n\n    line = FROM_EXPERIMENTAL.sub(' ', line)\n    # we now have: \" transformer1 [,...]\"\n    line = line.split(\"#\")[0]    # remove any end of line comments\n    # and insert each transformer as an item in a list\n    for trans in line.replace(' ', '').split(','):\n        import_transformer(trans)", "target": "Extract the transformers names from a line of code of the form\n       from __experimental__ import transformer1 [,...]\n       and adds them to the globally known dict"}
{"task_id": "codesearchnet/python/train/88", "input": "def import_transformer(name):\n    '''If needed, import a transformer, and adds it to the globally known dict\n       The code inside a module where a transformer is defined should be\n       standard Python code, which does not need any transformation.\n       So, we disable the import hook, and let the normal module import\n       do its job - which is faster and likely more reliable than our\n       custom method.\n    '''\n    if name in transformers:\n        return transformers[name]\n\n    # We are adding a transformer built from normal/standard Python code.\n    # As we are not performing transformations, we temporarily disable\n    # our import hook, both to avoid potential problems AND because we\n    # found that this resulted in much faster code.\n    hook = sys.meta_path[0]\n    sys.meta_path = sys.meta_path[1:]\n    try:\n        transformers[name] = __import__(name)\n        # Some transformers are not allowed in the console.\n        # If an attempt is made to activate one of them in the console,\n        # we replace it by a transformer that does nothing and print a\n        # message specific to that transformer as written in its module.\n        if CONSOLE_ACTIVE:\n            if hasattr(transformers[name], \"NO_CONSOLE\"):\n                print(transformers[name].NO_CONSOLE)\n                transformers[name] = NullTransformer()\n    except ImportError:\n        sys.stderr.write(\"Warning: Import Error in add_transformers: %s not found\\n\" % name)\n        transformers[name] = NullTransformer()\n    except Exception as e:\n        sys.stderr.write(\"Unexpected exception in transforms.import_transformer%s\\n \" %\n                         e.__class__.__name__)\n    finally:\n        sys.meta_path.insert(0, hook) # restore import hook\n\n    return transformers[name]", "target": "If needed, import a transformer, and adds it to the globally known dict\n       The code inside a module where a transformer is defined should be\n       standard Python code, which does not need any transformation.\n       So, we disable the import hook, and let the normal module import\n       do its job - which is faster and likely more reliable than our\n       custom method."}
{"task_id": "codesearchnet/python/train/89", "input": "def extract_transformers_from_source(source):\n    '''Scan a source for lines of the form\n       from __experimental__ import transformer1 [,...]\n       identifying transformers to be used. Such line is passed to the\n       add_transformer function, after which it is removed from the\n       code to be executed.\n    '''\n    lines = source.split('\\n')\n    linenumbers = []\n    for number, line in enumerate(lines):\n        if FROM_EXPERIMENTAL.match(line):\n            add_transformers(line)\n            linenumbers.insert(0, number)\n\n    # drop the \"fake\" import from the source code\n    for number in linenumbers:\n        del lines[number]\n    return '\\n'.join(lines)", "target": "Scan a source for lines of the form\n       from __experimental__ import transformer1 [,...]\n       identifying transformers to be used. Such line is passed to the\n       add_transformer function, after which it is removed from the\n       code to be executed."}
{"task_id": "codesearchnet/python/train/90", "input": "def remove_not_allowed_in_console():\n    '''This function should be called from the console, when it starts.\n\n    Some transformers are not allowed in the console and they could have\n    been loaded prior to the console being activated. We effectively remove them\n    and print an information message specific to that transformer\n    as written in the transformer module.\n\n    '''\n    not_allowed_in_console = []\n    if CONSOLE_ACTIVE:\n        for name in transformers:\n            tr_module = import_transformer(name)\n            if hasattr(tr_module, \"NO_CONSOLE\"):\n                not_allowed_in_console.append((name, tr_module))\n        for name, tr_module in not_allowed_in_console:\n            print(tr_module.NO_CONSOLE)\n            # Note: we do not remove them, so as to avoid seeing the\n            # information message displayed again if an attempt is\n            # made to re-import them from a console instruction.\n            transformers[name] = NullTransformer()", "target": "This function should be called from the console, when it starts.\n\n    Some transformers are not allowed in the console and they could have\n    been loaded prior to the console being activated. We effectively remove them\n    and print an information message specific to that transformer\n    as written in the transformer module."}
{"task_id": "codesearchnet/python/train/91", "input": "def transform(source):\n    '''Used to convert the source code, making use of known transformers.\n\n       \"transformers\" are modules which must contain a function\n\n           transform_source(source)\n\n       which returns a tranformed source.\n       Some transformers (for example, those found in the standard library\n       module lib2to3) cannot cope with non-standard syntax; as a result, they\n       may fail during a first attempt. We keep track of all failing\n       transformers and keep retrying them until either they all succeeded\n       or a fixed set of them fails twice in a row.\n    '''\n    source = extract_transformers_from_source(source)\n\n    # Some transformer fail when multiple non-Python constructs\n    # are present. So, we loop multiple times keeping track of\n    # which transformations have been unsuccessfully performed.\n    not_done = transformers\n    while True:\n        failed = {}\n        for name in not_done:\n            tr_module = import_transformer(name)\n            try:\n                source = tr_module.transform_source(source)\n            except Exception as e:\n                failed[name] = tr_module\n                # from traceback import print_exc\n                # print(\"Unexpected exception in transforms.transform\",\n                #       e.__class__.__name__)\n                # print_exc()\n\n        if not failed:\n            break\n        # Insanity is doing the same Tting over and overaAgain and\n        # expecting different results ...\n        # If the exact same set of transformations are not performed\n        # twice in a row, there is no point in trying out a third time.\n        if failed == not_done:\n            print(\"Warning: the following transforms could not be done:\")\n            for key in failed:\n                print(key)\n            break\n        not_done = failed  # attempt another pass\n\n    return source", "target": "Used to convert the source code, making use of known transformers.\n\n       \"transformers\" are modules which must contain a function\n\n           transform_source(source)\n\n       which returns a tranformed source.\n       Some transformers (for example, those found in the standard library\n       module lib2to3) cannot cope with non-standard syntax; as a result, they\n       may fail during a first attempt. We keep track of all failing\n       transformers and keep retrying them until either they all succeeded\n       or a fixed set of them fails twice in a row."}
{"task_id": "codesearchnet/python/train/92", "input": "def _match(self, request, response):\n        \"\"\"Match all requests/responses that satisfy the following conditions:\n\n        * An Admin App; i.e. the path is something like /admin/some_app/\n        * The ``include_flag`` is not in the response's content\n\n        \"\"\"\n        is_html = 'text/html' in response.get('Content-Type', '')\n        if is_html and hasattr(response, 'rendered_content'):\n            correct_path = PATH_MATCHER.match(request.path) is not None\n            not_included = self.include_flag not in response.rendered_content\n            return correct_path and not_included\n        return False", "target": "Match all requests/responses that satisfy the following conditions:\n\n        * An Admin App; i.e. the path is something like /admin/some_app/\n        * The ``include_flag`` is not in the response's content"}
{"task_id": "codesearchnet/python/train/93", "input": "def _chosen_css(self):\n        \"\"\"Read the minified CSS file including STATIC_URL in the references\n        to the sprite images.\"\"\"\n        css = render_to_string(self.css_template, {})\n        for sprite in self.chosen_sprites:  # rewrite path to sprites in the css\n            css = css.replace(sprite, settings.STATIC_URL + \"img/\" + sprite)\n        return css", "target": "Read the minified CSS file including STATIC_URL in the references\n        to the sprite images."}
{"task_id": "codesearchnet/python/train/94", "input": "def _embed(self, request, response):\n        \"\"\"Embed Chosen.js directly in html of the response.\"\"\"\n        if self._match(request, response):\n            # Render the <link> and the <script> tags to include Chosen.\n            head = render_to_string(\n                \"chosenadmin/_head_css.html\",\n                {\"chosen_css\": self._chosen_css()}\n            )\n            body = render_to_string(\n                \"chosenadmin/_script.html\",\n                {\"chosen_js\": self._chosen_js()}\n            )\n\n            # Re-write the Response's content to include our new html\n            content = response.rendered_content\n            content = content.replace('</head>', head)\n            content = content.replace('</body>', body)\n            response.content = content\n        return response", "target": "Embed Chosen.js directly in html of the response."}
{"task_id": "codesearchnet/python/train/95", "input": "def clean_up(self):\n        \"\"\"\n        Close the I2C bus\n        \"\"\"\n        self.log.debug(\"Closing I2C bus for address: 0x%02X\" % self.address)\n        self.bus.close()", "target": "Close the I2C bus"}
{"task_id": "codesearchnet/python/train/96", "input": "def write_quick(self):\n        \"\"\"\n        Send only the read / write bit\n        \"\"\"\n        self.bus.write_quick(self.address)\n        self.log.debug(\"write_quick: Sent the read / write bit\")", "target": "Send only the read / write bit"}
{"task_id": "codesearchnet/python/train/97", "input": "def write_byte(self, cmd, value):\n        \"\"\"\n        Writes an 8-bit byte to the specified command register\n        \"\"\"\n        self.bus.write_byte_data(self.address, cmd, value)\n        self.log.debug(\n            \"write_byte: Wrote 0x%02X to command register 0x%02X\" % (\n                value, cmd\n            )\n        )", "target": "Writes an 8-bit byte to the specified command register"}
{"task_id": "codesearchnet/python/train/98", "input": "def write_word(self, cmd, value):\n        \"\"\"\n        Writes a 16-bit word to the specified command register\n        \"\"\"\n        self.bus.write_word_data(self.address, cmd, value)\n        self.log.debug(\n            \"write_word: Wrote 0x%04X to command register 0x%02X\" % (\n                value, cmd\n            )\n        )", "target": "Writes a 16-bit word to the specified command register"}
{"task_id": "codesearchnet/python/train/99", "input": "def write_raw_byte(self, value):\n        \"\"\"\n        Writes an 8-bit byte directly to the bus\n        \"\"\"\n        self.bus.write_byte(self.address, value)\n        self.log.debug(\"write_raw_byte: Wrote 0x%02X\" % value)", "target": "Writes an 8-bit byte directly to the bus"}
{"task_id": "codesearchnet/python/train/100", "input": "def write_block_data(self, cmd, block):\n        \"\"\"\n        Writes a block of bytes to the bus using I2C format to the specified\n        command register\n        \"\"\"\n        self.bus.write_i2c_block_data(self.address, cmd, block)\n        self.log.debug(\n            \"write_block_data: Wrote [%s] to command register 0x%02X\" % (\n                ', '.join(['0x%02X' % x for x in block]),\n                cmd\n            )\n        )", "target": "Writes a block of bytes to the bus using I2C format to the specified\n        command register"}
{"task_id": "codesearchnet/python/train/101", "input": "def read_raw_byte(self):\n        \"\"\"\n        Read an 8-bit byte directly from the bus\n        \"\"\"\n        result = self.bus.read_byte(self.address)\n        self.log.debug(\"read_raw_byte: Read 0x%02X from the bus\" % result)\n        return result", "target": "Read an 8-bit byte directly from the bus"}
{"task_id": "codesearchnet/python/train/102", "input": "def read_block_data(self, cmd, length):\n        \"\"\"\n        Read a block of bytes from the bus from the specified command register\n        Amount of bytes read in is defined by length\n        \"\"\"\n        results = self.bus.read_i2c_block_data(self.address, cmd, length)\n        self.log.debug(\n            \"read_block_data: Read [%s] from command register 0x%02X\" % (\n                ', '.join(['0x%02X' % x for x in results]),\n                cmd\n            )\n        )\n        return results", "target": "Read a block of bytes from the bus from the specified command register\n        Amount of bytes read in is defined by length"}
{"task_id": "codesearchnet/python/train/103", "input": "def read_unsigned_byte(self, cmd):\n        \"\"\"\n        Read an unsigned byte from the specified command register\n        \"\"\"\n        result = self.bus.read_byte_data(self.address, cmd)\n        self.log.debug(\n            \"read_unsigned_byte: Read 0x%02X from command register 0x%02X\" % (\n                result, cmd\n            )\n        )\n        return result", "target": "Read an unsigned byte from the specified command register"}
{"task_id": "codesearchnet/python/train/104", "input": "def read_unsigned_word(self, cmd, little_endian=True):\n        \"\"\"\n        Read an unsigned word from the specified command register\n        We assume the data is in little endian mode, if it is in big endian\n        mode then set little_endian to False\n        \"\"\"\n        result = self.bus.read_word_data(self.address, cmd)\n\n        if not little_endian:\n            result = ((result << 8) & 0xFF00) + (result >> 8)\n\n        self.log.debug(\n            \"read_unsigned_word: Read 0x%04X from command register 0x%02X\" % (\n                result, cmd\n            )\n        )\n        return result", "target": "Read an unsigned word from the specified command register\n        We assume the data is in little endian mode, if it is in big endian\n        mode then set little_endian to False"}
{"task_id": "codesearchnet/python/train/105", "input": "def __connect_to_bus(self, bus):\n        \"\"\"\n        Attempt to connect to an I2C bus\n        \"\"\"\n        def connect(bus_num):\n            try:\n                self.log.debug(\"Attempting to connect to bus %s...\" % bus_num)\n                self.bus = smbus.SMBus(bus_num)\n                self.log.debug(\"Success\")\n            except IOError:\n                self.log.debug(\"Failed\")\n                raise\n\n        # If the bus is not explicitly stated, try 0 and then try 1 if that\n        # fails\n        if bus is None:\n            try:\n                connect(0)\n                return\n            except IOError:\n                pass\n\n            try:\n                connect(1)\n                return\n            except IOError:\n                raise\n        else:\n            try:\n                connect(bus)\n                return\n            except IOError:\n                raise", "target": "Attempt to connect to an I2C bus"}
{"task_id": "codesearchnet/python/train/106", "input": "def get_formset(self, request, obj=None, **kwargs):\n        \"\"\" Default user to the current version owner. \"\"\"\n        data = super().get_formset(request, obj, **kwargs)\n        if obj:\n            data.form.base_fields['user'].initial = request.user.id\n        return data", "target": "Default user to the current version owner."}
{"task_id": "codesearchnet/python/train/107", "input": "def reload(self):\n        \"\"\" Function reload\n        Reload the full object to ensure sync\n        \"\"\"\n        realData = self.load()\n        self.clear()\n        self.update(realData)", "target": "Function reload\n        Reload the full object to ensure sync"}
{"task_id": "codesearchnet/python/train/108", "input": "def updateAfterDecorator(function):\n        \"\"\" Function updateAfterDecorator\n        Decorator to ensure local dict is sync with remote foreman\n        \"\"\"\n        def _updateAfterDecorator(self, *args, **kwargs):\n            ret = function(self, *args, **kwargs)\n            self.reload()\n            return ret\n        return _updateAfterDecorator", "target": "Function updateAfterDecorator\n        Decorator to ensure local dict is sync with remote foreman"}
{"task_id": "codesearchnet/python/train/109", "input": "def updateBeforeDecorator(function):\n        \"\"\" Function updateAfterDecorator\n        Decorator to ensure local dict is sync with remote foreman\n        \"\"\"\n        def _updateBeforeDecorator(self, *args, **kwargs):\n            if self.forceFullSync:\n                self.reload()\n            return function(self, *args, **kwargs)\n        return _updateBeforeDecorator", "target": "Function updateAfterDecorator\n        Decorator to ensure local dict is sync with remote foreman"}
{"task_id": "codesearchnet/python/train/110", "input": "def load(self):\n        \"\"\" Function load\n        Get the list of all objects\n\n        @return RETURN: A ForemanItem list\n        \"\"\"\n        return {x[self.index]: self.itemType(self.api, x['id'],\n                                             self.objName, self.payloadObj,\n                                             x)\n                for x in self.api.list(self.objName,\n                                       limit=self.searchLimit)}", "target": "Function load\n        Get the list of all objects\n\n        @return RETURN: A ForemanItem list"}
{"task_id": "codesearchnet/python/train/111", "input": "def checkAndCreate(self, key, payload):\n        \"\"\" Function checkAndCreate\n        Check if an object exists and create it if not\n\n        @param key: The targeted object\n        @param payload: The targeted object description\n        @return RETURN: The id of the object\n        \"\"\"\n        if key not in self:\n            self[key] = payload\n        return self[key]['id']", "target": "Function checkAndCreate\n        Check if an object exists and create it if not\n\n        @param key: The targeted object\n        @param payload: The targeted object description\n        @return RETURN: The id of the object"}
{"task_id": "codesearchnet/python/train/112", "input": "def operations():\n    \"\"\"\n    Class decorator stores all calls into list.\n    Can be used until .invalidate() is called.\n    :return: decorated class\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapped_func(*args, **kwargs):\n\n            self = args[0]\n            assert self.__can_use, \"User operation queue only in 'with' block\"\n\n            def defaults_dict():\n                f_args, varargs, keywords, defaults = inspect.getargspec(func)\n                defaults = defaults or []\n                return dict(zip(f_args[-len(defaults)+len(args[1:]):], defaults[len(args[1:]):]))\n\n            route_args = dict(defaults_dict().items() + kwargs.items())\n\n            func(*args, **kwargs)\n            self.operations.append((func.__name__, args[1:], route_args, ))\n\n        return wrapped_func\n    def decorate(clazz):\n\n        for attr in clazz.__dict__:\n            if callable(getattr(clazz, attr)):\n                setattr(clazz, attr, decorator(getattr(clazz, attr)))\n        def __init__(self):  # simple parameter-less constructor\n            self.operations = []\n            self.__can_use = True\n        def invalidate(self):\n            self.__can_use = False\n        clazz.__init__ = __init__\n        clazz.invalidate = invalidate\n        return clazz\n    return decorate", "target": "Class decorator stores all calls into list.\n    Can be used until .invalidate() is called.\n    :return: decorated class"}
{"task_id": "codesearchnet/python/train/113", "input": "def process_actions(action_ids=None):\n    \"\"\"\n    Process actions in the publishing schedule.\n\n    Returns the number of actions processed.\n    \"\"\"\n    actions_taken = 0\n    action_list = PublishAction.objects.prefetch_related(\n        'content_object',\n    ).filter(\n        scheduled_time__lte=timezone.now(),\n    )\n\n    if action_ids is not None:\n        action_list = action_list.filter(id__in=action_ids)\n\n    for action in action_list:\n        action.process_action()\n        action.delete()\n        actions_taken += 1\n\n    return actions_taken", "target": "Process actions in the publishing schedule.\n\n    Returns the number of actions processed."}
{"task_id": "codesearchnet/python/train/114", "input": "def celery_enabled():\n    \"\"\"\n    Return a boolean if Celery tasks are enabled for this app.\n\n    If the ``GLITTER_PUBLISHER_CELERY`` setting is ``True`` or ``False`` - then that value will be\n    used. However if the setting isn't defined, then this will be enabled automatically if Celery\n    is installed.\n    \"\"\"\n    enabled = getattr(settings, 'GLITTER_PUBLISHER_CELERY', None)\n\n    if enabled is None:\n        try:\n            import celery  # noqa\n            enabled = True\n        except ImportError:\n            enabled = False\n\n    return enabled", "target": "Return a boolean if Celery tasks are enabled for this app.\n\n    If the ``GLITTER_PUBLISHER_CELERY`` setting is ``True`` or ``False`` - then that value will be\n    used. However if the setting isn't defined, then this will be enabled automatically if Celery\n    is installed."}
{"task_id": "codesearchnet/python/train/115", "input": "def checkAndCreate(self, key, payload):\n        \"\"\" Function checkAndCreate\n        Check if an object exists and create it if not\n\n        @param key: The targeted object\n        @param payload: The targeted object description\n        @return RETURN: The id of the object\n        \"\"\"\n        if key not in self:\n            if 'templates' in payload:\n                templates = payload.pop('templates')\n            self[key] = payload\n            self.reload()\n        return self[key]['id']", "target": "Function checkAndCreate\n        Check if an object exists and create it if not\n\n        @param key: The targeted object\n        @param payload: The targeted object description\n        @return RETURN: The id of the object"}
{"task_id": "codesearchnet/python/train/116", "input": "def __collect_interfaces_return(interfaces):\n        \"\"\"Collect new style (44.1+) return values to old-style kv-list\"\"\"\n        acc = []\n        for (interfaceName, interfaceData) in interfaces.items():\n            signalValues = interfaceData.get(\"signals\", {})\n            for (signalName, signalValue) in signalValues.items():\n                pinName = \"{0}.{1}\".format(interfaceName, signalName)\n                acc.append({'id': pinName, 'value': signalValue})\n        return acc", "target": "Collect new style (44.1+) return values to old-style kv-list"}
{"task_id": "codesearchnet/python/train/117", "input": "def return_values(self):\n        \"\"\" Guess what api we are using and return as public api does.\n        Private has {'id':'key', 'value':'keyvalue'} format, public has {'key':'keyvalue'}\n        \"\"\"\n\n        j = self.json()\n        #TODO: FIXME: get rid of old API when its support will be removed\n        public_api_value = j.get('returnValues')\n        old_private_value = j.get('endpoints')\n        new_private_value = self.__collect_interfaces_return(j.get('interfaces', {}))\n\n        retvals = new_private_value or old_private_value or public_api_value or []\n        # TODO: Public api hack.\n        if self._router.public_api_in_use:\n            return retvals\n        return self.__parse(retvals)", "target": "Guess what api we are using and return as public api does.\n        Private has {'id':'key', 'value':'keyvalue'} format, public has {'key':'keyvalue'}"}
{"task_id": "codesearchnet/python/train/118", "input": "def get_activitylog(self, after=None, severity=None, start=None, end=None):\n        \"\"\"\n        Returns activitylog object\n        severity - filter severity ('INFO', DEBUG')\n        start/end - time or log text\n\n        \"\"\"\n        if after:\n            log_raw = self._router.get_instance_activitylog(org_id=self.organizationId,\n                                                            instance_id=self.instanceId,\n                                                            params={\"after\": after}).json()\n        else:\n            log_raw = self._router.get_instance_activitylog(org_id=self.organizationId,\n                                                            instance_id=self.instanceId).json()\n\n        return ActivityLog(log_raw, severity=severity, start=start, end=end)", "target": "Returns activitylog object\n        severity - filter severity ('INFO', DEBUG')\n        start/end - time or log text"}
{"task_id": "codesearchnet/python/train/119", "input": "def json(self):\n        \"\"\"\n        return __cached_json, if accessed withing 300 ms.\n        This allows to optimize calls when many parameters of entity requires withing short time.\n        \"\"\"\n\n        if self.fresh():\n            return self.__cached_json\n        # noinspection PyAttributeOutsideInit\n        self.__last_read_time = time.time()\n        self.__cached_json = self._router.get_instance(org_id=self.organizationId, instance_id=self.instanceId).json()\n\n        return self.__cached_json", "target": "return __cached_json, if accessed withing 300 ms.\n        This allows to optimize calls when many parameters of entity requires withing short time."}
{"task_id": "codesearchnet/python/train/120", "input": "def get_most_recent_update_time(self):\n        \"\"\"\n        Indicated most recent update of the instance, assumption based on:\n        - if currentWorkflow exists, its startedAt time is most recent update.\n        - else max of workflowHistory startedAt is most recent update.\n        \"\"\"\n        def parse_time(t):\n            if t:\n                return time.gmtime(t/1000)\n            return None\n        try:\n            max_wf_started_at = max([i.get('startedAt') for i in self.workflowHistory])\n            return parse_time(max_wf_started_at)\n        except ValueError:\n            return None", "target": "Indicated most recent update of the instance, assumption based on:\n        - if currentWorkflow exists, its startedAt time is most recent update.\n        - else max of workflowHistory startedAt is most recent update."}
{"task_id": "codesearchnet/python/train/121", "input": "def _is_projection_updated_instance(self):\n        \"\"\"\n        This method tries to guess if instance was update since last time.\n        If return True, definitely Yes, if False, this means more unknown\n        :return: bool\n        \"\"\"\n        last = self._last_workflow_started_time\n        if not self._router.public_api_in_use:\n            most_recent = self.get_most_recent_update_time()\n        else:\n            most_recent = None\n        if last and most_recent:\n            return last < most_recent\n        return False", "target": "This method tries to guess if instance was update since last time.\n        If return True, definitely Yes, if False, this means more unknown\n        :return: bool"}
{"task_id": "codesearchnet/python/train/122", "input": "def find(self, item, description='', event_type=''):\n        \"\"\"\n        Find regexp in activitylog\n        find record as if type are in description.\n        \"\"\"\n        # TODO: should be refactored, dumb logic\n        if ': ' in item:\n            splited = item.split(': ', 1)\n            if splited[0] in self.TYPES:\n                description = item.split(': ')[1]\n                event_type = item.split(': ')[0]\n            else:\n                description = item\n        else:\n            if not description:\n                description = item\n\n        if event_type:\n            found = [x['time'] for x in self.log if re.search(description, x['description'])\n                     and x['eventTypeText'] == event_type]\n        else:\n            found = [x['time'] for x in self.log if re.search(description, x['description'])]\n\n        if len(found):\n            return found\n        raise exceptions.NotFoundError(\"Item '{}' is not found with (description='{}', event_type='{}')\".\n                                       format(item, description, event_type))", "target": "Find regexp in activitylog\n        find record as if type are in description."}
{"task_id": "codesearchnet/python/train/123", "input": "def do_command_line(infile: typing.IO[str]) -> int:\n    \"\"\"\n    Currently a small stub to create an instance of Checker for the passed\n    ``infile`` and run its test functions through linting.\n\n    Args:\n        infile\n\n    Returns:\n        int: Number of flake8 errors raised.\n    \"\"\"\n    lines = infile.readlines()\n    tree = ast.parse(''.join(lines))\n    checker = Checker(tree, lines, infile.name)\n    checker.load()\n    errors = []  # type: typing.List[AAAError]\n    for func in checker.all_funcs(skip_noqa=True):\n        try:\n            errors = list(func.check_all())\n        except ValidationError as error:\n            errors = [error.to_aaa()]\n        print(func.__str__(errors), end='')\n    return len(errors)", "target": "Currently a small stub to create an instance of Checker for the passed\n    ``infile`` and run its test functions through linting.\n\n    Args:\n        infile\n\n    Returns:\n        int: Number of flake8 errors raised."}
{"task_id": "codesearchnet/python/train/124", "input": "def find_spec(self, fullname, path, target=None):\n        '''finds the appropriate properties (spec) of a module, and sets\n           its loader.'''\n        if not path:\n            path = [os.getcwd()]\n        if \".\" in fullname:\n            name = fullname.split(\".\")[-1]\n        else:\n            name = fullname\n        for entry in path:\n            if os.path.isdir(os.path.join(entry, name)):\n                # this module has child modules\n                filename = os.path.join(entry, name, \"__init__.py\")\n                submodule_locations = [os.path.join(entry, name)]\n            else:\n                filename = os.path.join(entry, name + \".py\")\n                submodule_locations = None\n            if not os.path.exists(filename):\n                continue\n\n            return spec_from_file_location(fullname, filename,\n                                           loader=MyLoader(filename),\n                                           submodule_search_locations=submodule_locations)\n        return None", "target": "finds the appropriate properties (spec) of a module, and sets\n           its loader."}
{"task_id": "codesearchnet/python/train/125", "input": "def exec_module(self, module):\n        '''import the source code, transforma it before executing it so that\n           it is known to Python.'''\n        global MAIN_MODULE_NAME\n        if module.__name__ == MAIN_MODULE_NAME:\n            module.__name__ = \"__main__\"\n            MAIN_MODULE_NAME = None\n\n        with open(self.filename) as f:\n            source = f.read()\n\n        if transforms.transformers:\n            source = transforms.transform(source)\n        else:\n            for line in source.split('\\n'):\n                if transforms.FROM_EXPERIMENTAL.match(line):\n                    ## transforms.transform will extract all such relevant\n                    ## lines and add them all relevant transformers\n                    source = transforms.transform(source)\n                    break\n        exec(source, vars(module))", "target": "import the source code, transforma it before executing it so that\n           it is known to Python."}
{"task_id": "codesearchnet/python/train/126", "input": "def _izip(*iterables):\n    \"\"\" Iterate through multiple lists or arrays of equal size \"\"\"\n    # This izip routine is from itertools\n    # izip('ABCD', 'xy') --> Ax By\n\n    iterators = map(iter, iterables)\n    while iterators:\n        yield tuple(map(next, iterators))", "target": "Iterate through multiple lists or arrays of equal size"}
{"task_id": "codesearchnet/python/train/127", "input": "def _checkinput(zi, Mi, z=False, verbose=None):\n    \"\"\" Check and convert any input scalar or array to numpy array \"\"\"\n    # How many halo redshifts provided?\n    zi = np.array(zi, ndmin=1, dtype=float)\n\n    # How many halo masses provided?\n    Mi = np.array(Mi, ndmin=1, dtype=float)\n\n    # Check the input sizes for zi and Mi make sense, if not then exit unless\n    # one axis is length one, then replicate values to the size of the other\n    if (zi.size > 1) and (Mi.size > 1):\n        if(zi.size != Mi.size):\n            print(\"Error ambiguous request\")\n            print(\"Need individual redshifts for all haloes provided \")\n            print(\"Or have all haloes at same redshift \")\n            return(-1)\n    elif (zi.size == 1) and (Mi.size > 1):\n        if verbose:\n            print(\"Assume zi is the same for all Mi halo masses provided\")\n        # Replicate redshift for all halo masses\n        zi = np.ones_like(Mi)*zi[0]\n    elif (Mi.size == 1) and (zi.size > 1):\n        if verbose:\n            print(\"Assume Mi halo masses are the same for all zi provided\")\n        # Replicate redshift for all halo masses\n        Mi = np.ones_like(zi)*Mi[0]\n    else:\n        if verbose:\n            print(\"A single Mi and zi provided\")\n\n    # Very simple test for size / type of incoming array\n    # just in case numpy / list given\n    if z is False:\n        # Didn't pass anything, set zi = z\n        lenzout = 1\n    else:\n        # If something was passed, convert to 1D NumPy array\n        z = np.array(z, ndmin=1, dtype=float)\n        lenzout = z.size\n\n    return(zi, Mi, z, zi.size, Mi.size, lenzout)", "target": "Check and convert any input scalar or array to numpy array"}
{"task_id": "codesearchnet/python/train/128", "input": "def getcosmo(cosmology):\n    \"\"\" Find cosmological parameters for named cosmo in cosmology.py list \"\"\"\n\n    defaultcosmologies = {'dragons': cg.DRAGONS(), 'wmap1': cg.WMAP1_Mill(),\n                          'wmap3': cg.WMAP3_ML(), 'wmap5': cg.WMAP5_mean(),\n                          'wmap7': cg.WMAP7_ML(), 'wmap9': cg.WMAP9_ML(),\n                          'wmap1_lss': cg.WMAP1_2dF_mean(),\n                          'wmap3_mean': cg.WMAP3_mean(),\n                          'wmap5_ml': cg.WMAP5_ML(),\n                          'wmap5_lss': cg.WMAP5_BAO_SN_mean(),\n                          'wmap7_lss': cg.WMAP7_BAO_H0_mean(),\n                          'planck13': cg.Planck_2013(),\n                          'planck15': cg.Planck_2015()}\n\n    if isinstance(cosmology, dict):\n        # User providing their own variables\n        cosmo = cosmology\n        if 'A_scaling' not in cosmology.keys():\n            A_scaling = getAscaling(cosmology, newcosmo=True)\n            cosmo.update({'A_scaling': A_scaling})\n\n        # Add extra variables by hand that cosmolopy requires\n        # note that they aren't used (set to zero)\n        for paramnames in cg.WMAP5_mean().keys():\n            if paramnames not in cosmology.keys():\n                cosmo.update({paramnames: 0})\n    elif cosmology.lower() in defaultcosmologies.keys():\n        # Load by name of cosmology instead\n        cosmo = defaultcosmologies[cosmology.lower()]\n        A_scaling = getAscaling(cosmology)\n        cosmo.update({'A_scaling': A_scaling})\n    else:\n        print(\"You haven't passed a dict of cosmological parameters \")\n        print(\"OR a recognised cosmology, you gave %s\" % (cosmology))\n    # No idea why this has to be done by hand but should be O_k = 0\n    cosmo = cp.distance.set_omega_k_0(cosmo)\n\n    # Use the cosmology as **cosmo passed to cosmolopy routines\n    return(cosmo)", "target": "Find cosmological parameters for named cosmo in cosmology.py list"}
{"task_id": "codesearchnet/python/train/129", "input": "def _getcosmoheader(cosmo):\n    \"\"\" Output the cosmology to a string for writing to file \"\"\"\n\n    cosmoheader = (\"# Cosmology (flat) Om:{0:.3f}, Ol:{1:.3f}, h:{2:.2f}, \"\n                   \"sigma8:{3:.3f}, ns:{4:.2f}\".format(\n                       cosmo['omega_M_0'], cosmo['omega_lambda_0'], cosmo['h'],\n                       cosmo['sigma_8'], cosmo['n']))\n\n    return(cosmoheader)", "target": "Output the cosmology to a string for writing to file"}
{"task_id": "codesearchnet/python/train/130", "input": "def cduffy(z, M, vir='200crit', relaxed=True):\n    \"\"\" NFW conc from Duffy 08 Table 1 for halo mass and redshift\"\"\"\n\n    if(vir == '200crit'):\n        if relaxed:\n            params = [6.71, -0.091, -0.44]\n        else:\n            params = [5.71, -0.084, -0.47]\n    elif(vir == 'tophat'):\n        if relaxed:\n            params = [9.23, -0.090, -0.69]\n        else:\n            params = [7.85, -0.081, -0.71]\n    elif(vir == '200mean'):\n        if relaxed:\n            params = [11.93, -0.090, -0.99]\n        else:\n            params = [10.14, -0.081, -1.01]\n    else:\n        print(\"Didn't recognise the halo boundary definition provided %s\"\n              % (vir))\n\n    return(params[0] * ((M/(2e12/0.72))**params[1]) * ((1+z)**params[2]))", "target": "NFW conc from Duffy 08 Table 1 for halo mass and redshift"}
{"task_id": "codesearchnet/python/train/131", "input": "def _delta_sigma(**cosmo):\n    \"\"\" Perturb best-fit constant of proportionality Ascaling for\n        rho_crit - rho_2 relation for unknown cosmology (Correa et al 2015c)\n\n    Parameters\n    ----------\n    cosmo : dict\n        Dictionary of cosmological parameters, similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    Returns\n    -------\n    float\n        The perturbed 'A' relation between rho_2 and rho_crit for the cosmology\n\n    Raises\n    ------\n\n    \"\"\"\n\n    M8_cosmo = cp.perturbation.radius_to_mass(8, **cosmo)\n    perturbed_A = (0.796/cosmo['sigma_8']) * \\\n                  (M8_cosmo/2.5e14)**((cosmo['n']-0.963)/6)\n    return(perturbed_A)", "target": "Perturb best-fit constant of proportionality Ascaling for\n        rho_crit - rho_2 relation for unknown cosmology (Correa et al 2015c)\n\n    Parameters\n    ----------\n    cosmo : dict\n        Dictionary of cosmological parameters, similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    Returns\n    -------\n    float\n        The perturbed 'A' relation between rho_2 and rho_crit for the cosmology\n\n    Raises\n    ------"}
{"task_id": "codesearchnet/python/train/132", "input": "def getAscaling(cosmology, newcosmo=None):\n    \"\"\" Returns the normalisation constant between\n        Rho_-2 and Rho_mean(z_formation) for a given cosmology\n\n    Parameters\n    ----------\n    cosmology : str or dict\n        Can be named cosmology, default WMAP7 (aka DRAGONS), or\n        DRAGONS, WMAP1, WMAP3, WMAP5, WMAP7, WMAP9, Planck13, Planck15\n        or dictionary similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n    newcosmo : str, optional\n        If cosmology is not from predefined list have to perturbation\n        A_scaling variable. Defaults to None.\n\n    Returns\n    -------\n    float\n        The scaled 'A' relation between rho_2 and rho_crit for the cosmology\n\n    \"\"\"\n    # Values from Correa 15c\n    defaultcosmologies = {'dragons': 887, 'wmap1': 853, 'wmap3': 850,\n                          'wmap5': 887, 'wmap7': 887, 'wmap9': 950,\n                          'wmap1_lss': 853, 'wmap3_mean': 850,\n                          'wmap5_ml': 887, 'wmap5_lss': 887,\n                          'wmap7_lss': 887,\n                          'planck13': 880, 'planck15': 880}\n\n    if newcosmo:\n        # Scale from default WMAP5 cosmology using Correa et al 14b eqn C1\n        A_scaling = defaultcosmologies['wmap5'] * _delta_sigma(**cosmology)\n    else:\n        if cosmology.lower() in defaultcosmologies.keys():\n            A_scaling = defaultcosmologies[cosmology.lower()]\n        else:\n            print(\"Error, don't recognise your cosmology for A_scaling \")\n            print(\"You provided %s\" % (cosmology))\n\n    return(A_scaling)", "target": "Returns the normalisation constant between\n        Rho_-2 and Rho_mean(z_formation) for a given cosmology\n\n    Parameters\n    ----------\n    cosmology : str or dict\n        Can be named cosmology, default WMAP7 (aka DRAGONS), or\n        DRAGONS, WMAP1, WMAP3, WMAP5, WMAP7, WMAP9, Planck13, Planck15\n        or dictionary similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n    newcosmo : str, optional\n        If cosmology is not from predefined list have to perturbation\n        A_scaling variable. Defaults to None.\n\n    Returns\n    -------\n    float\n        The scaled 'A' relation between rho_2 and rho_crit for the cosmology"}
{"task_id": "codesearchnet/python/train/133", "input": "def _int_growth(z, **cosmo):\n    \"\"\" Returns integral of the linear growth factor from z=200 to z=z \"\"\"\n\n    zmax = 200\n\n    if hasattr(z, \"__len__\"):\n        for zval in z:\n            assert(zval < zmax)\n    else:\n        assert(z < zmax)\n\n    y, yerr = scipy.integrate.quad(\n        lambda z: (1 + z)/(cosmo['omega_M_0']*(1 + z)**3 +\n                           cosmo['omega_lambda_0'])**(1.5),\n        z, zmax)\n\n    return(y)", "target": "Returns integral of the linear growth factor from z=200 to z=z"}
{"task_id": "codesearchnet/python/train/134", "input": "def _deriv_growth(z, **cosmo):\n    \"\"\" Returns derivative of the linear growth factor at z\n        for a given cosmology **cosmo \"\"\"\n\n    inv_h = (cosmo['omega_M_0']*(1 + z)**3 + cosmo['omega_lambda_0'])**(-0.5)\n    fz = (1 + z) * inv_h**3\n\n    deriv_g = growthfactor(z, norm=True, **cosmo)*(inv_h**2) *\\\n        1.5 * cosmo['omega_M_0'] * (1 + z)**2 -\\\n        fz * growthfactor(z, norm=True, **cosmo)/_int_growth(z, **cosmo)\n\n    return(deriv_g)", "target": "Returns derivative of the linear growth factor at z\n        for a given cosmology **cosmo"}
{"task_id": "codesearchnet/python/train/135", "input": "def growthfactor(z, norm=True, **cosmo):\n    \"\"\" Returns linear growth factor at a given redshift, normalised to z=0\n        by default, for a given cosmology\n\n    Parameters\n    ----------\n\n    z : float or numpy array\n        The redshift at which the growth factor should be calculated\n    norm : boolean, optional\n        If true then normalise the growth factor to z=0 case defaults True\n    cosmo : dict\n        Dictionary of cosmological parameters, similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    Returns\n    -------\n    float or numpy array\n        The growth factor at a range of redshifts 'z'\n\n    Raises\n    ------\n\n    \"\"\"\n    H = np.sqrt(cosmo['omega_M_0'] * (1 + z)**3 +\n                cosmo['omega_lambda_0'])\n    growthval = H * _int_growth(z, **cosmo)\n    if norm:\n        growthval /= _int_growth(0, **cosmo)\n\n    return(growthval)", "target": "Returns linear growth factor at a given redshift, normalised to z=0\n        by default, for a given cosmology\n\n    Parameters\n    ----------\n\n    z : float or numpy array\n        The redshift at which the growth factor should be calculated\n    norm : boolean, optional\n        If true then normalise the growth factor to z=0 case defaults True\n    cosmo : dict\n        Dictionary of cosmological parameters, similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    Returns\n    -------\n    float or numpy array\n        The growth factor at a range of redshifts 'z'\n\n    Raises\n    ------"}
{"task_id": "codesearchnet/python/train/136", "input": "def _minimize_c(c, z=0, a_tilde=1, b_tilde=-1,\n                Ascaling=900, omega_M_0=0.25, omega_lambda_0=0.75):\n    \"\"\" Trial function to solve 2 eqns (17 and 18) from Correa et al. (2015c)\n        for 1 unknown, i.e. concentration, returned by a minimisation call \"\"\"\n\n    # Fn 1 (LHS of Eqn 18)\n\n    Y1 = np.log(2) - 0.5\n    Yc = np.log(1+c) - c/(1+c)\n    f1 = Y1/Yc\n\n    # Fn 2 (RHS of Eqn 18)\n\n    # Eqn 14 - Define the mean inner density\n    rho_2 = 200 * c**3 * Y1 / Yc\n\n    # Eqn 17 rearranged to solve for Formation Redshift\n    # essentially when universe had rho_2 density\n    zf = (((1 + z)**3 + omega_lambda_0/omega_M_0) *\n          (rho_2/Ascaling) - omega_lambda_0/omega_M_0)**(1/3) - 1\n\n    # RHS of Eqn 19\n    f2 = ((1 + zf - z)**a_tilde) * np.exp((zf - z) * b_tilde)\n\n    # LHS - RHS should be zero for the correct concentration\n    return(f1-f2)", "target": "Trial function to solve 2 eqns (17 and 18) from Correa et al. (2015c)\n        for 1 unknown, i.e. concentration, returned by a minimisation call"}
{"task_id": "codesearchnet/python/train/137", "input": "def formationz(c, z, Ascaling=900, omega_M_0=0.25, omega_lambda_0=0.75):\n    \"\"\" Rearrange eqn 18 from Correa et al (2015c) to return\n        formation redshift for a concentration at a given redshift\n\n    Parameters\n    ----------\n    c : float / numpy array\n        Concentration of halo\n    z : float / numpy array\n        Redshift of halo with concentration c\n    Ascaling : float\n        Cosmological dependent scaling between densities, use function\n        getAscaling('WMAP5') if unsure. Default is 900.\n    omega_M_0 : float\n        Mass density of the universe. Default is 0.25\n    omega_lambda_0 : float\n        Dark Energy density of the universe. Default is 0.75\n\n    Returns\n    -------\n    zf : float / numpy array\n        Formation redshift for halo of concentration 'c' at redshift 'z'\n\n    \"\"\"\n    Y1 = np.log(2) - 0.5\n    Yc = np.log(1+c) - c/(1+c)\n    rho_2 = 200*(c**3)*Y1/Yc\n\n    zf = (((1+z)**3 + omega_lambda_0/omega_M_0) *\n          (rho_2/Ascaling) - omega_lambda_0/omega_M_0)**(1/3) - 1\n\n    return(zf)", "target": "Rearrange eqn 18 from Correa et al (2015c) to return\n        formation redshift for a concentration at a given redshift\n\n    Parameters\n    ----------\n    c : float / numpy array\n        Concentration of halo\n    z : float / numpy array\n        Redshift of halo with concentration c\n    Ascaling : float\n        Cosmological dependent scaling between densities, use function\n        getAscaling('WMAP5') if unsure. Default is 900.\n    omega_M_0 : float\n        Mass density of the universe. Default is 0.25\n    omega_lambda_0 : float\n        Dark Energy density of the universe. Default is 0.75\n\n    Returns\n    -------\n    zf : float / numpy array\n        Formation redshift for halo of concentration 'c' at redshift 'z'"}
{"task_id": "codesearchnet/python/train/138", "input": "def calc_ab(zi, Mi, **cosmo):\n    \"\"\" Calculate growth rate indices a_tilde and b_tilde\n\n    Parameters\n    ----------\n    zi : float\n        Redshift\n    Mi : float\n        Halo mass at redshift 'zi'\n    cosmo : dict\n        Dictionary of cosmological parameters, similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    Returns\n    -------\n    (a_tilde, b_tilde) : float\n    \"\"\"\n\n    # When zi = 0, the a_tilde becomes alpha and b_tilde becomes beta\n\n    # Eqn 23 of Correa et al 2015a (analytically solve from Eqn 16 and 17)\n    # Arbitray formation redshift, z_-2 in COM is more physically motivated\n    zf = -0.0064 * (np.log10(Mi))**2 + 0.0237 * (np.log10(Mi)) + 1.8837\n\n    # Eqn 22 of Correa et al 2015a\n    q = 4.137 * zf**(-0.9476)\n\n    # Radius of a mass Mi\n    R_Mass = cp.perturbation.mass_to_radius(Mi, **cosmo)  # [Mpc]\n    # Radius of a mass Mi/q\n    Rq_Mass = cp.perturbation.mass_to_radius(Mi/q, **cosmo)  # [Mpc]\n\n    # Mass variance 'sigma' evaluate at z=0 to a good approximation\n    sig, err_sig = cp.perturbation.sigma_r(R_Mass, 0, **cosmo)  # [Mpc]\n    sigq, err_sigq = cp.perturbation.sigma_r(Rq_Mass, 0, **cosmo)  # [Mpc]\n\n    f = (sigq**2 - sig**2)**(-0.5)\n\n    # Eqn 9 and 10 from Correa et al 2015c\n    # (generalised to zi from Correa et al 2015a's z=0 special case)\n    # a_tilde is power law growth rate\n    a_tilde = (np.sqrt(2/np.pi) * 1.686 * _deriv_growth(zi, **cosmo) /\n               growthfactor(zi, norm=True, **cosmo)**2 + 1)*f\n    # b_tilde is exponential growth rate\n    b_tilde = -f\n\n    return(a_tilde, b_tilde)", "target": "Calculate growth rate indices a_tilde and b_tilde\n\n    Parameters\n    ----------\n    zi : float\n        Redshift\n    Mi : float\n        Halo mass at redshift 'zi'\n    cosmo : dict\n        Dictionary of cosmological parameters, similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    Returns\n    -------\n    (a_tilde, b_tilde) : float"}
{"task_id": "codesearchnet/python/train/139", "input": "def acc_rate(z, zi, Mi, **cosmo):\n    \"\"\" Calculate accretion rate and mass history of a halo at any\n        redshift 'z' with mass 'Mi' at a lower redshift 'z'\n\n    Parameters\n    ----------\n    z : float\n        Redshift to solve acc_rate / mass history. Note zi<z\n    zi : float\n        Redshift\n    Mi : float\n        Halo mass at redshift 'zi'\n    cosmo : dict\n        Dictionary of cosmological parameters, similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    Returns\n    -------\n    (dMdt, Mz) : float\n        Accretion rate [Msol/yr], halo mass [Msol] at redshift 'z'\n\n    \"\"\"\n    # Find parameters a_tilde and b_tilde for initial redshift\n    # use Eqn 9 and 10 of Correa et al. (2015c)\n    a_tilde, b_tilde = calc_ab(zi, Mi, **cosmo)\n\n    # Halo mass at z, in Msol\n    # use Eqn 8 in Correa et al. (2015c)\n    Mz = Mi * ((1 + z - zi)**a_tilde) * (np.exp(b_tilde * (z - zi)))\n\n    # Accretion rate at z, Msol yr^-1\n    # use Eqn 11 from Correa et al. (2015c)\n    dMdt = 71.6 * (Mz/1e12) * (cosmo['h']/0.7) *\\\n        (-a_tilde / (1 + z - zi) - b_tilde) * (1 + z) *\\\n        np.sqrt(cosmo['omega_M_0']*(1 + z)**3+cosmo['omega_lambda_0'])\n\n    return(dMdt, Mz)", "target": "Calculate accretion rate and mass history of a halo at any\n        redshift 'z' with mass 'Mi' at a lower redshift 'z'\n\n    Parameters\n    ----------\n    z : float\n        Redshift to solve acc_rate / mass history. Note zi<z\n    zi : float\n        Redshift\n    Mi : float\n        Halo mass at redshift 'zi'\n    cosmo : dict\n        Dictionary of cosmological parameters, similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    Returns\n    -------\n    (dMdt, Mz) : float\n        Accretion rate [Msol/yr], halo mass [Msol] at redshift 'z'"}
{"task_id": "codesearchnet/python/train/140", "input": "def MAH(z, zi, Mi, **cosmo):\n    \"\"\" Calculate mass accretion history by looping function acc_rate\n        over redshift steps 'z' for halo of mass 'Mi' at redshift 'zi'\n\n    Parameters\n    ----------\n    z : float / numpy array\n        Redshift to output MAH over. Note zi<z always\n    zi : float\n        Redshift\n    Mi : float\n        Halo mass at redshift 'zi'\n    cosmo : dict\n        Dictionary of cosmological parameters, similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    Returns\n    -------\n    (dMdt, Mz) : float / numpy arrays of equivalent size to 'z'\n        Accretion rate [Msol/yr], halo mass [Msol] at redshift 'z'\n\n    \"\"\"\n\n    # Ensure that z is a 1D NumPy array\n    z = np.array(z, ndmin=1, dtype=float)\n\n    # Create a full array\n    dMdt_array = np.empty_like(z)\n    Mz_array = np.empty_like(z)\n\n    for i_ind, zval in enumerate(z):\n        # Solve the accretion rate and halo mass at each redshift step\n        dMdt, Mz = acc_rate(zval, zi, Mi, **cosmo)\n\n        dMdt_array[i_ind] = dMdt\n        Mz_array[i_ind] = Mz\n\n    return(dMdt_array, Mz_array)", "target": "Calculate mass accretion history by looping function acc_rate\n        over redshift steps 'z' for halo of mass 'Mi' at redshift 'zi'\n\n    Parameters\n    ----------\n    z : float / numpy array\n        Redshift to output MAH over. Note zi<z always\n    zi : float\n        Redshift\n    Mi : float\n        Halo mass at redshift 'zi'\n    cosmo : dict\n        Dictionary of cosmological parameters, similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    Returns\n    -------\n    (dMdt, Mz) : float / numpy arrays of equivalent size to 'z'\n        Accretion rate [Msol/yr], halo mass [Msol] at redshift 'z'"}
{"task_id": "codesearchnet/python/train/141", "input": "def COM(z, M, **cosmo):\n    \"\"\" Calculate concentration for halo of mass 'M' at redshift 'z'\n\n    Parameters\n    ----------\n    z : float / numpy array\n        Redshift to find concentration of halo\n    M : float / numpy array\n        Halo mass at redshift 'z'. Must be same size as 'z'\n    cosmo : dict\n        Dictionary of cosmological parameters, similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    Returns\n    -------\n    (c_array, sig_array, nu_array, zf_array) : float / numpy arrays\n        of equivalent size to 'z' and 'M'. Variables are\n        Concentration, Mass Variance 'sigma' this corresponds too,\n        the dimnesionless fluctuation this represents and formation redshift\n\n    \"\"\"\n    # Check that z and M are arrays\n    z = np.array(z, ndmin=1, dtype=float)\n    M = np.array(M, ndmin=1, dtype=float)\n\n    # Create array\n    c_array = np.empty_like(z)\n    sig_array = np.empty_like(z)\n    nu_array = np.empty_like(z)\n    zf_array = np.empty_like(z)\n\n    for i_ind, (zval, Mval) in enumerate(_izip(z, M)):\n        # Evaluate the indices at each redshift and mass combination\n        # that you want a concentration for, different to MAH which\n        # uses one a_tilde and b_tilde at the starting redshift only\n        a_tilde, b_tilde = calc_ab(zval, Mval, **cosmo)\n\n        # Minimize equation to solve for 1 unknown, 'c'\n        c = scipy.optimize.brentq(_minimize_c, 2, 1000,\n                                  args=(zval, a_tilde, b_tilde,\n                                        cosmo['A_scaling'], cosmo['omega_M_0'],\n                                        cosmo['omega_lambda_0']))\n\n        if np.isclose(c, 0):\n            print(\"Error solving for concentration with given redshift and \"\n                  \"(probably) too small a mass\")\n            c = -1\n            sig = -1\n            nu = -1\n            zf = -1\n        else:\n            # Calculate formation redshift for this concentration,\n            # redshift at which the scale radius = virial radius: z_-2\n            zf = formationz(c, zval, Ascaling=cosmo['A_scaling'],\n                            omega_M_0=cosmo['omega_M_0'],\n                            omega_lambda_0=cosmo['omega_lambda_0'])\n\n            R_Mass = cp.perturbation.mass_to_radius(Mval, **cosmo)\n\n            sig, err_sig = cp.perturbation.sigma_r(R_Mass, 0, **cosmo)\n            nu = 1.686/(sig*growthfactor(zval, norm=True, **cosmo))\n\n        c_array[i_ind] = c\n        sig_array[i_ind] = sig\n        nu_array[i_ind] = nu\n        zf_array[i_ind] = zf\n\n    return(c_array, sig_array, nu_array, zf_array)", "target": "Calculate concentration for halo of mass 'M' at redshift 'z'\n\n    Parameters\n    ----------\n    z : float / numpy array\n        Redshift to find concentration of halo\n    M : float / numpy array\n        Halo mass at redshift 'z'. Must be same size as 'z'\n    cosmo : dict\n        Dictionary of cosmological parameters, similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    Returns\n    -------\n    (c_array, sig_array, nu_array, zf_array) : float / numpy arrays\n        of equivalent size to 'z' and 'M'. Variables are\n        Concentration, Mass Variance 'sigma' this corresponds too,\n        the dimnesionless fluctuation this represents and formation redshift"}
{"task_id": "codesearchnet/python/train/142", "input": "def run(cosmology, zi=0, Mi=1e12, z=False, com=True, mah=True,\n        filename=None, verbose=None, retcosmo=None):\n    \"\"\" Run commah code on halo of mass 'Mi' at redshift 'zi' with\n        accretion and profile history at higher redshifts 'z'\n        This is based on Correa et al. (2015a,b,c)\n\n    Parameters\n    ----------\n    cosmology : str or dict\n        Can be named cosmology, default WMAP7 (aka DRAGONS), or\n        DRAGONS, WMAP1, WMAP3, WMAP5, WMAP7, WMAP9, Planck13, Planck15\n        or dictionary similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    zi : float / numpy array, optional\n        Redshift at which halo has mass 'Mi'. If float then all\n        halo masses 'Mi' are assumed to be at this redshift.\n        If array but Mi is float, then this halo mass is used across\n        all starting redshifts. If both Mi and zi are arrays then they\n        have to be the same size for one - to - one correspondence between\n        halo mass and the redshift at which it has that mass. Default is 0.\n    Mi : float / numpy array, optional\n        Halo mass 'Mi' at a redshift 'zi'. If float then all redshifts 'zi'\n        are solved for this halo mass. If array but zi is float, then this\n        redshift is applied to all halo masses. If both Mi and zi are\n        arrays then they have to be the same size for one - to - one\n        correspondence between halo mass and the redshift at which it\n        has that mass. Default is 1e12 Msol.\n    z : float / numpy array, optional\n        Redshift to solve commah code at. Must have zi<z else these steps\n        are skipped. Default is False, meaning commah is solved at z=zi\n\n    com : bool, optional\n        If true then solve for concentration-mass,\n        default is True.\n    mah : bool, optional\n        If true then solve for accretion rate and halo mass history,\n        default is True.\n    filename : bool / str, optional\n        If str is passed this is used as a filename for output of commah\n    verbose : bool, optional\n        If true then give comments, default is None.\n    retcosmo : bool, optional\n        Return cosmological parameters used as a dict if retcosmo = True,\n        default is None.\n\n    Returns\n    -------\n    dataset : structured dataset\n        dataset contains structured columns of size\n        (size(Mi) > size(z)) by size(z)\n\n        If mah = True and com = False then columns are\n        ('zi',float),('Mi',float),('z',float),('dMdt',float),('Mz',float)\n        where 'zi' is the starting redshift, 'Mi' is halo mass at zi\n        'z' is output redshift (NB z>zi), 'dMdt' is accretion rate [Msol/yr]\n        and 'Mz' is the halo mass at 'z' for a halo which was 'Mi' massive\n        at starting redshift 'zi'\n\n        If mah = False and com = True then columns are\n        ('zi',float),('Mi',float),('z',float),('c',float),('sig',float),('nu',float),('zf',float)\n        where 'zi' is the starting redshift, 'Mi' is halo mass at zi\n        'z' is output redshift (NB z>zi), 'c' is NFW concentration of halo\n        at the redshift 'z', 'sig' is the mass variance 'sigma',\n        'nu' is the dimensionless fluctuation for halo mass 'Mi' at 'zi',\n        'zf' is the formation redshift for a halo of mass 'Mi' at redshift 'zi'\n\n        If mah = True and com = True then columns are:\n        ('zi',float),('Mi',float),('z',float),('dMdt',float),('Mz',float),\n        ('c',float),('sig',float),('nu',float),('zf',float)\n\n    file : structured dataset with name 'filename' if passed\n\n    Raises\n    ------\n    Output -1\n        If com = False and mah = False as user has to select something.\n    Output -1\n        If 'zi' and 'Mi' are arrays of unequal size. Impossible to match\n        corresponding masses and redshifts of output.\n\n    Examples\n    --------\n    Examples should be written in doctest format, and should illustrate how\n    to use the function.\n\n    >>> import examples\n    >>> examples.runcommands() #  A series of ways to query structured dataset\n    >>> examples.plotcommands() #  Examples to plot data\n\n    \"\"\"\n\n    # Check user choices...\n    if not com and not mah:\n        print(\"User has to choose com=True and / or mah=True \")\n        return(-1)\n\n    # Convert arrays / lists to np.array\n    # and inflate redshift / mass axis\n    # to match each other for later loop\n    results = _checkinput(zi, Mi, z=z, verbose=verbose)\n\n    # Return if results is -1\n    if(results == -1):\n        return(-1)\n    # If not, unpack the returned iterable\n    else:\n        zi, Mi, z, lenz, lenm, lenzout = results\n    # At this point we will have lenm objects to iterate over\n\n    # Get the cosmological parameters for the given cosmology\n    cosmo = getcosmo(cosmology)\n\n    # Create  output file if desired\n    if filename:\n        print(\"Output to file %r\" % (filename))\n        fout = open(filename, 'wb')\n\n    # Create the structured dataset\n    try:\n        if mah and com:\n            if verbose:\n                print(\"Output requested is zi, Mi, z, dMdt, Mz, c, sig, nu, \"\n                      \"zf\")\n            if filename:\n                fout.write(_getcosmoheader(cosmo)+'\\n')\n                fout.write(\"# Initial z - Initial Halo  - Output z - \"\n                           \" Accretion -  Final Halo  - concentration - \"\n                           \"    Mass   -    Peak    -  Formation z \"+'\\n')\n                fout.write(\"#           -     mass      -          -\"\n                           \"    rate    -     mass     -               - \"\n                           \" Variance  -   Height   -              \"+'\\n')\n                fout.write(\"#           -    (M200)     -          - \"\n                           \"  (dM/dt)  -    (M200)    -               - \"\n                           \"  (sigma)  -    (nu)    -              \"+'\\n')\n                fout.write(\"#           -    [Msol]     -          - \"\n                           \" [Msol/yr] -    [Msol]    -               - \"\n                           \"           -            -              \"+'\\n')\n            dataset = np.zeros((lenm, lenzout), dtype=[('zi', float),\n                               ('Mi', float), ('z', float), ('dMdt', float),\n                               ('Mz', float), ('c', float), ('sig', float),\n                               ('nu', float), ('zf', float)])\n        elif mah:\n            if verbose:\n                print(\"Output requested is zi, Mi, z, dMdt, Mz\")\n            if filename:\n                fout.write(_getcosmoheader(cosmo)+'\\n')\n                fout.write(\"# Initial z - Initial Halo  - Output z -\"\n                           \"   Accretion - Final Halo \"+'\\n')\n                fout.write(\"#           -     mass      -          -\"\n                           \"     rate    -   mass     \"+'\\n')\n                fout.write(\"#           -    (M200)     -          -\"\n                           \"    (dm/dt)  -  (M200)    \"+'\\n')\n                fout.write(\"#           -    [Msol]     -          -\"\n                           \"   [Msol/yr] -  [Msol]    \"+'\\n')\n            dataset = np.zeros((lenm, lenzout), dtype=[('zi', float),\n                               ('Mi', float), ('z', float),\n                               ('dMdt', float), ('Mz', float)])\n        else:\n            if verbose:\n                print(\"Output requested is zi, Mi, z, c, sig, nu, zf\")\n            if filename:\n                fout.write(_getcosmoheader(cosmo)+'\\n')\n                fout.write(\"# Initial z - Initial Halo  - Output z - \"\n                           \" concentration - \"\n                           \"  Mass    -    Peak    -  Formation z \"+'\\n')\n                fout.write(\"#           -     mass      -          -\"\n                           \"                -\"\n                           \" Variance  -   Height   -              \"+'\\n')\n                fout.write(\"#           -   (M200)      -          - \"\n                           \"               - \"\n                           \" (sigma)  -    (nu)    -              \"+'\\n')\n                fout.write(\"#           -   [Msol]      -          - \"\n                           \"               - \"\n                           \"          -            -            \"+'\\n')\n            dataset = np.zeros((lenm, lenzout), dtype=[('zi', float),\n                               ('Mi', float), ('z', float), ('c', float),\n                               ('sig', float), ('nu', float), ('zf', float)])\n\n        # Now loop over the combination of initial redshift and halo mamss\n        for i_ind, (zval, Mval) in enumerate(_izip(zi, Mi)):\n            if verbose:\n                print(\"Output Halo of Mass Mi=%s at zi=%s\" % (Mval, zval))\n            # For a given halo mass Mi at redshift zi need to know\n            # output redshifts 'z'\n            # Check that all requested redshifts are greater than\n            # input redshift, except if z is False, in which case\n            # only solve z at zi, i.e. remove a loop\n            if z is False:\n                ztemp = np.array(zval, ndmin=1, dtype=float)\n            else:\n                ztemp = np.array(z[z >= zval], dtype=float)\n\n            # Loop over the output redshifts\n            if ztemp.size:\n                # Return accretion rates and halo mass progenitors at\n                # redshifts 'z' for object of mass Mi at zi\n                dMdt, Mz = MAH(ztemp, zval, Mval, **cosmo)\n                if mah and com:\n                    # More expensive to return concentrations\n                    c, sig, nu, zf = COM(ztemp, Mz, **cosmo)\n                    # Save all arrays\n                    for j_ind, j_val in enumerate(ztemp):\n                        dataset[i_ind, j_ind] =\\\n                            (zval, Mval, ztemp[j_ind], dMdt[j_ind], Mz[j_ind],\n                             c[j_ind], sig[j_ind], nu[j_ind], zf[j_ind])\n                        if filename:\n                            fout.write(\n                                \"{}, {}, {}, {}, {}, {}, {}, {}, {} \\n\".format(\n                                    zval, Mval, ztemp[j_ind], dMdt[j_ind],\n                                    Mz[j_ind], c[j_ind], sig[j_ind], nu[j_ind],\n                                    zf[j_ind]))\n                elif mah:\n                    # Save only MAH arrays\n                    for j_ind, j_val in enumerate(ztemp):\n                        dataset[i_ind, j_ind] =\\\n                            (zval, Mval, ztemp[j_ind], dMdt[j_ind], Mz[j_ind])\n                        if filename:\n                            fout.write(\"{}, {}, {}, {}, {} \\n\".format(\n                                zval, Mval, ztemp[j_ind], dMdt[j_ind],\n                                Mz[j_ind]))\n                else:\n                    # Output only COM arrays\n                    c, sig, nu, zf = COM(ztemp, Mz, **cosmo)\n                    # For any halo mass Mi at redshift zi\n                    # solve for c, sig, nu and zf\n                    for j_ind, j_val in enumerate(ztemp):\n                        dataset[i_ind, j_ind] =\\\n                            (zval, Mval, ztemp[j_ind], c[j_ind], sig[j_ind],\n                             nu[j_ind], zf[j_ind])\n                        if filename:\n                            fout.write(\"{}, {}, {}, {}, {}, {}, {} \\n\".format(\n                                zval, Mval, ztemp[j_ind], c[j_ind], sig[j_ind],\n                                nu[j_ind], zf[j_ind]))\n\n    # Make sure to close the file if it was opened\n    finally:\n        fout.close() if filename else None\n\n    if retcosmo:\n        return(dataset, cosmo)\n    else:\n        return(dataset)", "target": "Run commah code on halo of mass 'Mi' at redshift 'zi' with\n        accretion and profile history at higher redshifts 'z'\n        This is based on Correa et al. (2015a,b,c)\n\n    Parameters\n    ----------\n    cosmology : str or dict\n        Can be named cosmology, default WMAP7 (aka DRAGONS), or\n        DRAGONS, WMAP1, WMAP3, WMAP5, WMAP7, WMAP9, Planck13, Planck15\n        or dictionary similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    zi : float / numpy array, optional\n        Redshift at which halo has mass 'Mi'. If float then all\n        halo masses 'Mi' are assumed to be at this redshift.\n        If array but Mi is float, then this halo mass is used across\n        all starting redshifts. If both Mi and zi are arrays then they\n        have to be the same size for one - to - one correspondence between\n        halo mass and the redshift at which it has that mass. Default is 0.\n    Mi : float / numpy array, optional\n        Halo mass 'Mi' at a redshift 'zi'. If float then all redshifts 'zi'\n        are solved for this halo mass. If array but zi is float, then this\n        redshift is applied to all halo masses. If both Mi and zi are\n        arrays then they have to be the same size for one - to - one\n        correspondence between halo mass and the redshift at which it\n        has that mass. Default is 1e12 Msol.\n    z : float / numpy array, optional\n        Redshift to solve commah code at. Must have zi<z else these steps\n        are skipped. Default is False, meaning commah is solved at z=zi\n\n    com : bool, optional\n        If true then solve for concentration-mass,\n        default is True.\n    mah : bool, optional\n        If true then solve for accretion rate and halo mass history,\n        default is True.\n    filename : bool / str, optional\n        If str is passed this is used as a filename for output of commah\n    verbose : bool, optional\n        If true then give comments, default is None.\n    retcosmo : bool, optional\n        Return cosmological parameters used as a dict if retcosmo = True,\n        default is None.\n\n    Returns\n    -------\n    dataset : structured dataset\n        dataset contains structured columns of size\n        (size(Mi) > size(z)) by size(z)\n\n        If mah = True and com = False then columns are\n        ('zi',float),('Mi',float),('z',float),('dMdt',float),('Mz',float)\n        where 'zi' is the starting redshift, 'Mi' is halo mass at zi\n        'z' is output redshift (NB z>zi), 'dMdt' is accretion rate [Msol/yr]\n        and 'Mz' is the halo mass at 'z' for a halo which was 'Mi' massive\n        at starting redshift 'zi'\n\n        If mah = False and com = True then columns are\n        ('zi',float),('Mi',float),('z',float),('c',float),('sig',float),('nu',float),('zf',float)\n        where 'zi' is the starting redshift, 'Mi' is halo mass at zi\n        'z' is output redshift (NB z>zi), 'c' is NFW concentration of halo\n        at the redshift 'z', 'sig' is the mass variance 'sigma',\n        'nu' is the dimensionless fluctuation for halo mass 'Mi' at 'zi',\n        'zf' is the formation redshift for a halo of mass 'Mi' at redshift 'zi'\n\n        If mah = True and com = True then columns are:\n        ('zi',float),('Mi',float),('z',float),('dMdt',float),('Mz',float),\n        ('c',float),('sig',float),('nu',float),('zf',float)\n\n    file : structured dataset with name 'filename' if passed\n\n    Raises\n    ------\n    Output -1\n        If com = False and mah = False as user has to select something.\n    Output -1\n        If 'zi' and 'Mi' are arrays of unequal size. Impossible to match\n        corresponding masses and redshifts of output.\n\n    Examples\n    --------\n    Examples should be written in doctest format, and should illustrate how\n    to use the function.\n\n    >>> import examples\n    >>> examples.runcommands() #  A series of ways to query structured dataset\n    >>> examples.plotcommands() #  Examples to plot data"}
{"task_id": "codesearchnet/python/train/143", "input": "def load(config_path: str):\n    \"\"\"\n    Load a configuration and keep it alive for the given context\n\n    :param config_path: path to a configuration file\n    \"\"\"\n    # we bind the config to _ to keep it alive\n    if os.path.splitext(config_path)[1] in ('.yaml', '.yml'):\n        _ = load_yaml_configuration(config_path, translator=PipelineTranslator())\n    elif os.path.splitext(config_path)[1] == '.py':\n        _ = load_python_configuration(config_path)\n    else:\n        raise ValueError('Unknown configuration extension: %r' % os.path.splitext(config_path)[1])\n    yield", "target": "Load a configuration and keep it alive for the given context\n\n    :param config_path: path to a configuration file"}
{"task_id": "codesearchnet/python/train/144", "input": "def transform_source(text):\n    '''Replaces instances of\n\n        repeat n:\n    by\n\n        for __VAR_i in range(n):\n\n    where __VAR_i is a string that does not appear elsewhere\n    in the code sample.\n    '''\n\n    loop_keyword = 'repeat'\n\n    nb = text.count(loop_keyword)\n    if nb == 0:\n        return text\n\n    var_names = get_unique_variable_names(text, nb)\n\n    toks = tokenize.generate_tokens(StringIO(text).readline)\n    result = []\n    replacing_keyword = False\n    for toktype, tokvalue, _, _, _ in toks:\n        if toktype == tokenize.NAME and tokvalue == loop_keyword:\n            result.extend([\n                (tokenize.NAME, 'for'),\n                (tokenize.NAME, var_names.pop()),\n                (tokenize.NAME, 'in'),\n                (tokenize.NAME, 'range'),\n                (tokenize.OP, '(')\n            ])\n            replacing_keyword = True\n        elif replacing_keyword and tokvalue == ':':\n            result.extend([\n                (tokenize.OP, ')'),\n                (tokenize.OP, ':')\n            ])\n            replacing_keyword = False\n        else:\n            result.append((toktype, tokvalue))\n    return tokenize.untokenize(result)", "target": "Replaces instances of\n\n        repeat n:\n    by\n\n        for __VAR_i in range(n):\n\n    where __VAR_i is a string that does not appear elsewhere\n    in the code sample."}
{"task_id": "codesearchnet/python/train/145", "input": "def get_unique_variable_names(text, nb):\n    '''returns a list of possible variables names that\n       are not found in the original text.'''\n    base_name = '__VAR_'\n    var_names = []\n    i = 0\n    j = 0\n    while j < nb:\n        tentative_name = base_name + str(i)\n        if text.count(tentative_name) == 0 and tentative_name not in ALL_NAMES:\n            var_names.append(tentative_name)\n            ALL_NAMES.append(tentative_name)\n            j += 1\n        i += 1\n    return var_names", "target": "returns a list of possible variables names that\n       are not found in the original text."}
{"task_id": "codesearchnet/python/train/146", "input": "def tag(self, tag):\n        \"\"\"Get a release by tag\n        \"\"\"\n        url = '%s/tags/%s' % (self, tag)\n        response = self.http.get(url, auth=self.auth)\n        response.raise_for_status()\n        return response.json()", "target": "Get a release by tag"}
{"task_id": "codesearchnet/python/train/147", "input": "def release_assets(self, release):\n        \"\"\"Assets for a given release\n        \"\"\"\n        release = self.as_id(release)\n        return self.get_list(url='%s/%s/assets' % (self, release))", "target": "Assets for a given release"}
{"task_id": "codesearchnet/python/train/148", "input": "def upload(self, release, filename, content_type=None):\n        \"\"\"Upload a file to a release\n\n        :param filename: filename to upload\n        :param content_type: optional content type\n        :return: json object from github\n        \"\"\"\n        release = self.as_id(release)\n        name = os.path.basename(filename)\n        if not content_type:\n            content_type, _ = mimetypes.guess_type(name)\n        if not content_type:\n            raise ValueError('content_type not known')\n        inputs = {'name': name}\n        url = '%s%s/%s/assets' % (self.uploads_url,\n                                  urlsplit(self.api_url).path,\n                                  release)\n        info = os.stat(filename)\n        size = info[stat.ST_SIZE]\n        response = self.http.post(\n            url, data=stream_upload(filename), auth=self.auth,\n            params=inputs,\n            headers={'content-type': content_type,\n                     'content-length': str(size)})\n        response.raise_for_status()\n        return response.json()", "target": "Upload a file to a release\n\n        :param filename: filename to upload\n        :param content_type: optional content type\n        :return: json object from github"}
{"task_id": "codesearchnet/python/train/149", "input": "def validate_tag(self, tag_name, prefix=None):\n        \"\"\"Validate ``tag_name`` with the latest tag from github\n\n        If ``tag_name`` is a valid candidate, return the latest tag from github\n        \"\"\"\n        new_version = semantic_version(tag_name)\n        current = self.latest()\n        if current:\n            tag_name = current['tag_name']\n            if prefix:\n                tag_name = tag_name[len(prefix):]\n            tag_name = semantic_version(tag_name)\n            if tag_name >= new_version:\n                what = 'equal to' if tag_name == new_version else 'older than'\n                raise GithubException(\n                    'Your local version \"%s\" is %s '\n                    'the current github version \"%s\".\\n'\n                    'Bump the local version to '\n                    'continue.' %\n                    (\n                        str(new_version),\n                        what,\n                        str(tag_name)\n                    )\n                )\n        return current", "target": "Validate ``tag_name`` with the latest tag from github\n\n        If ``tag_name`` is a valid candidate, return the latest tag from github"}
{"task_id": "codesearchnet/python/train/150", "input": "def full_url(self):\n        \"\"\"Return the full reddit URL associated with the usernote.\n\n        Arguments:\n            subreddit: the subreddit name for the note (PRAW Subreddit object)\n        \"\"\"\n        if self.link == '':\n            return None\n        else:\n            return Note._expand_url(self.link, self.subreddit)", "target": "Return the full reddit URL associated with the usernote.\n\n        Arguments:\n            subreddit: the subreddit name for the note (PRAW Subreddit object)"}
{"task_id": "codesearchnet/python/train/151", "input": "def _compress_url(link):\n        \"\"\"Convert a reddit URL into the short-hand used by usernotes.\n\n        Arguments:\n            link: a link to a comment, submission, or message (str)\n\n        Returns a String of the shorthand URL\n        \"\"\"\n        comment_re = re.compile(r'/comments/([A-Za-z\\d]{2,})(?:/[^\\s]+/([A-Za-z\\d]+))?')\n        message_re = re.compile(r'/message/messages/([A-Za-z\\d]+)')\n        matches = re.findall(comment_re, link)\n\n        if len(matches) == 0:\n            matches = re.findall(message_re, link)\n\n            if len(matches) == 0:\n                return None\n            else:\n                return 'm,' + matches[0]\n        else:\n            if matches[0][1] == '':\n                return 'l,' + matches[0][0]\n            else:\n                return 'l,' + matches[0][0] + ',' + matches[0][1]", "target": "Convert a reddit URL into the short-hand used by usernotes.\n\n        Arguments:\n            link: a link to a comment, submission, or message (str)\n\n        Returns a String of the shorthand URL"}
{"task_id": "codesearchnet/python/train/152", "input": "def _expand_url(short_link, subreddit=None):\n        \"\"\"Convert a usernote's URL short-hand into a full reddit URL.\n\n        Arguments:\n            subreddit: the subreddit the URL is for (PRAW Subreddit object or str)\n            short_link: the compressed link from a usernote (str)\n\n        Returns a String of the full URL.\n        \"\"\"\n        # Some URL structures for notes\n        message_scheme = 'https://reddit.com/message/messages/{}'\n        comment_scheme = 'https://reddit.com/r/{}/comments/{}/-/{}'\n        post_scheme = 'https://reddit.com/r/{}/comments/{}/'\n\n        if short_link == '':\n            return None\n        else:\n            parts = short_link.split(',')\n\n            if parts[0] == 'm':\n                return message_scheme.format(parts[1])\n            if parts[0] == 'l' and subreddit:\n                if len(parts) > 2:\n                    return comment_scheme.format(subreddit, parts[1], parts[2])\n                else:\n                    return post_scheme.format(subreddit, parts[1])\n            elif not subreddit:\n                raise ValueError('Subreddit name must be provided')\n            else:\n                return None", "target": "Convert a usernote's URL short-hand into a full reddit URL.\n\n        Arguments:\n            subreddit: the subreddit the URL is for (PRAW Subreddit object or str)\n            short_link: the compressed link from a usernote (str)\n\n        Returns a String of the full URL."}
{"task_id": "codesearchnet/python/train/153", "input": "def get_json(self):\n        \"\"\"Get the JSON stored on the usernotes wiki page.\n\n        Returns a dict representation of the usernotes (with the notes BLOB\n        decoded).\n\n        Raises:\n            RuntimeError if the usernotes version is incompatible with this\n                version of puni.\n        \"\"\"\n        try:\n            usernotes = self.subreddit.wiki[self.page_name].content_md\n            notes = json.loads(usernotes)\n        except NotFound:\n            self._init_notes()\n        else:\n            if notes['ver'] != self.schema:\n                raise RuntimeError(\n                    'Usernotes schema is v{0}, puni requires v{1}'.\n                    format(notes['ver'], self.schema)\n                )\n\n            self.cached_json = self._expand_json(notes)\n\n        return self.cached_json", "target": "Get the JSON stored on the usernotes wiki page.\n\n        Returns a dict representation of the usernotes (with the notes BLOB\n        decoded).\n\n        Raises:\n            RuntimeError if the usernotes version is incompatible with this\n                version of puni."}
{"task_id": "codesearchnet/python/train/154", "input": "def _init_notes(self):\n        \"\"\"Set up the UserNotes page with the initial JSON schema.\"\"\"\n        self.cached_json = {\n            'ver': self.schema,\n            'users': {},\n            'constants': {\n                'users': [x.name for x in self.subreddit.moderator()],\n                'warnings': Note.warnings\n            }\n        }\n\n        self.set_json('Initializing JSON via puni', True)", "target": "Set up the UserNotes page with the initial JSON schema."}
{"task_id": "codesearchnet/python/train/155", "input": "def set_json(self, reason='', new_page=False):\n        \"\"\"Send the JSON from the cache to the usernotes wiki page.\n\n        Arguments:\n            reason: the change reason that will be posted to the wiki changelog\n                (str)\n        Raises:\n            OverflowError if the new JSON data is greater than max_page_size\n        \"\"\"\n        compressed_json = json.dumps(self._compress_json(self.cached_json))\n\n        if len(compressed_json) > self.max_page_size:\n            raise OverflowError(\n                'Usernotes page is too large (>{0} characters)'.\n                format(self.max_page_size)\n            )\n\n        if new_page:\n            self.subreddit.wiki.create(\n                self.page_name,\n                compressed_json,\n                reason\n            )\n            # Set the page as hidden and available to moderators only\n            self.subreddit.wiki[self.page_name].mod.update(False, permlevel=2)\n        else:\n            self.subreddit.wiki[self.page_name].edit(\n                compressed_json,\n                reason\n            )", "target": "Send the JSON from the cache to the usernotes wiki page.\n\n        Arguments:\n            reason: the change reason that will be posted to the wiki changelog\n                (str)\n        Raises:\n            OverflowError if the new JSON data is greater than max_page_size"}
{"task_id": "codesearchnet/python/train/156", "input": "def get_notes(self, user):\n        \"\"\"Return a list of Note objects for the given user.\n\n        Return an empty list if no notes are found.\n\n        Arguments:\n            user: the user to search for in the usernotes (str)\n        \"\"\"\n        # Try to search for all notes on a user, return an empty list if none\n        # are found.\n        try:\n            users_notes = []\n\n            for note in self.cached_json['users'][user]['ns']:\n                users_notes.append(Note(\n                    user=user,\n                    note=note['n'],\n                    subreddit=self.subreddit,\n                    mod=self._mod_from_index(note['m']),\n                    link=note['l'],\n                    warning=self._warning_from_index(note['w']),\n                    note_time=note['t']\n                ))\n\n            return users_notes\n        except KeyError:\n            # User not found\n            return []", "target": "Return a list of Note objects for the given user.\n\n        Return an empty list if no notes are found.\n\n        Arguments:\n            user: the user to search for in the usernotes (str)"}
{"task_id": "codesearchnet/python/train/157", "input": "def _expand_json(self, j):\n        \"\"\"Decompress the BLOB portion of the usernotes.\n\n        Arguments:\n            j: the JSON returned from the wiki page (dict)\n\n        Returns a Dict with the 'blob' key removed and a 'users' key added\n        \"\"\"\n        decompressed_json = copy.copy(j)\n        decompressed_json.pop('blob', None)  # Remove BLOB portion of JSON\n\n        # Decode and decompress JSON\n        compressed_data = base64.b64decode(j['blob'])\n        original_json = zlib.decompress(compressed_data).decode('utf-8')\n\n        decompressed_json['users'] = json.loads(original_json)  # Insert users\n\n        return decompressed_json", "target": "Decompress the BLOB portion of the usernotes.\n\n        Arguments:\n            j: the JSON returned from the wiki page (dict)\n\n        Returns a Dict with the 'blob' key removed and a 'users' key added"}
{"task_id": "codesearchnet/python/train/158", "input": "def _compress_json(self, j):\n        \"\"\"Compress the BLOB data portion of the usernotes.\n\n        Arguments:\n            j: the JSON in Schema v5 format (dict)\n\n        Returns a dict with the 'users' key removed and 'blob' key added\n        \"\"\"\n        compressed_json = copy.copy(j)\n        compressed_json.pop('users', None)\n\n        compressed_data = zlib.compress(\n            json.dumps(j['users']).encode('utf-8'),\n            self.zlib_compression_strength\n        )\n        b64_data = base64.b64encode(compressed_data).decode('utf-8')\n\n        compressed_json['blob'] = b64_data\n\n        return compressed_json", "target": "Compress the BLOB data portion of the usernotes.\n\n        Arguments:\n            j: the JSON in Schema v5 format (dict)\n\n        Returns a dict with the 'users' key removed and 'blob' key added"}
{"task_id": "codesearchnet/python/train/159", "input": "def add_note(self, note):\n        \"\"\"Add a note to the usernotes wiki page.\n\n        Arguments:\n            note: the note to be added (Note)\n\n        Returns the update message for the usernotes wiki\n\n        Raises:\n            ValueError when the warning type of the note can not be found in the\n                stored list of warnings.\n        \"\"\"\n        notes = self.cached_json\n\n        if not note.moderator:\n            note.moderator = self.r.user.me().name\n\n        # Get index of moderator in mod list from usernotes\n        # Add moderator to list if not already there\n        try:\n            mod_index = notes['constants']['users'].index(note.moderator)\n        except ValueError:\n            notes['constants']['users'].append(note.moderator)\n            mod_index = notes['constants']['users'].index(note.moderator)\n\n        # Get index of warning type from warnings list\n        # Add warning type to list if not already there\n        try:\n            warn_index = notes['constants']['warnings'].index(note.warning)\n        except ValueError:\n            if note.warning in Note.warnings:\n                notes['constants']['warnings'].append(note.warning)\n                warn_index = notes['constants']['warnings'].index(note.warning)\n            else:\n                raise ValueError('Warning type not valid: ' + note.warning)\n\n        new_note = {\n            'n': note.note,\n            't': note.time,\n            'm': mod_index,\n            'l': note.link,\n            'w': warn_index\n        }\n\n        try:\n            notes['users'][note.username]['ns'].insert(0, new_note)\n        except KeyError:\n            notes['users'][note.username] = {'ns': [new_note]}\n\n        return '\"create new note on user {}\" via puni'.format(note.username)", "target": "Add a note to the usernotes wiki page.\n\n        Arguments:\n            note: the note to be added (Note)\n\n        Returns the update message for the usernotes wiki\n\n        Raises:\n            ValueError when the warning type of the note can not be found in the\n                stored list of warnings."}
{"task_id": "codesearchnet/python/train/160", "input": "def remove_note(self, username, index):\n        \"\"\"Remove a single usernote from the usernotes.\n\n        Arguments:\n            username: the user that for whom you're removing a note (str)\n            index: the index of the note which is to be removed (int)\n\n        Returns the update message for the usernotes wiki\n        \"\"\"\n        self.cached_json['users'][username]['ns'].pop(index)\n\n        # Go ahead and remove the user's entry if they have no more notes left\n        if len(self.cached_json['users'][username]['ns']) == 0:\n            del self.cached_json['users'][username]\n\n        return '\"delete note #{} on user {}\" via puni'.format(index, username)", "target": "Remove a single usernote from the usernotes.\n\n        Arguments:\n            username: the user that for whom you're removing a note (str)\n            index: the index of the note which is to be removed (int)\n\n        Returns the update message for the usernotes wiki"}
{"task_id": "codesearchnet/python/train/161", "input": "def load(self):\n        \"\"\" Function load\n        Get the list of all objects\n\n        @return RETURN: A ForemanItem list\n        \"\"\"\n        cl_tmp = self.api.list(self.objName, limit=self.searchLimit).values()\n        cl = []\n        for i in cl_tmp:\n            cl.extend(i)\n        return {x[self.index]: ItemPuppetClass(self.api, x['id'],\n                                               self.objName, self.payloadObj,\n                                               x)\n                for x in cl}", "target": "Function load\n        Get the list of all objects\n\n        @return RETURN: A ForemanItem list"}
{"task_id": "codesearchnet/python/train/162", "input": "def is_related_to(item, app_id, app_ver=None):\n    \"\"\"Return True if the item relates to the given app_id (and app_ver, if passed).\"\"\"\n    versionRange = item.get('versionRange')\n    if not versionRange:\n        return True\n\n    for vR in versionRange:\n        if not vR.get('targetApplication'):\n            return True\n        if get_related_targetApplication(vR, app_id, app_ver) is not None:\n            return True\n    return False", "target": "Return True if the item relates to the given app_id (and app_ver, if passed)."}
{"task_id": "codesearchnet/python/train/163", "input": "def get_related_targetApplication(vR, app_id, app_ver):\n    \"\"\"Return the first matching target application in this version range.\n    Returns None if there are no target applications or no matching ones.\"\"\"\n    targetApplication = vR.get('targetApplication')\n    if not targetApplication:\n        return None\n\n    for tA in targetApplication:\n        guid = tA.get('guid')\n        if not guid or guid == app_id:\n            if not app_ver:\n                return tA\n            # We purposefully use maxVersion only, so that the blocklist contains items\n            # whose minimum version is ahead of the version we get passed. This means\n            # the blocklist we serve is \"future-proof\" for app upgrades.\n            if between(version_int(app_ver), '0', tA.get('maxVersion', '*')):\n                return tA\n\n    return None", "target": "Return the first matching target application in this version range.\n    Returns None if there are no target applications or no matching ones."}
{"task_id": "codesearchnet/python/train/164", "input": "def write_addons_items(xml_tree, records, app_id, api_ver=3, app_ver=None):\n    \"\"\"Generate the addons blocklists.\n\n    <emItem blockID=\"i372\" id=\"5nc3QHFgcb@r06Ws9gvNNVRfH.com\">\n      <versionRange minVersion=\"0\" maxVersion=\"*\" severity=\"3\">\n        <targetApplication id=\"{ec8030f7-c20a-464f-9b0e-13a3a9e97384}\">\n          <versionRange minVersion=\"39.0a1\" maxVersion=\"*\"/>\n        </targetApplication>\n      </versionRange>\n      <prefs>\n        <pref>browser.startup.homepage</pref>\n        <pref>browser.search.defaultenginename</pref>\n      </prefs>\n    </emItem>\n    \"\"\"\n    if not records:\n        return\n\n    emItems = etree.SubElement(xml_tree, 'emItems')\n    groupby = {}\n    for item in records:\n        if is_related_to(item, app_id, app_ver):\n            if item['guid'] in groupby:\n                emItem = groupby[item['guid']]\n                # When creating new records from the Kinto Admin we don't have proper blockID.\n                if 'blockID' in item:\n                    # Remove the first caracter which is the letter i to\n                    # compare the numeric value i45 < i356.\n                    current_blockID = int(item['blockID'][1:])\n                    previous_blockID = int(emItem.attrib['blockID'][1:])\n                    # Group by and keep the biggest blockID in the XML file.\n                    if current_blockID > previous_blockID:\n                        emItem.attrib['blockID'] = item['blockID']\n                else:\n                    # If the latest entry does not have any blockID attribute, its\n                    # ID should be used. (the list of records is sorted by ascending\n                    # last_modified).\n                    # See https://bugzilla.mozilla.org/show_bug.cgi?id=1473194\n                    emItem.attrib['blockID'] = item['id']\n            else:\n                emItem = etree.SubElement(emItems, 'emItem',\n                                          blockID=item.get('blockID', item['id']))\n                groupby[item['guid']] = emItem\n                prefs = etree.SubElement(emItem, 'prefs')\n                for p in item['prefs']:\n                    pref = etree.SubElement(prefs, 'pref')\n                    pref.text = p\n\n            # Set the add-on ID\n            emItem.set('id', item['guid'])\n\n            for field in ['name', 'os']:\n                if field in item:\n                    emItem.set(field, item[field])\n\n            build_version_range(emItem, item, app_id)", "target": "Generate the addons blocklists.\n\n    <emItem blockID=\"i372\" id=\"5nc3QHFgcb@r06Ws9gvNNVRfH.com\">\n      <versionRange minVersion=\"0\" maxVersion=\"*\" severity=\"3\">\n        <targetApplication id=\"{ec8030f7-c20a-464f-9b0e-13a3a9e97384}\">\n          <versionRange minVersion=\"39.0a1\" maxVersion=\"*\"/>\n        </targetApplication>\n      </versionRange>\n      <prefs>\n        <pref>browser.startup.homepage</pref>\n        <pref>browser.search.defaultenginename</pref>\n      </prefs>\n    </emItem>"}
{"task_id": "codesearchnet/python/train/165", "input": "def write_plugin_items(xml_tree, records, app_id, api_ver=3, app_ver=None):\n    \"\"\"Generate the plugin blocklists.\n\n    <pluginItem blockID=\"p422\">\n        <match name=\"filename\" exp=\"JavaAppletPlugin\\\\.plugin\"/>\n        <versionRange minVersion=\"Java 7 Update 16\"\n                      maxVersion=\"Java 7 Update 24\"\n                      severity=\"0\" vulnerabilitystatus=\"1\">\n            <targetApplication id=\"{ec8030f7-c20a-464f-9b0e-13a3a9e97384}\">\n                <versionRange minVersion=\"17.0\" maxVersion=\"*\"/>\n            </targetApplication>\n        </versionRange>\n    </pluginItem>\n    \"\"\"\n\n    if not records:\n        return\n\n    pluginItems = etree.SubElement(xml_tree, 'pluginItems')\n    for item in records:\n        for versionRange in item.get('versionRange', []):\n            if not versionRange.get('targetApplication'):\n                add_plugin_item(pluginItems, item, versionRange,\n                                app_id=app_id, api_ver=api_ver,\n                                app_ver=app_ver)\n            else:\n                targetApplication = get_related_targetApplication(versionRange, app_id, app_ver)\n                if targetApplication is not None:\n                    add_plugin_item(pluginItems, item, versionRange, targetApplication,\n                                    app_id=app_id, api_ver=api_ver,\n                                    app_ver=app_ver)", "target": "Generate the plugin blocklists.\n\n    <pluginItem blockID=\"p422\">\n        <match name=\"filename\" exp=\"JavaAppletPlugin\\\\.plugin\"/>\n        <versionRange minVersion=\"Java 7 Update 16\"\n                      maxVersion=\"Java 7 Update 24\"\n                      severity=\"0\" vulnerabilitystatus=\"1\">\n            <targetApplication id=\"{ec8030f7-c20a-464f-9b0e-13a3a9e97384}\">\n                <versionRange minVersion=\"17.0\" maxVersion=\"*\"/>\n            </targetApplication>\n        </versionRange>\n    </pluginItem>"}
{"task_id": "codesearchnet/python/train/166", "input": "def write_gfx_items(xml_tree, records, app_id, api_ver=3):\n    \"\"\"Generate the gfxBlacklistEntry.\n\n    <gfxBlacklistEntry blockID=\"g35\">\n        <os>WINNT 6.1</os>\n        <vendor>0x10de</vendor>\n        <devices>\n            <device>0x0a6c</device>\n        </devices>\n        <feature>DIRECT2D</feature>\n        <featureStatus>BLOCKED_DRIVER_VERSION</featureStatus>\n        <driverVersion>8.17.12.5896</driverVersion>\n        <driverVersionComparator>LESS_THAN_OR_EQUAL</driverVersionComparator>\n        <versionRange minVersion=\"3.2\" maxVersion=\"3.4\" />\n    </gfxBlacklistEntry>\n    \"\"\"\n    if not records:\n        return\n\n    gfxItems = etree.SubElement(xml_tree, 'gfxItems')\n    for item in records:\n        is_record_related = ('guid' not in item or item['guid'] == app_id)\n\n        if is_record_related:\n            entry = etree.SubElement(gfxItems, 'gfxBlacklistEntry',\n                                     blockID=item.get('blockID', item['id']))\n            fields = ['os', 'vendor', 'feature', 'featureStatus',\n                      'driverVersion', 'driverVersionComparator']\n            for field in fields:\n                if field in item:\n                    node = etree.SubElement(entry, field)\n                    node.text = item[field]\n\n            # Devices\n            if item['devices']:\n                devices = etree.SubElement(entry, 'devices')\n                for d in item['devices']:\n                    device = etree.SubElement(devices, 'device')\n                    device.text = d\n\n            if 'versionRange' in item:\n                version = item['versionRange']\n                versionRange = etree.SubElement(entry, 'versionRange')\n\n                for field in ['minVersion', 'maxVersion']:\n                    value = version.get(field)\n                    if value:\n                        versionRange.set(field, str(value))", "target": "Generate the gfxBlacklistEntry.\n\n    <gfxBlacklistEntry blockID=\"g35\">\n        <os>WINNT 6.1</os>\n        <vendor>0x10de</vendor>\n        <devices>\n            <device>0x0a6c</device>\n        </devices>\n        <feature>DIRECT2D</feature>\n        <featureStatus>BLOCKED_DRIVER_VERSION</featureStatus>\n        <driverVersion>8.17.12.5896</driverVersion>\n        <driverVersionComparator>LESS_THAN_OR_EQUAL</driverVersionComparator>\n        <versionRange minVersion=\"3.2\" maxVersion=\"3.4\" />\n    </gfxBlacklistEntry>"}
{"task_id": "codesearchnet/python/train/167", "input": "def write_cert_items(xml_tree, records, api_ver=3, app_id=None, app_ver=None):\n    \"\"\"Generate the certificate blocklists.\n\n    <certItem issuerName=\"MIGQMQswCQYD...IENB\">\n      <serialNumber>UoRGnb96CUDTxIqVry6LBg==</serialNumber>\n    </certItem>\n\n    or\n\n    <certItem subject='MCIxIDAeBgNVBAMMF0Fub3RoZXIgVGVzdCBFbmQtZW50aXR5'\n              pubKeyHash='VCIlmPM9NkgFQtrs4Oa5TeFcDu6MWRTKSNdePEhOgD8='>\n    </certItem>\n    \"\"\"\n    if not records or not should_include_certs(app_id, app_ver):\n        return\n\n    certItems = etree.SubElement(xml_tree, 'certItems')\n    for item in records:\n        if item.get('subject') and item.get('pubKeyHash'):\n            cert = etree.SubElement(certItems, 'certItem',\n                                    subject=item['subject'],\n                                    pubKeyHash=item['pubKeyHash'])\n        else:\n            cert = etree.SubElement(certItems, 'certItem',\n                                    issuerName=item['issuerName'])\n            serialNumber = etree.SubElement(cert, 'serialNumber')\n            serialNumber.text = item['serialNumber']", "target": "Generate the certificate blocklists.\n\n    <certItem issuerName=\"MIGQMQswCQYD...IENB\">\n      <serialNumber>UoRGnb96CUDTxIqVry6LBg==</serialNumber>\n    </certItem>\n\n    or\n\n    <certItem subject='MCIxIDAeBgNVBAMMF0Fub3RoZXIgVGVzdCBFbmQtZW50aXR5'\n              pubKeyHash='VCIlmPM9NkgFQtrs4Oa5TeFcDu6MWRTKSNdePEhOgD8='>\n    </certItem>"}
{"task_id": "codesearchnet/python/train/168", "input": "def label(self, name, color, update=True):\n        \"\"\"Create or update a label\n        \"\"\"\n        url = '%s/labels' % self\n        data = dict(name=name, color=color)\n        response = self.http.post(\n            url, json=data, auth=self.auth, headers=self.headers\n        )\n        if response.status_code == 201:\n            return True\n        elif response.status_code == 422 and update:\n            url = '%s/%s' % (url, name)\n            response = self.http.patch(\n                url, json=data, auth=self.auth, headers=self.headers\n            )\n        response.raise_for_status()\n        return False", "target": "Create or update a label"}
{"task_id": "codesearchnet/python/train/169", "input": "def translate(source, dictionary):\n    '''A dictionary with a one-to-one translation of keywords is used\n    to provide the transformation.\n    '''\n    toks = tokenize.generate_tokens(StringIO(source).readline)\n    result = []\n    for toktype, tokvalue, _, _, _ in toks:\n        if toktype == tokenize.NAME and tokvalue in dictionary:\n            result.append((toktype, dictionary[tokvalue]))\n        else:\n            result.append((toktype, tokvalue))\n    return tokenize.untokenize(result)", "target": "A dictionary with a one-to-one translation of keywords is used\n    to provide the transformation."}
{"task_id": "codesearchnet/python/train/170", "input": "def enhance(self):\n        \"\"\" Function enhance\n        Enhance the object with new item or enhanced items\n        \"\"\"\n        self.update({'images':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemImages)})", "target": "Function enhance\n        Enhance the object with new item or enhanced items"}
{"task_id": "codesearchnet/python/train/171", "input": "async def _run_payloads(self):\n        \"\"\"Async component of _run\"\"\"\n        delay = 0.0\n        try:\n            while self.running.is_set():\n                await self._start_payloads()\n                await self._reap_payloads()\n                await asyncio.sleep(delay)\n                delay = min(delay + 0.1, 1.0)\n        except Exception:\n            await self._cancel_payloads()\n            raise", "target": "Async component of _run"}
{"task_id": "codesearchnet/python/train/172", "input": "async def _start_payloads(self):\n        \"\"\"Start all queued payloads\"\"\"\n        with self._lock:\n            for coroutine in self._payloads:\n                task = self.event_loop.create_task(coroutine())\n                self._tasks.add(task)\n            self._payloads.clear()\n        await asyncio.sleep(0)", "target": "Start all queued payloads"}
{"task_id": "codesearchnet/python/train/173", "input": "async def _reap_payloads(self):\n        \"\"\"Clean up all finished payloads\"\"\"\n        for task in self._tasks.copy():\n            if task.done():\n                self._tasks.remove(task)\n                if task.exception() is not None:\n                    raise task.exception()\n        await asyncio.sleep(0)", "target": "Clean up all finished payloads"}
{"task_id": "codesearchnet/python/train/174", "input": "async def _cancel_payloads(self):\n        \"\"\"Cancel all remaining payloads\"\"\"\n        for task in self._tasks:\n            task.cancel()\n            await asyncio.sleep(0)\n        for task in self._tasks:\n            while not task.done():\n                await asyncio.sleep(0.1)\n                task.cancel()", "target": "Cancel all remaining payloads"}
{"task_id": "codesearchnet/python/train/175", "input": "def check_password(password: str, encrypted: str) -> bool:\n    \"\"\" Check a plaintext password against a hashed password. \"\"\"\n    # some old passwords have {crypt} in lower case, and passlib wants it to be\n    # in upper case.\n    if encrypted.startswith(\"{crypt}\"):\n        encrypted = \"{CRYPT}\" + encrypted[7:]\n    return pwd_context.verify(password, encrypted)", "target": "Check a plaintext password against a hashed password."}
{"task_id": "codesearchnet/python/train/176", "input": "def validate(ctx, sandbox):\n    \"\"\"Check if version of repository is semantic\n    \"\"\"\n    m = RepoManager(ctx.obj['agile'])\n    if not sandbox or m.can_release('sandbox'):\n        click.echo(m.validate_version())", "target": "Check if version of repository is semantic"}
{"task_id": "codesearchnet/python/train/177", "input": "def reset(self, force_flush_cache: bool = False) -> None:\n        \"\"\"\n        Reset transaction back to original state, discarding all\n        uncompleted transactions.\n        \"\"\"\n        super(LDAPwrapper, self).reset()\n        if len(self._transactions) == 0:\n            raise RuntimeError(\"reset called outside a transaction.\")\n        self._transactions[-1] = []", "target": "Reset transaction back to original state, discarding all\n        uncompleted transactions."}
{"task_id": "codesearchnet/python/train/178", "input": "def _cache_get_for_dn(self, dn: str) -> Dict[str, bytes]:\n        \"\"\"\n        Object state is cached. When an update is required the update will be\n        simulated on this cache, so that rollback information can be correct.\n        This function retrieves the cached data.\n        \"\"\"\n\n        # no cached item, retrieve from ldap\n        self._do_with_retry(\n            lambda obj: obj.search(\n                dn,\n                '(objectclass=*)',\n                ldap3.BASE,\n                attributes=['*', '+']))\n        results = self._obj.response\n        if len(results) < 1:\n            raise NoSuchObject(\"No results finding current value\")\n        if len(results) > 1:\n            raise RuntimeError(\"Too many results finding current value\")\n\n        return results[0]['raw_attributes']", "target": "Object state is cached. When an update is required the update will be\n        simulated on this cache, so that rollback information can be correct.\n        This function retrieves the cached data."}
{"task_id": "codesearchnet/python/train/179", "input": "def is_dirty(self) -> bool:\n        \"\"\" Are there uncommitted changes? \"\"\"\n        if len(self._transactions) == 0:\n            raise RuntimeError(\"is_dirty called outside a transaction.\")\n        if len(self._transactions[-1]) > 0:\n            return True\n        return False", "target": "Are there uncommitted changes?"}
{"task_id": "codesearchnet/python/train/180", "input": "def leave_transaction_management(self) -> None:\n        \"\"\"\n        End a transaction. Must not be dirty when doing so. ie. commit() or\n        rollback() must be called if changes made. If dirty, changes will be\n        discarded.\n        \"\"\"\n        if len(self._transactions) == 0:\n            raise RuntimeError(\"leave_transaction_management called outside transaction\")\n        elif len(self._transactions[-1]) > 0:\n            raise RuntimeError(\"leave_transaction_management called with uncommited rollbacks\")\n        else:\n            self._transactions.pop()", "target": "End a transaction. Must not be dirty when doing so. ie. commit() or\n        rollback() must be called if changes made. If dirty, changes will be\n        discarded."}
{"task_id": "codesearchnet/python/train/181", "input": "def commit(self) -> None:\n        \"\"\"\n        Attempt to commit all changes to LDAP database. i.e. forget all\n        rollbacks.  However stay inside transaction management.\n        \"\"\"\n        if len(self._transactions) == 0:\n            raise RuntimeError(\"commit called outside transaction\")\n\n        # If we have nested transactions, we don't actually commit, but push\n        # rollbacks up to previous transaction.\n        if len(self._transactions) > 1:\n            for on_rollback in reversed(self._transactions[-1]):\n                self._transactions[-2].insert(0, on_rollback)\n\n        _debug(\"commit\")\n        self.reset()", "target": "Attempt to commit all changes to LDAP database. i.e. forget all\n        rollbacks.  However stay inside transaction management."}
{"task_id": "codesearchnet/python/train/182", "input": "def rollback(self) -> None:\n        \"\"\"\n        Roll back to previous database state. However stay inside transaction\n        management.\n        \"\"\"\n        if len(self._transactions) == 0:\n            raise RuntimeError(\"rollback called outside transaction\")\n\n        _debug(\"rollback:\", self._transactions[-1])\n        # if something goes wrong here, nothing we can do about it, leave\n        # database as is.\n        try:\n            # for every rollback action ...\n            for on_rollback in self._transactions[-1]:\n                # execute it\n                _debug(\"--> rolling back\", on_rollback)\n                self._do_with_retry(on_rollback)\n        except:  # noqa: E722\n            _debug(\"--> rollback failed\")\n            exc_class, exc, tb = sys.exc_info()\n            raise tldap.exceptions.RollbackError(\n                \"FATAL Unrecoverable rollback error: %r\" % exc)\n        finally:\n            # reset everything to clean state\n            _debug(\"--> rollback success\")\n            self.reset()", "target": "Roll back to previous database state. However stay inside transaction\n        management."}
{"task_id": "codesearchnet/python/train/183", "input": "def _process(self, on_commit: UpdateCallable, on_rollback: UpdateCallable) -> Any:\n        \"\"\"\n        Process action. oncommit is a callback to execute action, onrollback is\n        a callback to execute if the oncommit() has been called and a rollback\n        is required\n        \"\"\"\n\n        _debug(\"---> commiting\", on_commit)\n        result = self._do_with_retry(on_commit)\n\n        if len(self._transactions) > 0:\n            # add statement to rollback log in case something goes wrong\n            self._transactions[-1].insert(0, on_rollback)\n\n        return result", "target": "Process action. oncommit is a callback to execute action, onrollback is\n        a callback to execute if the oncommit() has been called and a rollback\n        is required"}
{"task_id": "codesearchnet/python/train/184", "input": "def add(self, dn: str, mod_list: dict) -> None:\n        \"\"\"\n        Add a DN to the LDAP database; See ldap module. Doesn't return a result\n        if transactions enabled.\n        \"\"\"\n\n        _debug(\"add\", self, dn, mod_list)\n\n        # if rollback of add required, delete it\n        def on_commit(obj):\n            obj.add(dn, None, mod_list)\n\n        def on_rollback(obj):\n            obj.delete(dn)\n\n        # process this action\n        return self._process(on_commit, on_rollback)", "target": "Add a DN to the LDAP database; See ldap module. Doesn't return a result\n        if transactions enabled."}
{"task_id": "codesearchnet/python/train/185", "input": "def modify(self, dn: str, mod_list: dict) -> None:\n        \"\"\"\n        Modify a DN in the LDAP database; See ldap module. Doesn't return a\n        result if transactions enabled.\n        \"\"\"\n\n        _debug(\"modify\", self, dn, mod_list)\n\n        # need to work out how to reverse changes in mod_list; result in revlist\n        revlist = {}\n\n        # get the current cached attributes\n        result = self._cache_get_for_dn(dn)\n\n        # find the how to reverse mod_list (for rollback) and put result in\n        # revlist. Also simulate actions on cache.\n        for mod_type, l in six.iteritems(mod_list):\n            for mod_op, mod_vals in l:\n\n                _debug(\"attribute:\", mod_type)\n                if mod_type in result:\n                    _debug(\"attribute cache:\", result[mod_type])\n                else:\n                    _debug(\"attribute cache is empty\")\n                _debug(\"attribute modify:\", (mod_op, mod_vals))\n\n                if mod_vals is not None:\n                    if not isinstance(mod_vals, list):\n                        mod_vals = [mod_vals]\n\n                if mod_op == ldap3.MODIFY_ADD:\n                    # reverse of MODIFY_ADD is MODIFY_DELETE\n                    reverse = (ldap3.MODIFY_DELETE, mod_vals)\n\n                elif mod_op == ldap3.MODIFY_DELETE and len(mod_vals) > 0:\n                    # Reverse of MODIFY_DELETE is MODIFY_ADD, but only if value\n                    # is given if mod_vals is None, this means all values where\n                    # deleted.\n                    reverse = (ldap3.MODIFY_ADD, mod_vals)\n\n                elif mod_op == ldap3.MODIFY_DELETE \\\n                        or mod_op == ldap3.MODIFY_REPLACE:\n                    if mod_type in result:\n                        # If MODIFY_DELETE with no values or MODIFY_REPLACE\n                        # then we have to replace all attributes with cached\n                        # state\n                        reverse = (\n                            ldap3.MODIFY_REPLACE,\n                            tldap.modlist.escape_list(result[mod_type])\n                        )\n                    else:\n                        # except if we have no cached state for this DN, in\n                        # which case we delete it.\n                        reverse = (ldap3.MODIFY_DELETE, [])\n\n                else:\n                    raise RuntimeError(\"mod_op of %d not supported\" % mod_op)\n\n                reverse = [reverse]\n                _debug(\"attribute reverse:\", reverse)\n                if mod_type in result:\n                    _debug(\"attribute cache:\", result[mod_type])\n                else:\n                    _debug(\"attribute cache is empty\")\n\n                revlist[mod_type] = reverse\n\n        _debug(\"--\")\n        _debug(\"mod_list:\", mod_list)\n        _debug(\"revlist:\", revlist)\n        _debug(\"--\")\n\n        # now the hard stuff is over, we get to the easy stuff\n        def on_commit(obj):\n            obj.modify(dn, mod_list)\n\n        def on_rollback(obj):\n            obj.modify(dn, revlist)\n\n        return self._process(on_commit, on_rollback)", "target": "Modify a DN in the LDAP database; See ldap module. Doesn't return a\n        result if transactions enabled."}
{"task_id": "codesearchnet/python/train/186", "input": "def modify_no_rollback(self, dn: str, mod_list: dict):\n        \"\"\"\n        Modify a DN in the LDAP database; See ldap module. Doesn't return a\n        result if transactions enabled.\n        \"\"\"\n\n        _debug(\"modify_no_rollback\", self, dn, mod_list)\n        result = self._do_with_retry(lambda obj: obj.modify_s(dn, mod_list))\n        _debug(\"--\")\n\n        return result", "target": "Modify a DN in the LDAP database; See ldap module. Doesn't return a\n        result if transactions enabled."}
{"task_id": "codesearchnet/python/train/187", "input": "def delete(self, dn: str) -> None:\n        \"\"\"\n        delete a dn in the ldap database; see ldap module. doesn't return a\n        result if transactions enabled.\n        \"\"\"\n\n        _debug(\"delete\", self)\n\n        # get copy of cache\n        result = self._cache_get_for_dn(dn)\n\n        # remove special values that can't be added\n        def delete_attribute(name):\n            if name in result:\n                del result[name]\n        delete_attribute('entryUUID')\n        delete_attribute('structuralObjectClass')\n        delete_attribute('modifiersName')\n        delete_attribute('subschemaSubentry')\n        delete_attribute('entryDN')\n        delete_attribute('modifyTimestamp')\n        delete_attribute('entryCSN')\n        delete_attribute('createTimestamp')\n        delete_attribute('creatorsName')\n        delete_attribute('hasSubordinates')\n        delete_attribute('pwdFailureTime')\n        delete_attribute('pwdChangedTime')\n        # turn into mod_list list.\n        mod_list = tldap.modlist.addModlist(result)\n\n        _debug(\"revlist:\", mod_list)\n\n        # on commit carry out action; on rollback restore cached state\n        def on_commit(obj):\n            obj.delete(dn)\n\n        def on_rollback(obj):\n            obj.add(dn, None, mod_list)\n\n        return self._process(on_commit, on_rollback)", "target": "delete a dn in the ldap database; see ldap module. doesn't return a\n        result if transactions enabled."}
{"task_id": "codesearchnet/python/train/188", "input": "def rename(self, dn: str, new_rdn: str, new_base_dn: Optional[str] = None) -> None:\n        \"\"\"\n        rename a dn in the ldap database; see ldap module. doesn't return a\n        result if transactions enabled.\n        \"\"\"\n\n        _debug(\"rename\", self, dn, new_rdn, new_base_dn)\n\n        # split up the parameters\n        split_dn = tldap.dn.str2dn(dn)\n        split_newrdn = tldap.dn.str2dn(new_rdn)\n        assert(len(split_newrdn) == 1)\n\n        # make dn unqualified\n        rdn = tldap.dn.dn2str(split_dn[0:1])\n\n        # make newrdn fully qualified dn\n        tmplist = [split_newrdn[0]]\n        if new_base_dn is not None:\n            tmplist.extend(tldap.dn.str2dn(new_base_dn))\n            old_base_dn = tldap.dn.dn2str(split_dn[1:])\n        else:\n            tmplist.extend(split_dn[1:])\n            old_base_dn = None\n        newdn = tldap.dn.dn2str(tmplist)\n\n        _debug(\"--> commit  \", self, dn, new_rdn, new_base_dn)\n        _debug(\"--> rollback\", self, newdn, rdn, old_base_dn)\n\n        # on commit carry out action; on rollback reverse rename\n        def on_commit(obj):\n            obj.modify_dn(dn, new_rdn, new_superior=new_base_dn)\n\n        def on_rollback(obj):\n            obj.modify_dn(newdn, rdn, new_superior=old_base_dn)\n\n        return self._process(on_commit, on_rollback)", "target": "rename a dn in the ldap database; see ldap module. doesn't return a\n        result if transactions enabled."}
{"task_id": "codesearchnet/python/train/189", "input": "def fail(self) -> None:\n        \"\"\" for testing purposes only. always fail in commit \"\"\"\n\n        _debug(\"fail\")\n\n        # on commit carry out action; on rollback reverse rename\n        def on_commit(_obj):\n            raise_testfailure(\"commit\")\n\n        def on_rollback(_obj):\n            raise_testfailure(\"rollback\")\n\n        return self._process(on_commit, on_rollback)", "target": "for testing purposes only. always fail in commit"}
{"task_id": "codesearchnet/python/train/190", "input": "def __experimental_range(start, stop, var, cond, loc={}):\n    '''Utility function made to reproduce range() with unit integer step\n       but with the added possibility of specifying a condition\n       on the looping variable  (e.g. var % 2  == 0)\n    '''\n    locals().update(loc)\n    if start < stop:\n        for __ in range(start, stop):\n            locals()[var] = __\n            if eval(cond, globals(), locals()):\n                yield __\n    else:\n        for __ in range(start, stop, -1):\n            locals()[var] = __\n            if eval(cond, globals(), locals()):\n                yield __", "target": "Utility function made to reproduce range() with unit integer step\n       but with the added possibility of specifying a condition\n       on the looping variable  (e.g. var % 2  == 0)"}
{"task_id": "codesearchnet/python/train/191", "input": "def create_for(line, search_result):\n    '''Create a new \"for loop\" line as a replacement for the original code.\n    '''\n    try:\n        return line.format(search_result.group(\"indented_for\"),\n                           search_result.group(\"var\"),\n                           search_result.group(\"start\"),\n                           search_result.group(\"stop\"),\n                           search_result.group(\"cond\"))\n    except IndexError:\n        return line.format(search_result.group(\"indented_for\"),\n                           search_result.group(\"var\"),\n                           search_result.group(\"start\"),\n                           search_result.group(\"stop\"))", "target": "Create a new \"for loop\" line as a replacement for the original code."}
{"task_id": "codesearchnet/python/train/192", "input": "def setOverrideValue(self, attributes, hostName):\n        \"\"\" Function __setitem__\n        Set a parameter of a foreman object as a dict\n\n        @param key: The key to modify\n        @param attribute: The data\n        @return RETURN: The API result\n        \"\"\"\n        self['override'] = True\n        attrType = type(attributes)\n        if attrType is dict:\n            self['parameter_type'] = 'hash'\n        elif attrType is list:\n            self['parameter_type'] = 'array'\n        else:\n            self['parameter_type'] = 'string'\n        orv = self.getOverrideValueForHost(hostName)\n        if orv:\n            orv['value'] = attributes\n            return True\n        else:\n            return self.api.create('{}/{}/{}'.format(self.objName,\n                                                     self.key,\n                                                     'override_values'),\n                                   {\"override_value\":\n                                       {\"match\": \"fqdn={}\".format(hostName),\n                                        \"value\": attributes}})", "target": "Function __setitem__\n        Set a parameter of a foreman object as a dict\n\n        @param key: The key to modify\n        @param attribute: The data\n        @return RETURN: The API result"}
{"task_id": "codesearchnet/python/train/193", "input": "def get_interval_timedelta(self):\n        \"\"\" Spits out the timedelta in days. \"\"\"\n\n        now_datetime = timezone.now()\n        current_month_days = monthrange(now_datetime.year, now_datetime.month)[1]\n\n        # Two weeks\n        if self.interval == reminders_choices.INTERVAL_2_WEEKS:\n            interval_timedelta = datetime.timedelta(days=14)\n\n        # One month\n        elif self.interval == reminders_choices.INTERVAL_ONE_MONTH:\n            interval_timedelta = datetime.timedelta(days=current_month_days)\n\n        # Three months\n        elif self.interval == reminders_choices.INTERVAL_THREE_MONTHS:\n            three_months = now_datetime + relativedelta(months=+3)\n            interval_timedelta = three_months - now_datetime\n\n        # Six months\n        elif self.interval == reminders_choices.INTERVAL_SIX_MONTHS:\n            six_months = now_datetime + relativedelta(months=+6)\n            interval_timedelta = six_months - now_datetime\n\n        # One year\n        elif self.interval == reminders_choices.INTERVAL_ONE_YEAR:\n            one_year = now_datetime + relativedelta(years=+1)\n            interval_timedelta = one_year - now_datetime\n\n        return interval_timedelta", "target": "Spits out the timedelta in days."}
{"task_id": "codesearchnet/python/train/194", "input": "async def awaitable_runner(runner: BaseRunner):\n    \"\"\"Execute a runner without blocking the event loop\"\"\"\n    runner_thread = CapturingThread(target=runner.run)\n    runner_thread.start()\n    delay = 0.0\n    while not runner_thread.join(timeout=0):\n        await asyncio.sleep(delay)\n        delay = min(delay + 0.1, 1.0)", "target": "Execute a runner without blocking the event loop"}
{"task_id": "codesearchnet/python/train/195", "input": "def asyncio_main_run(root_runner: BaseRunner):\n    \"\"\"\n    Create an ``asyncio`` event loop running in the main thread and watching runners\n\n    Using ``asyncio`` to handle suprocesses requires a specific loop type to run in the main thread.\n    This function sets up and runs the correct loop in a portable way.\n    In addition, it runs a single :py:class:`~.BaseRunner` until completion or failure.\n\n    .. seealso:: The `issue #8 <https://github.com/MatterMiners/cobald/issues/8>`_ for details.\n    \"\"\"\n    assert threading.current_thread() == threading.main_thread(), 'only main thread can accept asyncio subprocesses'\n    if sys.platform == 'win32':\n        event_loop = asyncio.ProactorEventLoop()\n        asyncio.set_event_loop(event_loop)\n    else:\n        event_loop = asyncio.get_event_loop()\n        asyncio.get_child_watcher().attach_loop(event_loop)\n    event_loop.run_until_complete(awaitable_runner(root_runner))", "target": "Create an ``asyncio`` event loop running in the main thread and watching runners\n\n    Using ``asyncio`` to handle suprocesses requires a specific loop type to run in the main thread.\n    This function sets up and runs the correct loop in a portable way.\n    In addition, it runs a single :py:class:`~.BaseRunner` until completion or failure.\n\n    .. seealso:: The `issue #8 <https://github.com/MatterMiners/cobald/issues/8>`_ for details."}
{"task_id": "codesearchnet/python/train/196", "input": "def enhance(self):\n        \"\"\" Function enhance\n        Enhance the object with new item or enhanced items\n        \"\"\"\n        self.update({'os_default_templates':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemOsDefaultTemplate)})\n        self.update({'operatingsystems':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemOperatingSystem)})", "target": "Function enhance\n        Enhance the object with new item or enhanced items"}
{"task_id": "codesearchnet/python/train/197", "input": "def retry(tries=10, delay=1, backoff=2, retry_exception=None):\n    \"\"\"\n    Retry \"tries\" times, with initial \"delay\", increasing delay \"delay*backoff\" each time.\n    Without exception success means when function returns valid object.\n    With exception success when no exceptions\n    \"\"\"\n    assert tries > 0, \"tries must be 1 or greater\"\n    catching_mode = bool(retry_exception)\n\n    def deco_retry(f):\n        @functools.wraps(f)\n        def f_retry(*args, **kwargs):\n            mtries, mdelay = tries, delay\n\n            while mtries > 0:\n                time.sleep(mdelay)\n                mdelay *= backoff\n                try:\n                    rv = f(*args, **kwargs)\n                    if not catching_mode and rv:\n                        return rv\n                except retry_exception:\n                    pass\n                else:\n                    if catching_mode:\n                        return rv\n                mtries -= 1\n                if mtries is 0 and not catching_mode:\n                    return False\n                if mtries is 0 and catching_mode:\n                    return f(*args, **kwargs)  # extra try, to avoid except-raise syntax\n                log.debug(\"{0} try, sleeping for {1} sec\".format(tries-mtries, mdelay))\n            raise Exception(\"unreachable code\")\n        return f_retry\n    return deco_retry", "target": "Retry \"tries\" times, with initial \"delay\", increasing delay \"delay*backoff\" each time.\n    Without exception success means when function returns valid object.\n    With exception success when no exceptions"}
{"task_id": "codesearchnet/python/train/198", "input": "def dump(node):\n    \"\"\" Dump initialized object structure to yaml\n    \"\"\"\n\n    from qubell.api.private.platform import Auth, QubellPlatform\n    from qubell.api.private.organization import Organization\n    from qubell.api.private.application import Application\n    from qubell.api.private.instance import Instance\n    from qubell.api.private.revision import Revision\n    from qubell.api.private.environment import Environment\n    from qubell.api.private.zone import Zone\n    from qubell.api.private.manifest import Manifest\n\n    # Exclude keys from dump\n    # Format: { 'ClassName': ['fields', 'to', 'exclude']}\n    exclusion_list = {\n        Auth: ['cookies'],\n        QubellPlatform:['auth', ],\n        Organization: ['auth', 'organizationId', 'zone'],\n        Application: ['auth', 'applicationId', 'organization'],\n        Instance: ['auth', 'instanceId', 'application'],\n        Manifest: ['name', 'content'],\n        Revision: ['auth', 'revisionId'],\n        Environment: ['auth', 'environmentId', 'organization'],\n        Zone: ['auth', 'zoneId', 'organization'],\n    }\n\n    def obj_presenter(dumper, obj):\n        for x in exclusion_list.keys():\n            if isinstance(obj, x): # Find class\n                fields = obj.__dict__.copy()\n                for excl_item in exclusion_list[x]:\n                    try:\n                        fields.pop(excl_item)\n                    except:\n                        log.warn('No item %s in object %s' % (excl_item, x))\n                return dumper.represent_mapping('tag:yaml.org,2002:map', fields)\n        return dumper.represent_mapping('tag:yaml.org,2002:map', obj.__dict__)\n\n\n    noalias_dumper = yaml.dumper.Dumper\n    noalias_dumper.ignore_aliases = lambda self, data: True\n\n    yaml.add_representer(unicode, lambda dumper, value: dumper.represent_scalar(u'tag:yaml.org,2002:str', value))\n    yaml.add_multi_representer(object, obj_presenter)\n    serialized = yaml.dump(node, default_flow_style=False, Dumper=noalias_dumper)\n    return serialized", "target": "Dump initialized object structure to yaml"}
{"task_id": "codesearchnet/python/train/199", "input": "def load_env(file):\n    \"\"\"\n    Generate environment used for 'org.restore' method\n    :param file: env file\n    :return: env\n    \"\"\"\n\n    env = yaml.load(open(file))\n\n    for org in env.get('organizations', []):\n        if not org.get('applications'):\n            org['applications'] = []\n\n        if org.get('starter-kit'):\n            kit_meta = get_starter_kit_meta(org.get('starter-kit'))\n            for meta_app in get_applications_from_metadata(kit_meta):\n                org['applications'].append(meta_app)\n\n        if org.get('meta'):\n            for meta_app in get_applications_from_metadata(org.get('meta')):\n                org['applications'].append(meta_app)\n\n        for app in org.get('applications', []):\n            if app.get('file'):\n                app['file'] = os.path.realpath(os.path.join(os.path.dirname(file), app['file']))\n    return env", "target": "Generate environment used for 'org.restore' method\n    :param file: env file\n    :return: env"}
